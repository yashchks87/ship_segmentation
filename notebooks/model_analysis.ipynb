{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "import sys\n",
    "import PIL\n",
    "import os\n",
    "import tensorflow.keras.backend as K\n",
    "import glob\n",
    "sys.path.append('../scripts/helper_functions_cv/tensorflow_helpers/')\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import segmentation_models as sm\n",
    "sm.set_framework('tf.keras')\n",
    "import multiprocessing as mp\n",
    "from save_weights_every_epoch import CallbackForSavingModelWeights\n",
    "from change_learning_rate_epoch import ChangeLR\n",
    "import multiprocessing as mp\n",
    "import dask.array as da\n",
    "import cv2\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = pd.read_csv('../../files/train_ship_segmentations_v2.csv')\n",
    "csv_file = csv_file.groupby('ImageId')['EncodedPixels'].apply(list).reset_index()\n",
    "image_ids, pixels = csv_file['ImageId'].values.tolist(), csv_file['EncodedPixels'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file['fixed_inputs'] = csv_file['ImageId'].apply(lambda x: '../../files/train_v2/' + x)\n",
    "csv_file['mask_paths'] = csv_file['ImageId'].apply(lambda x: '../../files/masks_v1/train/' + x.split('.')[0] + '.' + 'png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192556/192556 [00:01<00:00, 180359.56it/s]\n"
     ]
    }
   ],
   "source": [
    "for x in tqdm(csv_file['fixed_inputs'].values.tolist()):\n",
    "    if os.path.exists(x) == False:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192556/192556 [00:01<00:00, 183694.46it/s]\n"
     ]
    }
   ],
   "source": [
    "for x in tqdm(csv_file['mask_paths'].values.tolist()):\n",
    "    if os.path.exists(x) == False:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = csv_file[csv_file['fixed_inputs'] != '../../files/train_v2/6384c3e78.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "allowed_gpus = [0]\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "final_gpu_list = [gpus[x] for x in allowed_gpus]\n",
    "tf.config.set_visible_devices(final_gpu_list, \"GPU\")\n",
    "for gpu in final_gpu_list:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('../config_files/dev.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datasets(csv_file, test_size = 0.01):\n",
    "    train, test = train_test_split(csv_file, test_size = test_size, random_state=42)\n",
    "    train, val = train_test_split(train, test_size = test_size, random_state=42)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = pd.read_csv\n",
    "train, val, test = split_datasets(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_label = []\n",
    "for x in train['EncodedPixels'].values.tolist():\n",
    "    if type(x[0]) == str:\n",
    "        type_of_label.append(1)\n",
    "    else:\n",
    "        type_of_label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/ship_segmentation/TB/'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['tb_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_imgs(img, shape):\n",
    "    img = tf.io.read_file(img)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, size=shape)\n",
    "    img = img / 255\n",
    "    return img\n",
    "\n",
    "def only_imgs_masks(img, mask, shape):\n",
    "    img = tf.io.read_file(img)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, size=shape)\n",
    "    mask = tf.io.read_file(mask)\n",
    "    mask = tf.image.decode_jpeg(mask, channels=1)\n",
    "    mask = tf.image.resize(mask, size=shape)\n",
    "    img = img / 255\n",
    "    mask = mask / 255\n",
    "    return img, mask\n",
    "\n",
    "def get_data(data, shape = (256, 256), shuffle = True, repeat = True, batch = True, batch_size = 32):\n",
    "    imgs, masks = data['fixed_inputs'].values.tolist(), data['mask_paths'].values.tolist()\n",
    "    shapes = [shape for x in range(len(imgs))]\n",
    "    tensor = tf.data.Dataset.from_tensor_slices((imgs, masks, shapes))\n",
    "    tensor = tensor.cache()\n",
    "    if repeat:\n",
    "        tensor = tensor.repeat()\n",
    "    if shuffle:\n",
    "        tensor = tensor.shuffle(256 * REPLICAS)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        tensor = tensor.with_options(opt)\n",
    "    tensor = tensor.map(only_imgs_masks)\n",
    "    if batch:\n",
    "        tensor = tensor.batch(batch_size * REPLICAS)\n",
    "    tensor = tensor.prefetch(AUTO)\n",
    "    return tensor\n",
    "\n",
    "def get_data_test(data, shape = (256, 256), shuffle = True, repeat = True, batch = True, batch_size = 32):\n",
    "    imgs = data\n",
    "    shapes = [shape for x in range(len(imgs))]\n",
    "    tensor = tf.data.Dataset.from_tensor_slices((imgs, shapes))\n",
    "    tensor = tensor.cache()\n",
    "    if repeat:\n",
    "        tensor = tensor.repeat()\n",
    "    if shuffle:\n",
    "        tensor = tensor.shuffle(256 * REPLICAS)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        tensor = tensor.with_options(opt)\n",
    "    tensor = tensor.map(only_imgs)\n",
    "    if batch:\n",
    "        tensor = tensor.batch(batch_size * REPLICAS)\n",
    "    tensor = tensor.prefetch(AUTO)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = get_data(train, shape=(512, 512), shuffle=False, repeat=False, batch_size=32)\n",
    "# val_dataset = get_data(val, shape=(512, 512), shuffle=False, repeat=False, batch_size=32)\n",
    "# test_dataset = get_data(test, shape=(512, 512), shuffle=False, repeat=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    with strategy.scope():\n",
    "        model = sm.Unet(model_name)\n",
    "        model.compile(\n",
    "            tf.keras.optimizers.SGD(momentum=0.8),\n",
    "            loss = sm.losses.bce_dice_loss,\n",
    "            metrics = [sm.metrics.iou_score]\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = get_model('inceptionv3')\n",
    "    model.load_weights('../../best_weight/100.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(train_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(val_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = get_data_test(train['fixed_inputs'].values.tolist(), shape=(512, 512), shuffle=False, repeat=False, batch_size=32)\n",
    "# val_dataset = get_data(val, shape=(512, 512), shuffle=False, repeat=False, batch_size=32)\n",
    "# test_dataset = get_data(test, shape=(512, 512), shuffle=False, repeat=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = train['fixed_inputs'].values\n",
    "train_masks = train['mask_paths'].values\n",
    "train_paths_splits = np.split(train_paths, np.arange(5000, len(train_paths), 5000))\n",
    "train_masks_splits = np.split(train_masks, np.arange(5000, len(train_paths), 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_mask_read(img_path):\n",
    "    im = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    im = cv2.resize(im, (512, 512), cv2.INTER_LINEAR)\n",
    "    im = im / 255\n",
    "    im = np.where(im < 1, 0, 1)\n",
    "    return im "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_numpy_files(data):\n",
    "#     try:\n",
    "#         paths, masks, count = data[0], data[1], data[2]\n",
    "#         K.clear_session()\n",
    "#         dataset = get_data_test(paths, shape=(512, 512), shuffle=False, repeat=False, batch_size=32)\n",
    "#         preds = model.predict(dataset, verbose = 1)\n",
    "#         with mp.Pool(15) as p:\n",
    "#             g_truths = np.array(p.map(g_mask_read, masks)).flatten()\n",
    "#         preds = preds.flatten()\n",
    "#         np.save(f'../../np_arrays/g_truths/{count}.npy', g_truths)\n",
    "#         np.save(f'../../np_arrays/preds/{count}.npy', preds)\n",
    "#         return 'DONE'\n",
    "#     except:\n",
    "#         return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 31s 193ms/step\n",
      "157/157 [==============================] - 31s 195ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 31s 197ms/step\n",
      "157/157 [==============================] - 31s 198ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 32s 196ms/step\n",
      "157/157 [==============================] - 31s 195ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 32s 196ms/step\n",
      "157/157 [==============================] - 31s 195ms/step\n",
      "157/157 [==============================] - 31s 195ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 31s 195ms/step\n",
      "157/157 [==============================] - 32s 196ms/step\n",
      "157/157 [==============================] - 32s 196ms/step\n",
      "157/157 [==============================] - 31s 195ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 31s 195ms/step\n",
      "157/157 [==============================] - 32s 196ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 31s 195ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 32s 195ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 31s 195ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "157/157 [==============================] - 31s 195ms/step\n",
      "157/157 [==============================] - 31s 196ms/step\n",
      "117/117 [==============================] - 24s 200ms/step\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "g_t, p_p = None, None\n",
    "for idx in range(len(train_paths_splits)):\n",
    "    K.clear_session()\n",
    "    dataset = get_data_test(train_paths_splits[idx], shape=(512, 512), shuffle=False, repeat=False, batch_size=32)\n",
    "    preds = model.predict(dataset, verbose = 1)\n",
    "    with mp.Pool(15) as p:\n",
    "        g_truths = np.array(p.map(g_mask_read, train_masks_splits[idx])).flatten()\n",
    "    preds = preds.flatten()\n",
    "    g_t, p_p = g_truths, preds\n",
    "    np.save(f'../../np_arrays/g_truths/{count}.npy', g_truths)\n",
    "    np.save(f'../../np_arrays/preds/{count}.npy', preds)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(g_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.7145459e-04, 3.2782152e-06, 7.7346285e-06, ..., 2.8677005e-06,\n",
       "       6.9047573e-08, 3.1193013e-06], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_files(data):\n",
    "    g_p, p_p, thres = data[0], data[1], data[2]\n",
    "    g_truth = np.load(g_p)\n",
    "    pred = np.load(p_p)\n",
    "    gt_da = dask.array.from_array(g_truth)\n",
    "    p_da = dask.array.from_array(pred)\n",
    "    p_da = da.where(p_da > thres, 1, 0)\n",
    "    TP = da.sum(da.logical_and(gt_da == 1, p_da == 1))\n",
    "    TN = da.sum(da.logical_and(gt_da == 0, p_da == 0))\n",
    "    FP = da.sum(da.logical_and(gt_da == 0, p_da == 1))\n",
    "    FN = da.sum(da.logical_and(gt_da == 1, p_da == 0))\n",
    "    TP, TN, FP, FN = TP.compute(), TN.compute(), FP.compute(), FN.compute()\n",
    "    return [TP, TN, FP, FN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres_preds = []\n",
    "count = 0\n",
    "results = None\n",
    "thres = np.linspace(0,1,num=100)\n",
    "for x in range(len(thres)):\n",
    "    g_truth_paths, pred_paths = [f'../../np_arrays/g_truths/{x}.npy' for x in range(38)], [f'../../np_arrays/preds/{x}.npy' for x in range(38)]\n",
    "    thres_list = [thres[x] for y in range(len(g_truth_paths))]\n",
    "    data_preps = list(zip(g_truth_paths, pred_paths, thres_list))\n",
    "    with mp.Pool(6) as p:\n",
    "        results = np.array(p.map(process_large_files, data_preps))\n",
    "    TP, TN, FP, FN = np.sum(results[:, 0]), np.sum(results[:, 1]), np.sum(results[:, 2]), np.sum(results[:, 3])\n",
    "    thres_preds.append((thres[x], TP, TN, FP, FN))\n",
    "    with open('../../confusion_mat.pickle', 'wb') as handle:\n",
    "        pickle.dump(thres_preds, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_env",
   "language": "python",
   "name": "tensor_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
