{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"aa8401d73c7a19e1a43fdd6a992ea9dcb60039a2"},"source":["# Overview\n","The notebook shows how to extract the segmentation map for the ships, augment the images and train a simple DNN model to detect them. A few additional tweaks like balancing the ship-count out a little better have been done."]},{"cell_type":"markdown","metadata":{"_uuid":"a6cd9d5ad61ffe3b8858769f20a5f9493f024a56"},"source":["## Model Parameters\n","We might want to adjust these later (or do some hyperparameter optimizations)"]},{"cell_type":"code","execution_count":1,"metadata":{"_uuid":"301a5d939c566d1487a049bb2554d09b592b18b1","trusted":true},"outputs":[],"source":["BATCH_SIZE = 4\n","EDGE_CROP = 16\n","NB_EPOCHS = 5\n","GAUSSIAN_NOISE = 0.1\n","UPSAMPLE_MODE = 'SIMPLE'\n","# downsampling inside the network\n","NET_SCALING = None\n","# downsampling in preprocessing\n","IMG_SCALING = (1, 1)\n","# number of validation images to use\n","VALID_IMG_COUNT = 400\n","# maximum number of steps_per_epoch in training\n","MAX_TRAIN_STEPS = 200\n","AUGMENT_BRIGHTNESS = False"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-03-30 17:20:53.338283: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-30 17:20:53.485055: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-03-30 17:20:54.036237: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2023-03-30 17:20:54.036319: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2023-03-30 17:20:54.036340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"]}],"source":["import os\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from skimage.io import imread\n","import matplotlib.pyplot as plt\n","from skimage.segmentation import mark_boundaries\n","from sklearn.model_selection import train_test_split\n","# from skimage.util.montage import montage2d as montage\n","# montage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\n","# ship_dir = '../input'\n","# train_image_dir = os.path.join(ship_dir, 'train_v2')\n","# test_image_dir = os.path.join(ship_dir, 'test_v2')\n","# import gc; gc.enable() # memory is tight\n","os.environ['CUDA_VISIBLE_DEVICES'] = ''\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","# from skimage.morphology import label\n","# def multi_rle_encode(img):\n","#     labels = label(img[:, :, 0])\n","#     return [rle_encode(labels==k) for k in np.unique(labels[labels>0])]\n","\n","# # ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n","# def rle_encode(img):\n","#     '''\n","#     img: numpy array, 1 - mask, 0 - background\n","#     Returns run length as string formated\n","#     '''\n","#     pixels = img.T.flatten()\n","#     pixels = np.concatenate([[0], pixels, [0]])\n","#     runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","#     runs[1::2] -= runs[::2]\n","#     return ' '.join(str(x) for x in runs)\n","\n","# def rle_decode(mask_rle, shape=(768, 768)):\n","#     '''\n","#     mask_rle: run-length as string formated (start length)\n","#     shape: (height,width) of array to return \n","#     Returns numpy array, 1 - mask, 0 - background\n","#     '''\n","#     s = mask_rle.split()\n","#     starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","#     starts -= 1\n","#     ends = starts + lengths\n","#     img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","#     for lo, hi in zip(starts, ends):\n","#         img[lo:hi] = 1\n","#     return img.reshape(shape).T  # Needed to align to RLE direction\n","\n","# def masks_as_image(in_mask_list):\n","#     # Take the individual ship masks and create a single mask array for all ships\n","#     all_masks = np.zeros((768, 768), dtype = np.int16)\n","#     #if isinstance(in_mask_list, list):\n","#     for mask in in_mask_list:\n","#         if isinstance(mask, str):\n","#             all_masks += rle_decode(mask)\n","#     return np.expand_dims(all_masks, -1)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["REPLICAS, AUTO = 1, -1"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["csv_file = pd.read_csv('../../ml-data-training/ship_segmentation_data/train_ship_segmentations_v2.csv')\n","csv_file = csv_file.groupby('ImageId')['EncodedPixels'].apply(list).reset_index()\n","image_ids, pixels = csv_file['ImageId'].values.tolist(), csv_file['EncodedPixels'].values.tolist()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["csv_file['fixed_inputs'] = csv_file['ImageId'].apply(lambda x: '../../ml-data-training/ship_segmentation_data/train_v2/' + x)\n","csv_file['mask_paths'] = csv_file['ImageId'].apply(lambda x: '../../ml-data-training/ship_segmentation_data/masks_v1/train/' + x.split('.')[0] + '.' + 'png')"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["csv_file = csv_file[csv_file['fixed_inputs'] != '../../ml-data-training/ship_segmentation_data/train_v2/6384c3e78.jpg']"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def split_datasets(csv_file):\n","    train, test = train_test_split(csv_file, test_size = 0.01)\n","    train, val = train_test_split(train, test_size = 0.01)\n","    return train, val, test"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["train, val, test = split_datasets(csv_file)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def read_train_imgs(img, mask, shape):\n","    img = tf.io.read_file(img)\n","    mask = tf.io.read_file(mask)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    mask = tf.image.decode_jpeg(mask, channels=1)\n","    img = tf.image.resize(img, size=shape)\n","    mask = tf.image.resize(mask, size=shape)\n","    img = img / 255\n","    mask = mask / 255\n","    return img, mask\n","\n","def get_data(data, shape = (256, 256), shuffle = True, repeat = True, batch = True, batch_size = 32):\n","    imgs, masks = data['fixed_inputs'].values.tolist(), data['mask_paths'].values.tolist()\n","    shapes = [shape for x in range(len(imgs))]\n","    tensor = tf.data.Dataset.from_tensor_slices((imgs, masks, shapes))\n","    tensor = tensor.cache()\n","    if repeat:\n","        tensor = tensor.repeat()\n","    if shuffle:\n","        tensor = tensor.shuffle(256 * REPLICAS)\n","        opt = tf.data.Options()\n","        opt.experimental_deterministic = False\n","        tensor = tensor.with_options(opt)\n","    tensor = tensor.map(read_train_imgs)\n","    if batch:\n","        tensor = tensor.batch(batch_size * REPLICAS)\n","    tensor = tensor.prefetch(AUTO)\n","    return tensor"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["train_dataset = get_data(train, shape = (32, 32), batch_size=32)\n","val_dataset = get_data(val, shape = (32, 32), batch_size=32)"]},{"cell_type":"markdown","metadata":{"_uuid":"ba08494eb9736ec3556b7c879143cdcdea89febf"},"source":["# Build a Model\n","Here we use a slight deviation on the U-Net standard"]},{"cell_type":"code","execution_count":15,"metadata":{"_uuid":"2687377309d3cbbab1197f4eccd2b50ab996f5a6","trusted":true},"outputs":[{"ename":"ValueError","evalue":"Exception encountered when calling layer \"cropping2d\" (type Cropping2D).\n\nArgument `cropping` must be greater than the input shape. Received: inputs.shape=(None, 32, 32, 1), and cropping=((16, 16), (16, 16))\n\nCall arguments received by layer \"cropping2d\" (type Cropping2D):\n  â€¢ inputs=tf.Tensor(shape=(None, 32, 32, 1), dtype=float32)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m c9 \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mConv2D(\u001b[39m8\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m) (c9)\n\u001b[1;32m     61\u001b[0m d \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mConv2D(\u001b[39m1\u001b[39m, (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m) (c9)\n\u001b[0;32m---> 62\u001b[0m d \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39;49mCropping2D((EDGE_CROP, EDGE_CROP))(d)\n\u001b[1;32m     63\u001b[0m d \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mZeroPadding2D((EDGE_CROP, EDGE_CROP))(d)\n\u001b[1;32m     64\u001b[0m \u001b[39mif\u001b[39;00m NET_SCALING \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/test_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m~/test_env/lib/python3.8/site-packages/keras/layers/reshaping/cropping2d.py:185\u001b[0m, in \u001b[0;36mCropping2D.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    179\u001b[0m         inputs\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    180\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcropping[\u001b[39m0\u001b[39m]) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcropping[\u001b[39m1\u001b[39m]) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n\u001b[1;32m    184\u001b[0m     ):\n\u001b[0;32m--> 185\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    186\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mArgument `cropping` must be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgreater than the input shape. Received: inputs.shape=\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00minputs\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, and cropping=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcropping\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m         )\n\u001b[1;32m    190\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcropping[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcropping[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    191\u001b[0m         \u001b[39mreturn\u001b[39;00m inputs[\n\u001b[1;32m    192\u001b[0m             :, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcropping[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] :, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcropping[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m] :, :\n\u001b[1;32m    193\u001b[0m         ]\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"cropping2d\" (type Cropping2D).\n\nArgument `cropping` must be greater than the input shape. Received: inputs.shape=(None, 32, 32, 1), and cropping=((16, 16), (16, 16))\n\nCall arguments received by layer \"cropping2d\" (type Cropping2D):\n  â€¢ inputs=tf.Tensor(shape=(None, 32, 32, 1), dtype=float32)"]}],"source":["from keras import models, layers\n","# Build U-Net model\n","def upsample_conv(filters, kernel_size, strides, padding):\n","    return layers.Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)\n","def upsample_simple(filters, kernel_size, strides, padding):\n","    return layers.UpSampling2D(strides)\n","\n","if UPSAMPLE_MODE=='DECONV':\n","    upsample=upsample_conv\n","else:\n","    upsample=upsample_simple\n","    \n","input_img = layers.Input((32, 32, 3), name = 'RGB_Input')\n","pp_in_layer = input_img\n","if NET_SCALING is not None:\n","    pp_in_layer = layers.AvgPool2D(NET_SCALING)(pp_in_layer)\n","    \n","pp_in_layer = layers.GaussianNoise(GAUSSIAN_NOISE)(pp_in_layer)\n","pp_in_layer = layers.BatchNormalization()(pp_in_layer)\n","\n","c1 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (pp_in_layer)\n","c1 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\n","p1 = layers.MaxPooling2D((2, 2)) (c1)\n","\n","c2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\n","c2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\n","p2 = layers.MaxPooling2D((2, 2)) (c2)\n","\n","c3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\n","c3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\n","p3 = layers.MaxPooling2D((2, 2)) (c3)\n","\n","c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\n","c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\n","p4 = layers.MaxPooling2D(pool_size=(2, 2)) (c4)\n","\n","\n","c5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\n","c5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n","\n","u6 = upsample(64, (2, 2), strides=(2, 2), padding='same') (c5)\n","u6 = layers.concatenate([u6, c4])\n","c6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\n","c6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n","\n","u7 = upsample(32, (2, 2), strides=(2, 2), padding='same') (c6)\n","u7 = layers.concatenate([u7, c3])\n","c7 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\n","c7 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n","\n","u8 = upsample(16, (2, 2), strides=(2, 2), padding='same') (c7)\n","u8 = layers.concatenate([u8, c2])\n","c8 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\n","c8 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n","\n","u9 = upsample(8, (2, 2), strides=(2, 2), padding='same') (c8)\n","u9 = layers.concatenate([u9, c1], axis=3)\n","c9 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\n","c9 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n","\n","d = layers.Conv2D(1, (1, 1), activation='sigmoid') (c9)\n","d = layers.Cropping2D((EDGE_CROP, EDGE_CROP))(d)\n","d = layers.ZeroPadding2D((EDGE_CROP, EDGE_CROP))(d)\n","if NET_SCALING is not None:\n","    d = layers.UpSampling2D(NET_SCALING)(d)\n","\n","seg_model = models.Model(inputs=[input_img], outputs=[d])\n","seg_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1678069aa8013510264ba898291c6ae2dce88a76","trusted":true},"outputs":[],"source":["import keras.backend as K\n","from keras.optimizers import Adam\n","from keras.losses import binary_crossentropy\n","def dice_coef(y_true, y_pred, smooth=1):\n","    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n","    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n","    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n","def dice_p_bce(in_gt, in_pred):\n","    return 1e-3*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\n","def true_positive_rate(y_true, y_pred):\n","    return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))/K.sum(y_true)\n","seg_model.compile(optimizer=Adam(1e-4, decay=1e-6), loss=dice_p_bce, metrics=[dice_coef, 'binary_accuracy', true_positive_rate])"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"7282d18de3aff1cee12ff89b7d511a391702814f","trusted":true},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n","weight_path=\"{}_weights.best.hdf5\".format('seg_model')\n","\n","checkpoint = ModelCheckpoint(weight_path, monitor='val_dice_coef', verbose=1, \n","                             save_best_only=True, mode='max', save_weights_only = True)\n","\n","reduceLROnPlat = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.5, \n","                                   patience=3, \n","                                   verbose=1, mode='max', epsilon=0.0001, cooldown=2, min_lr=1e-6)\n","early = EarlyStopping(monitor=\"val_dice_coef\", \n","                      mode=\"max\", \n","                      patience=15) # probably needs to be more patient, but kaggle time is limited\n","callbacks_list = [checkpoint, early, reduceLROnPlat]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5b67d808c0b8c7e28bff41e6d3858ff6f09dd626","scrolled":false,"trusted":true},"outputs":[],"source":["step_count = min(MAX_TRAIN_STEPS, balanced_train_df.shape[0]//BATCH_SIZE)\n","aug_gen = create_aug_gen(make_image_gen(balanced_train_df))\n","loss_history = [seg_model.fit_generator(aug_gen, \n","                             steps_per_epoch=step_count, \n","                             epochs=NB_EPOCHS, \n","                             validation_data=(valid_x, valid_y),\n","                             callbacks=callbacks_list,\n","                            workers=1 # the generator is not very thread safe\n","                                       )]"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a168c8b1af446b800f6129104906003ededd61c4","trusted":true},"outputs":[],"source":["def show_loss(loss_history):\n","    epich = np.cumsum(np.concatenate(\n","        [np.linspace(0.5, 1, len(mh.epoch)) for mh in loss_history]))\n","    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(22, 10))\n","    _ = ax1.plot(epich,\n","                 np.concatenate([mh.history['loss'] for mh in loss_history]),\n","                 'b-',\n","                 epich, np.concatenate(\n","            [mh.history['val_loss'] for mh in loss_history]), 'r-')\n","    ax1.legend(['Training', 'Validation'])\n","    ax1.set_title('Loss')\n","\n","    _ = ax2.plot(epich, np.concatenate(\n","        [mh.history['true_positive_rate'] for mh in loss_history]), 'b-',\n","                     epich, np.concatenate(\n","            [mh.history['val_true_positive_rate'] for mh in loss_history]),\n","                     'r-')\n","    ax2.legend(['Training', 'Validation'])\n","    ax2.set_title('True Positive Rate\\n(Positive Accuracy)')\n","    \n","    _ = ax3.plot(epich, np.concatenate(\n","        [mh.history['binary_accuracy'] for mh in loss_history]), 'b-',\n","                     epich, np.concatenate(\n","            [mh.history['val_binary_accuracy'] for mh in loss_history]),\n","                     'r-')\n","    ax3.legend(['Training', 'Validation'])\n","    ax3.set_title('Binary Accuracy (%)')\n","    \n","    _ = ax4.plot(epich, np.concatenate(\n","        [mh.history['dice_coef'] for mh in loss_history]), 'b-',\n","                     epich, np.concatenate(\n","            [mh.history['val_dice_coef'] for mh in loss_history]),\n","                     'r-')\n","    ax4.legend(['Training', 'Validation'])\n","    ax4.set_title('DICE')\n","\n","show_loss(loss_history)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ce1167e9f09200f537e61f93f486168a13be1711","trusted":true},"outputs":[],"source":["seg_model.load_weights(weight_path)\n","seg_model.save('seg_model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"275b411dc97a350aacaba46c8562efcf2658b1a7","trusted":true},"outputs":[],"source":["pred_y = seg_model.predict(valid_x)\n","print(pred_y.shape, pred_y.min(), pred_y.max(), pred_y.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"6a4fd2ca0cf47ba069a314356bf74c7b531c56ac","trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n","ax.hist(pred_y.ravel(), np.linspace(0, 1, 10))\n","ax.set_xlim(0, 1)\n","ax.set_yscale('log', nonposy='clip')"]},{"cell_type":"markdown","metadata":{"_uuid":"0018ab172d18936f8cc2c5df33d2f840dc16bf4f"},"source":["# Prepare Full Resolution Model\n","Here we account for the scaling so everything can happen in the model itself"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"17408f0ee8dc16149b8eff0447a1427ab3ed82ba","trusted":true},"outputs":[],"source":["if IMG_SCALING is not None:\n","    fullres_model = models.Sequential()\n","    fullres_model.add(layers.AvgPool2D(IMG_SCALING, input_shape = (None, None, 3)))\n","    fullres_model.add(seg_model)\n","    fullres_model.add(layers.UpSampling2D(IMG_SCALING))\n","else:\n","    fullres_model = seg_model\n","fullres_model.save('fullres_model.h5')"]},{"cell_type":"markdown","metadata":{"_uuid":"17edb177402ae51651692511827a7e9d60646533"},"source":["# Run the test data"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4911811f267f9f3397a58902da9e75c6f261ad40","trusted":true},"outputs":[],"source":["test_paths = os.listdir(test_image_dir)\n","print(len(test_paths), 'test images found')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"73ef7b3b2a74bf64968c79b4005075d4f0e23143","trusted":true},"outputs":[],"source":["fig, m_axs = plt.subplots(20, 2, figsize = (10, 40))\n","[c_ax.axis('off') for c_ax in m_axs.flatten()]\n","for (ax1, ax2), c_img_name in zip(m_axs, test_paths):\n","    c_path = os.path.join(test_image_dir, c_img_name)\n","    c_img = imread(c_path)\n","    first_img = np.expand_dims(c_img, 0)/255.0\n","    first_seg = fullres_model.predict(first_img)\n","    ax1.imshow(first_img[0])\n","    ax1.set_title('Image')\n","    ax2.imshow(first_seg[0, :, :, 0], vmin = 0, vmax = 1)\n","    ax2.set_title('Prediction')\n","fig.savefig('test_predictions.png')"]},{"cell_type":"markdown","metadata":{"_uuid":"11a6c6615131ff8c317f95a5097b46565ef21121","collapsed":true,"trusted":true},"source":["# Submission\n","Since gneerating the submission takes a long time and quite a bit of memory we run it in a seperate kernel located at https://www.kaggle.com/kmader/from-trained-u-net-to-submission-part-2 \n","That kernel takes the model saved in this kernel and applies it to all the test data"]}],"metadata":{"kernelspec":{"display_name":"test_env","language":"python","name":"test_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":1}
