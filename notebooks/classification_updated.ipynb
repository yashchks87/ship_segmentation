{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/test_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import glob\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow.keras.backend as K\n",
    "sys.path.append('../scripts/helper_functions_cv/tensorflow_helpers/')\n",
    "from save_weights_every_epoch import CallbackForSavingModelWeights\n",
    "from multiprocessing import Pool\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "import multiprocessing as mp\n",
    "sys.path.append('../scripts/')\n",
    "from find_bad_ones import find_bad_ones\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n"
     ]
    }
   ],
   "source": [
    "allowed_gpus = [x for x in range(8)]\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "final_gpu_list = [gpus[x] for x in allowed_gpus]\n",
    "tf.config.set_visible_devices(final_gpu_list, \"GPU\")\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(REPLICAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('../config_files/dev.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data.pickle', 'rb') as handle:\n",
    "    updated_train_csv = pickle.load(handle)\n",
    "\n",
    "def split_datasets(data, test_size = 0.01):\n",
    "    train, test = train_test_split(data, test_size = test_size, random_state = 42) \n",
    "    train, val = train_test_split(train, test_size = test_size, random_state = 42)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = split_datasets(updated_train_csv)\n",
    "\n",
    "train_labels = train['class_labels'].values.tolist()\n",
    "computed = compute_class_weight(class_weight='balanced', classes=[0, 1], y=train_labels)\n",
    "class_weight_dict = {\n",
    "    0: computed[0],\n",
    "    1: computed[1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_imgs(img, label, shape):\n",
    "    img = tf.io.read_file(img)\n",
    "    img = tf.image.decode_jpeg(img, channels = 3)\n",
    "    img = tf.image.resize(img, size = shape)\n",
    "    img = img / 255\n",
    "    return img, label\n",
    "\n",
    "def get_data(data, shape = (256, 256), shuffle = True, repeat = True, batch = True, batch_size = 32):\n",
    "    imgs, labels = data['fixed_paths'].values.tolist(), data['class_labels'].values.tolist()\n",
    "    shapes = [shape for x in range(len(imgs))]\n",
    "    tensor = tf.data.Dataset.from_tensor_slices((imgs, labels, shapes))\n",
    "    tensor = tensor.cache()\n",
    "    if repeat:\n",
    "        tensor = tensor.repeat()\n",
    "    if shuffle:\n",
    "        tensor = tensor.shuffle(8048 * 1)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        tensor = tensor.with_options(opt)\n",
    "    tensor = tensor.map(read_train_imgs)\n",
    "    if batch:\n",
    "        tensor = tensor.batch(batch_size * REPLICAS)\n",
    "    tensor = tensor.prefetch(AUTO)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name, shape):\n",
    "    with strategy.scope():\n",
    "        input_layer = tf.keras.Input(shape = shape)\n",
    "        construct = getattr(keras.applications, model_name)\n",
    "        mid_layer = construct(include_top = False, \n",
    "                            weights = None, \n",
    "                            pooling = 'avg')(input_layer)\n",
    "        last_layer = keras.layers.Dense(1, activation = 'sigmoid')(mid_layer)\n",
    "        model = keras.Model(input_layer, last_layer)\n",
    "    return model\n",
    "def compile_new_model(model):\n",
    "    with strategy.scope():\n",
    "        loss = keras.losses.BinaryCrossentropy(label_smoothing=0.05)\n",
    "        optimizer = keras.optimizers.SGD()\n",
    "        prec = keras.metrics.Precision(name = 'prec')\n",
    "        rec = keras.metrics.Recall(name = 'rec')\n",
    "        model.compile(\n",
    "            loss = loss,\n",
    "            optimizer = optimizer,\n",
    "            metrics = [prec, rec]\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/ship_segmentation/TB/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['tb_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "  6/184 [..............................] - ETA: 30s - loss: 0.9848 - prec: 0.2471 - rec: 0.2533WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1672s vs `on_train_batch_end` time: 0.4839s). Check your callbacks.\n",
      "184/184 [==============================] - 185s 259ms/step - loss: 0.6816 - prec: 0.3246 - rec: 0.1562 - val_loss: 0.5668 - val_prec: 0.0000e+00 - val_rec: 0.0000e+00\n",
      "Epoch 2/30\n",
      "184/184 [==============================] - 39s 210ms/step - loss: 0.5519 - prec: 0.4758 - rec: 0.2385 - val_loss: 0.5562 - val_prec: 0.5000 - val_rec: 0.0023\n",
      "Epoch 3/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.5122 - prec: 0.5281 - rec: 0.2991 - val_loss: 0.5565 - val_prec: 0.6588 - val_rec: 0.1299\n",
      "Epoch 4/30\n",
      "184/184 [==============================] - 38s 208ms/step - loss: 0.4811 - prec: 0.5765 - rec: 0.3441 - val_loss: 0.4640 - val_prec: 0.5734 - val_rec: 0.3898\n",
      "Epoch 5/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.4670 - prec: 0.5966 - rec: 0.3887 - val_loss: 0.4673 - val_prec: 0.6364 - val_rec: 0.3573\n",
      "Epoch 6/30\n",
      "184/184 [==============================] - 39s 211ms/step - loss: 0.4503 - prec: 0.6151 - rec: 0.4162 - val_loss: 0.5010 - val_prec: 0.6859 - val_rec: 0.2483\n",
      "Epoch 7/30\n",
      "184/184 [==============================] - 39s 211ms/step - loss: 0.4388 - prec: 0.6310 - rec: 0.4385 - val_loss: 0.5594 - val_prec: 0.7045 - val_rec: 0.2877\n",
      "Epoch 8/30\n",
      "184/184 [==============================] - 39s 210ms/step - loss: 0.4301 - prec: 0.6425 - rec: 0.4592 - val_loss: 0.5487 - val_prec: 0.6852 - val_rec: 0.3434\n",
      "Epoch 9/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.4248 - prec: 0.6526 - rec: 0.4782 - val_loss: 0.4614 - val_prec: 0.6034 - val_rec: 0.5754\n",
      "Epoch 10/30\n",
      "184/184 [==============================] - 39s 209ms/step - loss: 0.4173 - prec: 0.6619 - rec: 0.4903 - val_loss: 0.4187 - val_prec: 0.7108 - val_rec: 0.4733\n",
      "Epoch 11/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.4096 - prec: 0.6761 - rec: 0.5040 - val_loss: 0.4519 - val_prec: 0.7259 - val_rec: 0.4548\n",
      "Epoch 12/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.4053 - prec: 0.6834 - rec: 0.5163 - val_loss: 0.4400 - val_prec: 0.7012 - val_rec: 0.5499\n",
      "Epoch 13/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.3994 - prec: 0.6930 - rec: 0.5253 - val_loss: 0.4608 - val_prec: 0.7681 - val_rec: 0.4687\n",
      "Epoch 14/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.3969 - prec: 0.6953 - rec: 0.5315 - val_loss: 0.4417 - val_prec: 0.6848 - val_rec: 0.5545\n",
      "Epoch 15/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.3906 - prec: 0.7078 - rec: 0.5476 - val_loss: 0.3977 - val_prec: 0.6658 - val_rec: 0.6288\n",
      "Epoch 16/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.3852 - prec: 0.7136 - rec: 0.5526 - val_loss: 0.3946 - val_prec: 0.7051 - val_rec: 0.5824\n",
      "Epoch 17/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.3808 - prec: 0.7220 - rec: 0.5645 - val_loss: 0.3951 - val_prec: 0.7692 - val_rec: 0.5104\n",
      "Epoch 18/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.3758 - prec: 0.7268 - rec: 0.5712 - val_loss: 0.3883 - val_prec: 0.7134 - val_rec: 0.5313\n",
      "Epoch 19/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.3731 - prec: 0.7323 - rec: 0.5765 - val_loss: 0.3894 - val_prec: 0.7734 - val_rec: 0.4988\n",
      "Epoch 20/30\n",
      "184/184 [==============================] - 39s 209ms/step - loss: 0.3704 - prec: 0.7371 - rec: 0.5821 - val_loss: 0.4166 - val_prec: 0.7107 - val_rec: 0.5244\n",
      "Epoch 21/30\n",
      "184/184 [==============================] - 38s 208ms/step - loss: 0.3659 - prec: 0.7409 - rec: 0.5890 - val_loss: 0.4550 - val_prec: 0.6127 - val_rec: 0.6937\n",
      "Epoch 22/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.3634 - prec: 0.7453 - rec: 0.5973 - val_loss: 0.3754 - val_prec: 0.7762 - val_rec: 0.5151\n",
      "Epoch 23/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.3592 - prec: 0.7532 - rec: 0.6040 - val_loss: 0.4096 - val_prec: 0.6667 - val_rec: 0.5800\n",
      "Epoch 24/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.3566 - prec: 0.7550 - rec: 0.6100 - val_loss: 0.3930 - val_prec: 0.6902 - val_rec: 0.5893\n",
      "Epoch 25/30\n",
      "184/184 [==============================] - 38s 208ms/step - loss: 0.3531 - prec: 0.7598 - rec: 0.6157 - val_loss: 0.3644 - val_prec: 0.7515 - val_rec: 0.5824\n",
      "Epoch 26/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.3496 - prec: 0.7690 - rec: 0.6230 - val_loss: 0.4061 - val_prec: 0.6296 - val_rec: 0.6705\n",
      "Epoch 27/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.3478 - prec: 0.7686 - rec: 0.6248 - val_loss: 0.4065 - val_prec: 0.7423 - val_rec: 0.5615\n",
      "Epoch 28/30\n",
      "184/184 [==============================] - 37s 203ms/step - loss: 0.3459 - prec: 0.7719 - rec: 0.6273 - val_loss: 0.3691 - val_prec: 0.7649 - val_rec: 0.5661\n",
      "Epoch 29/30\n",
      "184/184 [==============================] - 38s 205ms/step - loss: 0.3417 - prec: 0.7783 - rec: 0.6358 - val_loss: 0.3974 - val_prec: 0.6919 - val_rec: 0.5940\n",
      "Epoch 30/30\n",
      "184/184 [==============================] - 38s 205ms/step - loss: 0.3408 - prec: 0.7776 - rec: 0.6369 - val_loss: 0.3795 - val_prec: 0.7500 - val_rec: 0.6195\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/res_50_baseline/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(32, 32), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(32, 32), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (32, 32, 3))\n",
    "model = compile_new_model(model)\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 30,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "  6/184 [..............................] - ETA: 29s - loss: 0.3327 - prec: 0.7977 - rec: 0.6529WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1617s vs `on_train_batch_end` time: 0.5167s). Check your callbacks.\n",
      "184/184 [==============================] - 172s 242ms/step - loss: 0.3373 - prec: 0.7850 - rec: 0.6445 - val_loss: 0.4081 - val_prec: 0.6700 - val_rec: 0.6311\n",
      "Epoch 2/30\n",
      "184/184 [==============================] - 39s 210ms/step - loss: 0.3330 - prec: 0.7888 - rec: 0.6492 - val_loss: 0.3788 - val_prec: 0.7194 - val_rec: 0.6009\n",
      "Epoch 3/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.3316 - prec: 0.7898 - rec: 0.6536 - val_loss: 0.3745 - val_prec: 0.7683 - val_rec: 0.6079\n",
      "Epoch 4/30\n",
      "184/184 [==============================] - 38s 208ms/step - loss: 0.3293 - prec: 0.7972 - rec: 0.6552 - val_loss: 0.3774 - val_prec: 0.7205 - val_rec: 0.6520\n",
      "Epoch 5/30\n",
      "184/184 [==============================] - 38s 208ms/step - loss: 0.3255 - prec: 0.7992 - rec: 0.6640 - val_loss: 0.4080 - val_prec: 0.7873 - val_rec: 0.4896\n",
      "Epoch 6/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.3234 - prec: 0.8017 - rec: 0.6666 - val_loss: 0.3849 - val_prec: 0.7147 - val_rec: 0.5986\n",
      "Epoch 7/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.3211 - prec: 0.8054 - rec: 0.6690 - val_loss: 0.3682 - val_prec: 0.7341 - val_rec: 0.6148\n",
      "Epoch 8/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.3204 - prec: 0.8070 - rec: 0.6730 - val_loss: 0.3874 - val_prec: 0.7697 - val_rec: 0.5661\n",
      "Epoch 9/30\n",
      "184/184 [==============================] - 38s 204ms/step - loss: 0.3163 - prec: 0.8107 - rec: 0.6803 - val_loss: 0.3918 - val_prec: 0.7874 - val_rec: 0.5499\n",
      "Epoch 10/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.3149 - prec: 0.8106 - rec: 0.6812 - val_loss: 0.4592 - val_prec: 0.7666 - val_rec: 0.5104\n",
      "Epoch 11/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.3130 - prec: 0.8154 - rec: 0.6833 - val_loss: 0.3635 - val_prec: 0.7591 - val_rec: 0.6288\n",
      "Epoch 12/30\n",
      "184/184 [==============================] - 38s 205ms/step - loss: 0.3095 - prec: 0.8195 - rec: 0.6891 - val_loss: 0.3965 - val_prec: 0.7601 - val_rec: 0.5661\n",
      "Epoch 13/30\n",
      "184/184 [==============================] - 39s 209ms/step - loss: 0.3071 - prec: 0.8238 - rec: 0.6922 - val_loss: 0.3925 - val_prec: 0.7185 - val_rec: 0.6218\n",
      "Epoch 14/30\n",
      "184/184 [==============================] - 38s 208ms/step - loss: 0.3059 - prec: 0.8267 - rec: 0.6963 - val_loss: 0.3997 - val_prec: 0.6889 - val_rec: 0.6218\n",
      "Epoch 15/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.3040 - prec: 0.8255 - rec: 0.7020 - val_loss: 0.3943 - val_prec: 0.7643 - val_rec: 0.5267\n",
      "Epoch 16/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.3014 - prec: 0.8293 - rec: 0.7036 - val_loss: 0.4309 - val_prec: 0.6008 - val_rec: 0.7332\n",
      "Epoch 17/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.2992 - prec: 0.8300 - rec: 0.7056 - val_loss: 0.3786 - val_prec: 0.7528 - val_rec: 0.6148\n",
      "Epoch 18/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.2978 - prec: 0.8342 - rec: 0.7094 - val_loss: 0.4079 - val_prec: 0.7508 - val_rec: 0.5592\n",
      "Epoch 19/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.2956 - prec: 0.8368 - rec: 0.7130 - val_loss: 0.4123 - val_prec: 0.6399 - val_rec: 0.6845\n",
      "Epoch 20/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.2936 - prec: 0.8396 - rec: 0.7161 - val_loss: 0.4331 - val_prec: 0.5757 - val_rec: 0.7146\n",
      "Epoch 21/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.2925 - prec: 0.8409 - rec: 0.7185 - val_loss: 0.4124 - val_prec: 0.6245 - val_rec: 0.7796\n",
      "Epoch 22/30\n",
      "184/184 [==============================] - 38s 205ms/step - loss: 0.2914 - prec: 0.8442 - rec: 0.7232 - val_loss: 0.3641 - val_prec: 0.7948 - val_rec: 0.6381\n",
      "Epoch 23/30\n",
      "184/184 [==============================] - 38s 208ms/step - loss: 0.2883 - prec: 0.8461 - rec: 0.7249 - val_loss: 0.3692 - val_prec: 0.7574 - val_rec: 0.6520\n",
      "Epoch 24/30\n",
      "184/184 [==============================] - 38s 205ms/step - loss: 0.2866 - prec: 0.8457 - rec: 0.7273 - val_loss: 0.3834 - val_prec: 0.7193 - val_rec: 0.7077\n",
      "Epoch 25/30\n",
      "184/184 [==============================] - 37s 202ms/step - loss: 0.2845 - prec: 0.8498 - rec: 0.7328 - val_loss: 0.3927 - val_prec: 0.7018 - val_rec: 0.7100\n",
      "Epoch 26/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.2841 - prec: 0.8503 - rec: 0.7344 - val_loss: 0.4154 - val_prec: 0.7611 - val_rec: 0.5174\n",
      "Epoch 27/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.2833 - prec: 0.8506 - rec: 0.7351 - val_loss: 0.3839 - val_prec: 0.7337 - val_rec: 0.6520\n",
      "Epoch 28/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.2812 - prec: 0.8533 - rec: 0.7390 - val_loss: 0.3844 - val_prec: 0.7500 - val_rec: 0.6265\n",
      "Epoch 29/30\n",
      "184/184 [==============================] - 39s 210ms/step - loss: 0.2791 - prec: 0.8578 - rec: 0.7420 - val_loss: 0.3844 - val_prec: 0.7117 - val_rec: 0.6473\n",
      "Epoch 30/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.2773 - prec: 0.8606 - rec: 0.7452 - val_loss: 0.3801 - val_prec: 0.7097 - val_rec: 0.6636\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/res_50_baseline_v1/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path, epoch_number=31)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(32, 32), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(32, 32), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (32, 32, 3))\n",
    "model = compile_new_model(model)\n",
    "model.load_weights('/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline/30.h5')\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 30,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "  6/184 [..............................] - ETA: 31s - loss: 0.6757 - prec: 0.2471 - rec: 0.1064WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1745s vs `on_train_batch_end` time: 0.5093s). Check your callbacks.\n",
      "184/184 [==============================] - 184s 261ms/step - loss: 0.6489 - prec: 0.3073 - rec: 0.0972 - val_loss: 0.5703 - val_prec: 0.0000e+00 - val_rec: 0.0000e+00\n",
      "Epoch 2/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.5496 - prec: 0.4712 - rec: 0.1734 - val_loss: 0.5416 - val_prec: 0.2273 - val_rec: 0.0232\n",
      "Epoch 3/30\n",
      "184/184 [==============================] - 42s 227ms/step - loss: 0.5069 - prec: 0.5339 - rec: 0.2608 - val_loss: 0.5957 - val_prec: 0.4400 - val_rec: 0.0255\n",
      "Epoch 4/30\n",
      "184/184 [==============================] - 42s 228ms/step - loss: 0.4808 - prec: 0.5815 - rec: 0.3388 - val_loss: 0.6401 - val_prec: 0.6087 - val_rec: 0.0650\n",
      "Epoch 5/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.4645 - prec: 0.6023 - rec: 0.3890 - val_loss: 0.6708 - val_prec: 0.6724 - val_rec: 0.1810\n",
      "Epoch 6/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.4476 - prec: 0.6265 - rec: 0.4245 - val_loss: 0.4527 - val_prec: 0.5833 - val_rec: 0.4872\n",
      "Epoch 7/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.4353 - prec: 0.6498 - rec: 0.4501 - val_loss: 0.4795 - val_prec: 0.6102 - val_rec: 0.5012\n",
      "Epoch 8/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.4226 - prec: 0.6646 - rec: 0.4738 - val_loss: 0.4642 - val_prec: 0.6817 - val_rec: 0.4919\n",
      "Epoch 9/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.4125 - prec: 0.6847 - rec: 0.4947 - val_loss: 0.4210 - val_prec: 0.7280 - val_rec: 0.4223\n",
      "Epoch 10/30\n",
      "184/184 [==============================] - 41s 223ms/step - loss: 0.4028 - prec: 0.6951 - rec: 0.5097 - val_loss: 0.4499 - val_prec: 0.7449 - val_rec: 0.4200\n",
      "Epoch 11/30\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.3934 - prec: 0.7155 - rec: 0.5275 - val_loss: 0.5171 - val_prec: 0.7892 - val_rec: 0.3735\n",
      "Epoch 12/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3880 - prec: 0.7283 - rec: 0.5420 - val_loss: 0.3957 - val_prec: 0.6477 - val_rec: 0.6613\n",
      "Epoch 13/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3784 - prec: 0.7407 - rec: 0.5530 - val_loss: 0.3847 - val_prec: 0.7597 - val_rec: 0.5429\n",
      "Epoch 14/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3750 - prec: 0.7487 - rec: 0.5623 - val_loss: 0.3779 - val_prec: 0.7445 - val_rec: 0.5476\n",
      "Epoch 15/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3671 - prec: 0.7563 - rec: 0.5756 - val_loss: 0.4156 - val_prec: 0.7155 - val_rec: 0.5777\n",
      "Epoch 16/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.3624 - prec: 0.7630 - rec: 0.5870 - val_loss: 0.3689 - val_prec: 0.8316 - val_rec: 0.5499\n",
      "Epoch 17/30\n",
      "184/184 [==============================] - 42s 228ms/step - loss: 0.3579 - prec: 0.7711 - rec: 0.5955 - val_loss: 0.3626 - val_prec: 0.8047 - val_rec: 0.5545\n",
      "Epoch 18/30\n",
      "184/184 [==============================] - 41s 222ms/step - loss: 0.3534 - prec: 0.7747 - rec: 0.6024 - val_loss: 0.3541 - val_prec: 0.7359 - val_rec: 0.6659\n",
      "Epoch 19/30\n",
      "184/184 [==============================] - 41s 222ms/step - loss: 0.3495 - prec: 0.7806 - rec: 0.6122 - val_loss: 0.3704 - val_prec: 0.7908 - val_rec: 0.5174\n",
      "Epoch 20/30\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.3459 - prec: 0.7861 - rec: 0.6215 - val_loss: 0.3777 - val_prec: 0.8656 - val_rec: 0.5081\n",
      "Epoch 21/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3415 - prec: 0.7908 - rec: 0.6277 - val_loss: 0.3526 - val_prec: 0.7446 - val_rec: 0.6357\n",
      "Epoch 22/30\n",
      "184/184 [==============================] - 42s 227ms/step - loss: 0.3386 - prec: 0.7957 - rec: 0.6340 - val_loss: 0.3547 - val_prec: 0.8495 - val_rec: 0.5499\n",
      "Epoch 23/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3357 - prec: 0.7974 - rec: 0.6392 - val_loss: 0.3442 - val_prec: 0.8688 - val_rec: 0.5684\n",
      "Epoch 24/30\n",
      "184/184 [==============================] - 41s 221ms/step - loss: 0.3315 - prec: 0.8026 - rec: 0.6470 - val_loss: 0.3769 - val_prec: 0.8755 - val_rec: 0.5383\n",
      "Epoch 25/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3293 - prec: 0.8059 - rec: 0.6517 - val_loss: 0.3276 - val_prec: 0.8358 - val_rec: 0.6613\n",
      "Epoch 26/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3267 - prec: 0.8090 - rec: 0.6595 - val_loss: 0.3255 - val_prec: 0.8000 - val_rec: 0.6311\n",
      "Epoch 27/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3247 - prec: 0.8087 - rec: 0.6642 - val_loss: 0.3319 - val_prec: 0.8360 - val_rec: 0.6032\n",
      "Epoch 28/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.3204 - prec: 0.8151 - rec: 0.6666 - val_loss: 0.3438 - val_prec: 0.7890 - val_rec: 0.6334\n",
      "Epoch 29/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3179 - prec: 0.8164 - rec: 0.6738 - val_loss: 0.3278 - val_prec: 0.8022 - val_rec: 0.6682\n",
      "Epoch 30/30\n",
      "184/184 [==============================] - 41s 221ms/step - loss: 0.3149 - prec: 0.8227 - rec: 0.6788 - val_loss: 0.3412 - val_prec: 0.7603 - val_rec: 0.6404\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/res_50_baseline_64/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline_64/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(64, 64), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(64, 64), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (64, 64, 3))\n",
    "model = compile_new_model(model)\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 30,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "  6/184 [..............................] - ETA: 32s - loss: 0.3144 - prec: 0.8325 - rec: 0.6836WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1764s vs `on_train_batch_end` time: 0.4367s). Check your callbacks.\n",
      "184/184 [==============================] - 187s 260ms/step - loss: 0.3133 - prec: 0.8231 - rec: 0.6829 - val_loss: 0.3358 - val_prec: 0.7760 - val_rec: 0.6752\n",
      "Epoch 2/30\n",
      "184/184 [==============================] - 42s 230ms/step - loss: 0.3115 - prec: 0.8268 - rec: 0.6867 - val_loss: 0.3180 - val_prec: 0.8254 - val_rec: 0.6473\n",
      "Epoch 3/30\n",
      "184/184 [==============================] - 42s 229ms/step - loss: 0.3085 - prec: 0.8269 - rec: 0.6893 - val_loss: 0.4022 - val_prec: 0.7473 - val_rec: 0.6311\n",
      "Epoch 4/30\n",
      "184/184 [==============================] - 42s 229ms/step - loss: 0.3062 - prec: 0.8300 - rec: 0.6961 - val_loss: 0.3267 - val_prec: 0.8424 - val_rec: 0.6450\n",
      "Epoch 5/30\n",
      "184/184 [==============================] - 42s 227ms/step - loss: 0.3042 - prec: 0.8327 - rec: 0.7007 - val_loss: 0.3190 - val_prec: 0.7958 - val_rec: 0.6961\n",
      "Epoch 6/30\n",
      "184/184 [==============================] - 42s 229ms/step - loss: 0.3026 - prec: 0.8322 - rec: 0.7064 - val_loss: 0.3314 - val_prec: 0.8553 - val_rec: 0.6311\n",
      "Epoch 7/30\n",
      "184/184 [==============================] - 42s 228ms/step - loss: 0.2987 - prec: 0.8387 - rec: 0.7088 - val_loss: 0.3789 - val_prec: 0.6569 - val_rec: 0.7819\n",
      "Epoch 8/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.2975 - prec: 0.8365 - rec: 0.7146 - val_loss: 0.3534 - val_prec: 0.7035 - val_rec: 0.7541\n",
      "Epoch 9/30\n",
      "184/184 [==============================] - 41s 223ms/step - loss: 0.2943 - prec: 0.8405 - rec: 0.7196 - val_loss: 0.3110 - val_prec: 0.8110 - val_rec: 0.6868\n",
      "Epoch 10/30\n",
      "184/184 [==============================] - 42s 229ms/step - loss: 0.2933 - prec: 0.8438 - rec: 0.7201 - val_loss: 0.3191 - val_prec: 0.8529 - val_rec: 0.6589\n",
      "Epoch 11/30\n",
      "184/184 [==============================] - 42s 227ms/step - loss: 0.2906 - prec: 0.8469 - rec: 0.7258 - val_loss: 0.3305 - val_prec: 0.8861 - val_rec: 0.5777\n",
      "Epoch 12/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.2886 - prec: 0.8469 - rec: 0.7284 - val_loss: 0.3172 - val_prec: 0.8309 - val_rec: 0.6613\n",
      "Epoch 13/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.2882 - prec: 0.8463 - rec: 0.7289 - val_loss: 0.3201 - val_prec: 0.8107 - val_rec: 0.6659\n",
      "Epoch 14/30\n",
      "184/184 [==============================] - 42s 227ms/step - loss: 0.2834 - prec: 0.8530 - rec: 0.7375 - val_loss: 0.3302 - val_prec: 0.7448 - val_rec: 0.7517\n",
      "Epoch 15/30\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.2830 - prec: 0.8523 - rec: 0.7407 - val_loss: 0.3254 - val_prec: 0.8433 - val_rec: 0.6241\n",
      "Epoch 16/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.2808 - prec: 0.8581 - rec: 0.7425 - val_loss: 0.3115 - val_prec: 0.8395 - val_rec: 0.6798\n",
      "Epoch 17/30\n",
      "184/184 [==============================] - 42s 228ms/step - loss: 0.2783 - prec: 0.8587 - rec: 0.7486 - val_loss: 0.3233 - val_prec: 0.7488 - val_rec: 0.7541\n",
      "Epoch 18/30\n",
      "184/184 [==============================] - 42s 227ms/step - loss: 0.2773 - prec: 0.8594 - rec: 0.7505 - val_loss: 0.3538 - val_prec: 0.8671 - val_rec: 0.6659\n",
      "Epoch 19/30\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.2750 - prec: 0.8622 - rec: 0.7545 - val_loss: 0.3177 - val_prec: 0.7756 - val_rec: 0.7216\n",
      "Epoch 20/30\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.2716 - prec: 0.8646 - rec: 0.7594 - val_loss: 0.3093 - val_prec: 0.8429 - val_rec: 0.6845\n",
      "Epoch 21/30\n",
      "184/184 [==============================] - 41s 223ms/step - loss: 0.2708 - prec: 0.8676 - rec: 0.7611 - val_loss: 0.3028 - val_prec: 0.8146 - val_rec: 0.7239\n",
      "Epoch 22/30\n",
      "184/184 [==============================] - 42s 227ms/step - loss: 0.2688 - prec: 0.8686 - rec: 0.7658 - val_loss: 0.3732 - val_prec: 0.8922 - val_rec: 0.5568\n",
      "Epoch 23/30\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.2664 - prec: 0.8707 - rec: 0.7683 - val_loss: 0.3064 - val_prec: 0.7939 - val_rec: 0.7239\n",
      "Epoch 24/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.2644 - prec: 0.8741 - rec: 0.7725 - val_loss: 0.3698 - val_prec: 0.6883 - val_rec: 0.7633\n",
      "Epoch 25/30\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.2629 - prec: 0.8750 - rec: 0.7757 - val_loss: 0.3235 - val_prec: 0.8097 - val_rec: 0.6613\n",
      "Epoch 26/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.2604 - prec: 0.8790 - rec: 0.7794 - val_loss: 0.3622 - val_prec: 0.7359 - val_rec: 0.6659\n",
      "Epoch 27/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.2595 - prec: 0.8805 - rec: 0.7828 - val_loss: 0.3580 - val_prec: 0.8585 - val_rec: 0.6334\n",
      "Epoch 28/30\n",
      "184/184 [==============================] - 41s 223ms/step - loss: 0.2573 - prec: 0.8817 - rec: 0.7835 - val_loss: 0.3078 - val_prec: 0.8550 - val_rec: 0.6705\n",
      "Epoch 29/30\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.2528 - prec: 0.8870 - rec: 0.7914 - val_loss: 0.3280 - val_prec: 0.8571 - val_rec: 0.6682\n",
      "Epoch 30/30\n",
      "184/184 [==============================] - 41s 221ms/step - loss: 0.2520 - prec: 0.8871 - rec: 0.7955 - val_loss: 0.3017 - val_prec: 0.8529 - val_rec: 0.7262\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/res_50_baseline_64_v1/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline_64/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path, epoch_number=31)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(64, 64), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(64, 64), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (64, 64, 3))\n",
    "model = compile_new_model(model)\n",
    "model.load_weights('/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline_64/30.h5')\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 30,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  6/184 [..............................] - ETA: 39s - loss: 1.1728 - prec: 0.2396 - rec: 0.2001WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2162s vs `on_train_batch_end` time: 0.4461s). Check your callbacks.\n",
      "184/184 [==============================] - 187s 297ms/step - loss: 0.6092 - prec: 0.4112 - rec: 0.1381 - val_loss: 0.6192 - val_prec: 0.0000e+00 - val_rec: 0.0000e+00\n",
      "Epoch 2/30\n",
      "184/184 [==============================] - 52s 283ms/step - loss: 0.5211 - prec: 0.5183 - rec: 0.2663 - val_loss: 0.5579 - val_prec: 0.5128 - val_rec: 0.0464\n",
      "Epoch 3/30\n",
      "184/184 [==============================] - 52s 283ms/step - loss: 0.4843 - prec: 0.5614 - rec: 0.3410 - val_loss: 0.6362 - val_prec: 0.4049 - val_rec: 0.6566\n",
      "Epoch 4/30\n",
      "184/184 [==============================] - 52s 282ms/step - loss: 0.4479 - prec: 0.6240 - rec: 0.4201 - val_loss: 0.5203 - val_prec: 0.6974 - val_rec: 0.2459\n",
      "Epoch 5/30\n",
      "184/184 [==============================] - 52s 283ms/step - loss: 0.4247 - prec: 0.6740 - rec: 0.4781 - val_loss: 0.5731 - val_prec: 0.5862 - val_rec: 0.4733\n",
      "Epoch 6/30\n",
      "184/184 [==============================] - 52s 282ms/step - loss: 0.3943 - prec: 0.7252 - rec: 0.5182 - val_loss: 0.4434 - val_prec: 0.6134 - val_rec: 0.6148\n",
      "Epoch 7/30\n",
      "184/184 [==============================] - 51s 280ms/step - loss: 0.3785 - prec: 0.7518 - rec: 0.5491 - val_loss: 0.4495 - val_prec: 0.5991 - val_rec: 0.6102\n",
      "Epoch 8/30\n",
      "184/184 [==============================] - 52s 280ms/step - loss: 0.3616 - prec: 0.7771 - rec: 0.5764 - val_loss: 0.3786 - val_prec: 0.7032 - val_rec: 0.6543\n",
      "Epoch 9/30\n",
      "184/184 [==============================] - 51s 279ms/step - loss: 0.3482 - prec: 0.7962 - rec: 0.6026 - val_loss: 0.4761 - val_prec: 0.5615 - val_rec: 0.8051\n",
      "Epoch 10/30\n",
      "184/184 [==============================] - 51s 278ms/step - loss: 0.3379 - prec: 0.8093 - rec: 0.6258 - val_loss: 0.4146 - val_prec: 0.9357 - val_rec: 0.3712\n",
      "Epoch 11/30\n",
      "184/184 [==============================] - 51s 279ms/step - loss: 0.3265 - prec: 0.8231 - rec: 0.6478 - val_loss: 0.3275 - val_prec: 0.8276 - val_rec: 0.6682\n",
      "Epoch 12/30\n",
      "184/184 [==============================] - 52s 283ms/step - loss: 0.3168 - prec: 0.8325 - rec: 0.6671 - val_loss: 0.3272 - val_prec: 0.8624 - val_rec: 0.6543\n",
      "Epoch 13/30\n",
      "184/184 [==============================] - 52s 283ms/step - loss: 0.3065 - prec: 0.8429 - rec: 0.6884 - val_loss: 0.3595 - val_prec: 0.8690 - val_rec: 0.6311\n",
      "Epoch 14/30\n",
      "184/184 [==============================] - 52s 281ms/step - loss: 0.3018 - prec: 0.8447 - rec: 0.7018 - val_loss: 0.3219 - val_prec: 0.7657 - val_rec: 0.7053\n",
      "Epoch 15/30\n",
      "184/184 [==============================] - 52s 280ms/step - loss: 0.2942 - prec: 0.8537 - rec: 0.7157 - val_loss: 0.3397 - val_prec: 0.8338 - val_rec: 0.6752\n",
      "Epoch 16/30\n",
      "184/184 [==============================] - 52s 282ms/step - loss: 0.2861 - prec: 0.8591 - rec: 0.7300 - val_loss: 0.2991 - val_prec: 0.8645 - val_rec: 0.6659\n",
      "Epoch 17/30\n",
      "184/184 [==============================] - 51s 279ms/step - loss: 0.2822 - prec: 0.8638 - rec: 0.7409 - val_loss: 0.2976 - val_prec: 0.8736 - val_rec: 0.7378\n",
      "Epoch 18/30\n",
      "184/184 [==============================] - 52s 280ms/step - loss: 0.2765 - prec: 0.8664 - rec: 0.7484 - val_loss: 0.3108 - val_prec: 0.8082 - val_rec: 0.7819\n",
      "Epoch 19/30\n",
      "184/184 [==============================] - 52s 280ms/step - loss: 0.2725 - prec: 0.8688 - rec: 0.7592 - val_loss: 0.2972 - val_prec: 0.8622 - val_rec: 0.7401\n",
      "Epoch 20/30\n",
      "184/184 [==============================] - 51s 279ms/step - loss: 0.2688 - prec: 0.8734 - rec: 0.7665 - val_loss: 0.2846 - val_prec: 0.8964 - val_rec: 0.7425\n",
      "Epoch 21/30\n",
      "184/184 [==============================] - 51s 280ms/step - loss: 0.2649 - prec: 0.8763 - rec: 0.7723 - val_loss: 0.2867 - val_prec: 0.8118 - val_rec: 0.8306\n",
      "Epoch 22/30\n",
      "184/184 [==============================] - 52s 281ms/step - loss: 0.2609 - prec: 0.8798 - rec: 0.7809 - val_loss: 0.2827 - val_prec: 0.8362 - val_rec: 0.7819\n",
      "Epoch 23/30\n",
      "184/184 [==============================] - 52s 280ms/step - loss: 0.2574 - prec: 0.8815 - rec: 0.7844 - val_loss: 0.3120 - val_prec: 0.8663 - val_rec: 0.6914\n",
      "Epoch 24/30\n",
      "184/184 [==============================] - 52s 280ms/step - loss: 0.2548 - prec: 0.8833 - rec: 0.7931 - val_loss: 0.2718 - val_prec: 0.8564 - val_rec: 0.7889\n",
      "Epoch 25/30\n",
      "184/184 [==============================] - 51s 279ms/step - loss: 0.2524 - prec: 0.8836 - rec: 0.7957 - val_loss: 0.2554 - val_prec: 0.9005 - val_rec: 0.7773\n",
      "Epoch 26/30\n",
      "184/184 [==============================] - 52s 282ms/step - loss: 0.2496 - prec: 0.8870 - rec: 0.8019 - val_loss: 0.2552 - val_prec: 0.8645 - val_rec: 0.8144\n",
      "Epoch 27/30\n",
      "184/184 [==============================] - 52s 283ms/step - loss: 0.2466 - prec: 0.8890 - rec: 0.8071 - val_loss: 0.2820 - val_prec: 0.9159 - val_rec: 0.7332\n",
      "Epoch 28/30\n",
      "184/184 [==============================] - 52s 280ms/step - loss: 0.2445 - prec: 0.8919 - rec: 0.8085 - val_loss: 0.2675 - val_prec: 0.8416 - val_rec: 0.8260\n",
      "Epoch 29/30\n",
      "184/184 [==============================] - 52s 282ms/step - loss: 0.2427 - prec: 0.8917 - rec: 0.8136 - val_loss: 0.2537 - val_prec: 0.8947 - val_rec: 0.7889\n",
      "Epoch 30/30\n",
      "121/184 [==================>...........] - ETA: 16s - loss: 0.2393 - prec: 0.8960 - rec: 0.8179"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/res_50_baseline_128/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline_128/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(128, 128), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(128, 128), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (128, 128, 3))\n",
    "model = compile_new_model(model)\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 30,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/res_50_baseline_128_v1/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline_128/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path, epoch_number = 31)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(128, 128), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(128, 128), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (128, 128, 3))\n",
    "model = compile_new_model(model)\n",
    "model.load_weights('/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline_128/30.h5')\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 30,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1/30\n",
      "INFO:tensorflow:batch_all_reduce: 213 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 213 all-reduces with algorithm = nccl, num_packs = 1\n",
      "  6/184 [..............................] - ETA: 47s - loss: 0.6690 - prec: 0.2951 - rec: 0.1275WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2594s vs `on_train_batch_end` time: 0.6325s). Check your callbacks.\n",
      "184/184 [==============================] - 231s 345ms/step - loss: 0.5723 - prec: 0.2760 - rec: 0.0164 - val_loss: 0.5548 - val_prec: 0.0000e+00 - val_rec: 0.0000e+00\n",
      "Epoch 2/30\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.5456 - prec: 0.3230 - rec: 0.0088 - val_loss: 0.5511 - val_prec: 0.0000e+00 - val_rec: 0.0000e+00\n",
      "Epoch 3/30\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.5409 - prec: 0.4149 - rec: 0.0165 - val_loss: 0.5527 - val_prec: 0.0000e+00 - val_rec: 0.0000e+00\n",
      "Epoch 4/30\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.5348 - prec: 0.4939 - rec: 0.0359 - val_loss: 0.5452 - val_prec: 0.6000 - val_rec: 0.0070\n",
      "Epoch 5/30\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.5223 - prec: 0.5993 - rec: 0.0840 - val_loss: 0.5324 - val_prec: 0.6290 - val_rec: 0.0905\n",
      "Epoch 6/30\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.5004 - prec: 0.6895 - rec: 0.1609 - val_loss: 0.5357 - val_prec: 0.4643 - val_rec: 0.1206\n",
      "Epoch 7/30\n",
      "184/184 [==============================] - 52s 285ms/step - loss: 0.4696 - prec: 0.7262 - rec: 0.2628 - val_loss: 0.4715 - val_prec: 0.8922 - val_rec: 0.2111\n",
      "Epoch 8/30\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.4417 - prec: 0.7473 - rec: 0.3489 - val_loss: 2.1440 - val_prec: 0.2277 - val_rec: 0.5452\n",
      "Epoch 9/30\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.4192 - prec: 0.7731 - rec: 0.4078 - val_loss: 0.4540 - val_prec: 0.6872 - val_rec: 0.2854\n",
      "Epoch 10/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.4020 - prec: 0.7850 - rec: 0.4541 - val_loss: 0.4327 - val_prec: 0.7885 - val_rec: 0.4756\n",
      "Epoch 11/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3889 - prec: 0.7937 - rec: 0.4887 - val_loss: 0.7277 - val_prec: 0.9286 - val_rec: 0.0302\n",
      "Epoch 12/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3784 - prec: 0.7984 - rec: 0.5157 - val_loss: 0.4478 - val_prec: 0.8553 - val_rec: 0.3155\n",
      "Epoch 13/30\n",
      "184/184 [==============================] - 50s 271ms/step - loss: 0.3710 - prec: 0.8018 - rec: 0.5365 - val_loss: 0.3983 - val_prec: 0.8532 - val_rec: 0.4316\n",
      "Epoch 14/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3642 - prec: 0.8094 - rec: 0.5554 - val_loss: 0.4473 - val_prec: 0.9114 - val_rec: 0.3341\n",
      "Epoch 15/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3587 - prec: 0.8112 - rec: 0.5662 - val_loss: 0.3913 - val_prec: 0.8839 - val_rec: 0.4594\n",
      "Epoch 16/30\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.3538 - prec: 0.8145 - rec: 0.5821 - val_loss: 0.3674 - val_prec: 0.8741 - val_rec: 0.5476\n",
      "Epoch 17/30\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.3493 - prec: 0.8178 - rec: 0.5906 - val_loss: 0.3560 - val_prec: 0.8830 - val_rec: 0.5429\n",
      "Epoch 18/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3452 - prec: 0.8250 - rec: 0.5990 - val_loss: 0.3703 - val_prec: 0.8800 - val_rec: 0.5104\n",
      "Epoch 19/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3406 - prec: 0.8280 - rec: 0.6126 - val_loss: 0.3661 - val_prec: 0.8837 - val_rec: 0.5290\n",
      "Epoch 20/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3369 - prec: 0.8297 - rec: 0.6227 - val_loss: 0.4060 - val_prec: 0.8787 - val_rec: 0.4872\n",
      "Epoch 21/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3348 - prec: 0.8282 - rec: 0.6233 - val_loss: 0.3916 - val_prec: 0.8952 - val_rec: 0.5151\n",
      "Epoch 22/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3320 - prec: 0.8319 - rec: 0.6342 - val_loss: 0.3909 - val_prec: 0.9042 - val_rec: 0.5035\n",
      "Epoch 23/30\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.3289 - prec: 0.8363 - rec: 0.6408 - val_loss: 0.3728 - val_prec: 0.9066 - val_rec: 0.5406\n",
      "Epoch 24/30\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.3263 - prec: 0.8353 - rec: 0.6451 - val_loss: 0.4199 - val_prec: 0.9238 - val_rec: 0.4780\n",
      "Epoch 25/30\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.3242 - prec: 0.8384 - rec: 0.6518 - val_loss: 0.3465 - val_prec: 0.8796 - val_rec: 0.5592\n",
      "Epoch 26/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3221 - prec: 0.8383 - rec: 0.6559 - val_loss: 0.3724 - val_prec: 0.9271 - val_rec: 0.5313\n",
      "Epoch 27/30\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.3188 - prec: 0.8414 - rec: 0.6619 - val_loss: 0.3543 - val_prec: 0.8864 - val_rec: 0.5615\n",
      "Epoch 28/30\n",
      "184/184 [==============================] - 50s 271ms/step - loss: 0.3172 - prec: 0.8432 - rec: 0.6669 - val_loss: 0.3715 - val_prec: 0.8810 - val_rec: 0.5499\n",
      "Epoch 29/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3156 - prec: 0.8437 - rec: 0.6684 - val_loss: 0.4174 - val_prec: 0.9008 - val_rec: 0.5058\n",
      "Epoch 30/30\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.3138 - prec: 0.8441 - rec: 0.6760 - val_loss: 0.3519 - val_prec: 0.9205 - val_rec: 0.5638\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/efb0_baseline_128/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/efb0_baseline_128/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path, epoch_number = 31)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(128, 128), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(128, 128), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('EfficientNetB0', (128, 128, 3))\n",
    "model = compile_new_model(model)\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 30,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "  6/184 [..............................] - ETA: 33s - loss: 1.6456 - prec: 0.2234 - rec: 0.5059WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1846s vs `on_train_batch_end` time: 0.5341s). Check your callbacks.\n",
      "184/184 [==============================] - 199s 299ms/step - loss: 0.8470 - prec: 0.3285 - rec: 0.6218 - val_loss: 0.5538 - val_prec: 0.0000e+00 - val_rec: 0.0000e+00\n",
      "Epoch 2/50\n",
      "184/184 [==============================] - 53s 288ms/step - loss: 0.5924 - prec: 0.4475 - rec: 0.7401 - val_loss: 0.5971 - val_prec: 0.3091 - val_rec: 0.3063\n",
      "Epoch 3/50\n",
      "184/184 [==============================] - 51s 278ms/step - loss: 0.5185 - prec: 0.5001 - rec: 0.7813 - val_loss: 0.7650 - val_prec: 0.3998 - val_rec: 0.7773\n",
      "Epoch 4/50\n",
      "184/184 [==============================] - 51s 279ms/step - loss: 0.4805 - prec: 0.5297 - rec: 0.8003 - val_loss: 0.5128 - val_prec: 0.4804 - val_rec: 0.7981\n",
      "Epoch 5/50\n",
      "184/184 [==============================] - 51s 278ms/step - loss: 0.4547 - prec: 0.5564 - rec: 0.8127 - val_loss: 0.4341 - val_prec: 0.6090 - val_rec: 0.7193\n",
      "Epoch 6/50\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.4350 - prec: 0.5781 - rec: 0.8199 - val_loss: 0.4608 - val_prec: 0.5681 - val_rec: 0.7935\n",
      "Epoch 7/50\n",
      "184/184 [==============================] - 51s 278ms/step - loss: 0.4173 - prec: 0.5976 - rec: 0.8278 - val_loss: 0.5625 - val_prec: 0.4711 - val_rec: 0.8144\n",
      "Epoch 8/50\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.4011 - prec: 0.6151 - rec: 0.8368 - val_loss: 0.4306 - val_prec: 0.6842 - val_rec: 0.6334\n",
      "Epoch 9/50\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.3863 - prec: 0.6349 - rec: 0.8441 - val_loss: 0.4726 - val_prec: 0.5250 - val_rec: 0.8538\n",
      "Epoch 10/50\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.3703 - prec: 0.6507 - rec: 0.8520 - val_loss: 0.4852 - val_prec: 0.5530 - val_rec: 0.8237\n",
      "Epoch 11/50\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.3597 - prec: 0.6644 - rec: 0.8638 - val_loss: 0.3347 - val_prec: 0.7117 - val_rec: 0.8306\n",
      "Epoch 12/50\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.3453 - prec: 0.6809 - rec: 0.8731 - val_loss: 0.3583 - val_prec: 0.7584 - val_rec: 0.7355\n",
      "Epoch 13/50\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.3325 - prec: 0.6933 - rec: 0.8818 - val_loss: 0.3216 - val_prec: 0.7289 - val_rec: 0.8422\n",
      "Epoch 14/50\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.3241 - prec: 0.7037 - rec: 0.8881 - val_loss: 0.3434 - val_prec: 0.7212 - val_rec: 0.8701\n",
      "Epoch 15/50\n",
      "184/184 [==============================] - 51s 280ms/step - loss: 0.3164 - prec: 0.7087 - rec: 0.8951 - val_loss: 0.2852 - val_prec: 0.7891 - val_rec: 0.8422\n",
      "Epoch 16/50\n",
      "184/184 [==============================] - 51s 279ms/step - loss: 0.3082 - prec: 0.7182 - rec: 0.8999 - val_loss: 0.3158 - val_prec: 0.6880 - val_rec: 0.8492\n",
      "Epoch 17/50\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3036 - prec: 0.7218 - rec: 0.9028 - val_loss: 0.2994 - val_prec: 0.7579 - val_rec: 0.8933\n",
      "Epoch 18/50\n",
      "184/184 [==============================] - 51s 278ms/step - loss: 0.2976 - prec: 0.7299 - rec: 0.9072 - val_loss: 0.3530 - val_prec: 0.6429 - val_rec: 0.8979\n",
      "Epoch 19/50\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.2904 - prec: 0.7358 - rec: 0.9102 - val_loss: 0.2748 - val_prec: 0.7905 - val_rec: 0.8840\n",
      "Epoch 20/50\n",
      "184/184 [==============================] - 51s 279ms/step - loss: 0.2875 - prec: 0.7385 - rec: 0.9125 - val_loss: 0.3574 - val_prec: 0.6164 - val_rec: 0.9582\n",
      "Epoch 21/50\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.2835 - prec: 0.7423 - rec: 0.9158 - val_loss: 0.3304 - val_prec: 0.7038 - val_rec: 0.8933\n",
      "Epoch 22/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2788 - prec: 0.7494 - rec: 0.9177 - val_loss: 0.2619 - val_prec: 0.8427 - val_rec: 0.8329\n",
      "Epoch 23/50\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.2752 - prec: 0.7530 - rec: 0.9206 - val_loss: 0.2967 - val_prec: 0.7634 - val_rec: 0.8608\n",
      "Epoch 24/50\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2715 - prec: 0.7564 - rec: 0.9229 - val_loss: 0.3086 - val_prec: 0.8422 - val_rec: 0.8051\n",
      "Epoch 25/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2698 - prec: 0.7606 - rec: 0.9243 - val_loss: 0.3166 - val_prec: 0.7156 - val_rec: 0.9165\n",
      "Epoch 26/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2643 - prec: 0.7661 - rec: 0.9288 - val_loss: 0.2837 - val_prec: 0.8086 - val_rec: 0.8329\n",
      "Epoch 27/50\n",
      "184/184 [==============================] - 50s 271ms/step - loss: 0.2613 - prec: 0.7690 - rec: 0.9299 - val_loss: 0.2847 - val_prec: 0.7754 - val_rec: 0.8492\n",
      "Epoch 28/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2589 - prec: 0.7730 - rec: 0.9300 - val_loss: 0.3291 - val_prec: 0.6738 - val_rec: 0.9536\n",
      "Epoch 29/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2583 - prec: 0.7717 - rec: 0.9315 - val_loss: 0.3137 - val_prec: 0.7547 - val_rec: 0.8353\n",
      "Epoch 30/50\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.2545 - prec: 0.7788 - rec: 0.9316 - val_loss: 0.2948 - val_prec: 0.8027 - val_rec: 0.8306\n",
      "Epoch 31/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2523 - prec: 0.7797 - rec: 0.9347 - val_loss: 0.2616 - val_prec: 0.8292 - val_rec: 0.8561\n",
      "Epoch 32/50\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.2504 - prec: 0.7834 - rec: 0.9358 - val_loss: 0.2787 - val_prec: 0.7572 - val_rec: 0.9188\n",
      "Epoch 33/50\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.2477 - prec: 0.7873 - rec: 0.9373 - val_loss: 0.2735 - val_prec: 0.7882 - val_rec: 0.8979\n",
      "Epoch 34/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2450 - prec: 0.7919 - rec: 0.9389 - val_loss: 0.2578 - val_prec: 0.8054 - val_rec: 0.8933\n",
      "Epoch 35/50\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2419 - prec: 0.7950 - rec: 0.9410 - val_loss: 0.3959 - val_prec: 0.6072 - val_rec: 0.9397\n",
      "Epoch 36/50\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2419 - prec: 0.7957 - rec: 0.9412 - val_loss: 0.3125 - val_prec: 0.7654 - val_rec: 0.8631\n",
      "Epoch 37/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2407 - prec: 0.7966 - rec: 0.9412 - val_loss: 0.2691 - val_prec: 0.8395 - val_rec: 0.8376\n",
      "Epoch 38/50\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.2384 - prec: 0.8004 - rec: 0.9434 - val_loss: 0.2713 - val_prec: 0.8226 - val_rec: 0.8608\n",
      "Epoch 39/50\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.2355 - prec: 0.8035 - rec: 0.9445 - val_loss: 0.3364 - val_prec: 0.6787 - val_rec: 0.9606\n",
      "Epoch 40/50\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.2325 - prec: 0.8089 - rec: 0.9473 - val_loss: 0.2640 - val_prec: 0.8992 - val_rec: 0.7657\n",
      "Epoch 41/50\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2302 - prec: 0.8109 - rec: 0.9472 - val_loss: 0.2457 - val_prec: 0.8462 - val_rec: 0.8933\n",
      "Epoch 42/50\n",
      "184/184 [==============================] - 50s 270ms/step - loss: 0.2299 - prec: 0.8111 - rec: 0.9478 - val_loss: 0.2523 - val_prec: 0.8535 - val_rec: 0.8654\n",
      "Epoch 43/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2269 - prec: 0.8164 - rec: 0.9502 - val_loss: 0.2438 - val_prec: 0.8337 - val_rec: 0.8840\n",
      "Epoch 44/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2254 - prec: 0.8207 - rec: 0.9506 - val_loss: 0.2505 - val_prec: 0.8601 - val_rec: 0.8701\n",
      "Epoch 45/50\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.2250 - prec: 0.8208 - rec: 0.9497 - val_loss: 0.2875 - val_prec: 0.7543 - val_rec: 0.9258\n",
      "Epoch 46/50\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2226 - prec: 0.8226 - rec: 0.9523 - val_loss: 0.2850 - val_prec: 0.8208 - val_rec: 0.8074\n",
      "Epoch 47/50\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.2202 - prec: 0.8284 - rec: 0.9534 - val_loss: 0.3302 - val_prec: 0.7180 - val_rec: 0.8979\n",
      "Epoch 48/50\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2177 - prec: 0.8318 - rec: 0.9530 - val_loss: 0.3004 - val_prec: 0.7610 - val_rec: 0.8422\n",
      "Epoch 49/50\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2161 - prec: 0.8330 - rec: 0.9562 - val_loss: 0.2688 - val_prec: 0.7908 - val_rec: 0.8770\n",
      "Epoch 50/50\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.2155 - prec: 0.8352 - rec: 0.9550 - val_loss: 0.3200 - val_prec: 0.7589 - val_rec: 0.9350\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/res50_baseline_128_class_weights/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/res50_baseline_128_class_weights/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path, epoch_number = 31)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(128, 128), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(128, 128), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (128, 128, 3))\n",
    "model = compile_new_model(model)\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 50,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    "    class_weight = class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "  6/184 [..............................] - ETA: 33s - loss: 0.2177 - prec: 0.8299 - rec: 0.9558WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1812s vs `on_train_batch_end` time: 0.5564s). Check your callbacks.\n",
      "184/184 [==============================] - 175s 291ms/step - loss: 0.2135 - prec: 0.8382 - rec: 0.9565 - val_loss: 0.2470 - val_prec: 0.8762 - val_rec: 0.8376\n",
      "Epoch 2/100\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.2114 - prec: 0.8421 - rec: 0.9567 - val_loss: 0.2808 - val_prec: 0.7840 - val_rec: 0.8840\n",
      "Epoch 3/100\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.2091 - prec: 0.8448 - rec: 0.9584 - val_loss: 0.2501 - val_prec: 0.8555 - val_rec: 0.8654\n",
      "Epoch 4/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2065 - prec: 0.8488 - rec: 0.9603 - val_loss: 0.2709 - val_prec: 0.8017 - val_rec: 0.8724\n",
      "Epoch 5/100\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2059 - prec: 0.8511 - rec: 0.9600 - val_loss: 0.3113 - val_prec: 0.7321 - val_rec: 0.9258\n",
      "Epoch 6/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.2040 - prec: 0.8521 - rec: 0.9612 - val_loss: 0.2645 - val_prec: 0.8668 - val_rec: 0.8005\n",
      "Epoch 7/100\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2037 - prec: 0.8543 - rec: 0.9608 - val_loss: 0.2854 - val_prec: 0.7366 - val_rec: 0.9281\n",
      "Epoch 8/100\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.2011 - prec: 0.8586 - rec: 0.9625 - val_loss: 0.2726 - val_prec: 0.7778 - val_rec: 0.8933\n",
      "Epoch 9/100\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.1992 - prec: 0.8602 - rec: 0.9639 - val_loss: 0.2441 - val_prec: 0.8637 - val_rec: 0.9118\n",
      "Epoch 10/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1973 - prec: 0.8646 - rec: 0.9642 - val_loss: 0.2574 - val_prec: 0.8637 - val_rec: 0.8237\n",
      "Epoch 11/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1957 - prec: 0.8673 - rec: 0.9660 - val_loss: 0.2432 - val_prec: 0.8732 - val_rec: 0.8631\n",
      "Epoch 12/100\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.1936 - prec: 0.8713 - rec: 0.9670 - val_loss: 0.2668 - val_prec: 0.8843 - val_rec: 0.8515\n",
      "Epoch 13/100\n",
      "184/184 [==============================] - 51s 278ms/step - loss: 0.1925 - prec: 0.8725 - rec: 0.9657 - val_loss: 0.2666 - val_prec: 0.8682 - val_rec: 0.8097\n",
      "Epoch 14/100\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.1922 - prec: 0.8745 - rec: 0.9662 - val_loss: 0.2595 - val_prec: 0.8458 - val_rec: 0.8399\n",
      "Epoch 15/100\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.1904 - prec: 0.8789 - rec: 0.9673 - val_loss: 0.2374 - val_prec: 0.8555 - val_rec: 0.8794\n",
      "Epoch 16/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.1878 - prec: 0.8805 - rec: 0.9690 - val_loss: 0.2387 - val_prec: 0.8604 - val_rec: 0.8863\n",
      "Epoch 17/100\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.1862 - prec: 0.8846 - rec: 0.9690 - val_loss: 0.2645 - val_prec: 0.8430 - val_rec: 0.8724\n",
      "Epoch 18/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1860 - prec: 0.8855 - rec: 0.9690 - val_loss: 0.2548 - val_prec: 0.8897 - val_rec: 0.8237\n",
      "Epoch 19/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1851 - prec: 0.8884 - rec: 0.9698 - val_loss: 0.2520 - val_prec: 0.8555 - val_rec: 0.8515\n",
      "Epoch 20/100\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.1835 - prec: 0.8897 - rec: 0.9709 - val_loss: 0.2947 - val_prec: 0.7591 - val_rec: 0.9211\n",
      "Epoch 21/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1832 - prec: 0.8899 - rec: 0.9705 - val_loss: 0.2450 - val_prec: 0.8174 - val_rec: 0.9142\n",
      "Epoch 22/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1813 - prec: 0.8939 - rec: 0.9713 - val_loss: 0.2467 - val_prec: 0.8510 - val_rec: 0.8747\n",
      "Epoch 23/100\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.1796 - prec: 0.8971 - rec: 0.9726 - val_loss: 0.2725 - val_prec: 0.7952 - val_rec: 0.9281\n",
      "Epoch 24/100\n",
      "184/184 [==============================] - 50s 270ms/step - loss: 0.1785 - prec: 0.8993 - rec: 0.9734 - val_loss: 0.2791 - val_prec: 0.7784 - val_rec: 0.9374\n",
      "Epoch 25/100\n",
      "184/184 [==============================] - 50s 271ms/step - loss: 0.1762 - prec: 0.9030 - rec: 0.9744 - val_loss: 0.2429 - val_prec: 0.8509 - val_rec: 0.9002\n",
      "Epoch 26/100\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.1741 - prec: 0.9073 - rec: 0.9754 - val_loss: 0.2878 - val_prec: 0.8399 - val_rec: 0.7912\n",
      "Epoch 27/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1752 - prec: 0.9069 - rec: 0.9744 - val_loss: 0.2582 - val_prec: 0.8326 - val_rec: 0.9118\n",
      "Epoch 28/100\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.1745 - prec: 0.9082 - rec: 0.9745 - val_loss: 0.2787 - val_prec: 0.8330 - val_rec: 0.8561\n",
      "Epoch 29/100\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.1725 - prec: 0.9101 - rec: 0.9758 - val_loss: 0.2874 - val_prec: 0.7816 - val_rec: 0.9466\n",
      "Epoch 30/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1701 - prec: 0.9138 - rec: 0.9770 - val_loss: 0.2682 - val_prec: 0.8070 - val_rec: 0.9118\n",
      "Epoch 31/100\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.1704 - prec: 0.9139 - rec: 0.9768 - val_loss: 0.2846 - val_prec: 0.8174 - val_rec: 0.8515\n",
      "Epoch 32/100\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.1693 - prec: 0.9164 - rec: 0.9766 - val_loss: 0.2475 - val_prec: 0.8531 - val_rec: 0.9026\n",
      "Epoch 33/100\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.1673 - prec: 0.9181 - rec: 0.9787 - val_loss: 0.2521 - val_prec: 0.8841 - val_rec: 0.8492\n",
      "Epoch 34/100\n",
      "184/184 [==============================] - 54s 292ms/step - loss: 0.1679 - prec: 0.9191 - rec: 0.9778 - val_loss: 0.2560 - val_prec: 0.8371 - val_rec: 0.8585\n",
      "Epoch 35/100\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.1651 - prec: 0.9237 - rec: 0.9791 - val_loss: 0.2525 - val_prec: 0.8581 - val_rec: 0.8701\n",
      "Epoch 36/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.1655 - prec: 0.9224 - rec: 0.9790 - val_loss: 0.2539 - val_prec: 0.8269 - val_rec: 0.8979\n",
      "Epoch 37/100\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.1639 - prec: 0.9251 - rec: 0.9799 - val_loss: 0.2933 - val_prec: 0.8238 - val_rec: 0.8677\n",
      "Epoch 38/100\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.1631 - prec: 0.9270 - rec: 0.9797 - val_loss: 0.2639 - val_prec: 0.8808 - val_rec: 0.8399\n",
      "Epoch 39/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.1627 - prec: 0.9288 - rec: 0.9798 - val_loss: 0.2725 - val_prec: 0.8510 - val_rec: 0.8747\n",
      "Epoch 40/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.1612 - prec: 0.9310 - rec: 0.9818 - val_loss: 0.2799 - val_prec: 0.7906 - val_rec: 0.9374\n",
      "Epoch 41/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1605 - prec: 0.9314 - rec: 0.9815 - val_loss: 0.3146 - val_prec: 0.7715 - val_rec: 0.9165\n",
      "Epoch 42/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.1598 - prec: 0.9333 - rec: 0.9815 - val_loss: 0.2990 - val_prec: 0.9308 - val_rec: 0.7494\n",
      "Epoch 43/100\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.1591 - prec: 0.9353 - rec: 0.9830 - val_loss: 0.2585 - val_prec: 0.8442 - val_rec: 0.8677\n",
      "Epoch 44/100\n",
      "184/184 [==============================] - 51s 274ms/step - loss: 0.1583 - prec: 0.9370 - rec: 0.9818 - val_loss: 0.2822 - val_prec: 0.7771 - val_rec: 0.9304\n",
      "Epoch 45/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.1587 - prec: 0.9358 - rec: 0.9820 - val_loss: 0.2708 - val_prec: 0.8157 - val_rec: 0.9142\n",
      "Epoch 46/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.1578 - prec: 0.9373 - rec: 0.9826 - val_loss: 0.2733 - val_prec: 0.9150 - val_rec: 0.7494\n",
      "Epoch 47/100\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.1554 - prec: 0.9414 - rec: 0.9832 - val_loss: 0.2722 - val_prec: 0.8980 - val_rec: 0.8167\n",
      "Epoch 48/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1561 - prec: 0.9389 - rec: 0.9838 - val_loss: 0.2484 - val_prec: 0.8916 - val_rec: 0.8585\n",
      "Epoch 49/100\n",
      "184/184 [==============================] - 50s 271ms/step - loss: 0.1550 - prec: 0.9430 - rec: 0.9837 - val_loss: 0.2533 - val_prec: 0.8679 - val_rec: 0.8840\n",
      "Epoch 50/100\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.1534 - prec: 0.9461 - rec: 0.9842 - val_loss: 0.2546 - val_prec: 0.8467 - val_rec: 0.8840\n",
      "Epoch 51/100\n",
      "184/184 [==============================] - 50s 271ms/step - loss: 0.1539 - prec: 0.9447 - rec: 0.9842 - val_loss: 0.2905 - val_prec: 0.9366 - val_rec: 0.7541\n",
      "Epoch 52/100\n",
      "184/184 [==============================] - 50s 271ms/step - loss: 0.1518 - prec: 0.9485 - rec: 0.9858 - val_loss: 0.2485 - val_prec: 0.8485 - val_rec: 0.9095\n",
      "Epoch 53/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.1530 - prec: 0.9466 - rec: 0.9846 - val_loss: 0.2676 - val_prec: 0.8732 - val_rec: 0.8631\n",
      "Epoch 54/100\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.1508 - prec: 0.9492 - rec: 0.9866 - val_loss: 0.2397 - val_prec: 0.8613 - val_rec: 0.8933\n",
      "Epoch 55/100\n",
      " 89/184 [=============>................] - ETA: 24s - loss: 0.1490 - prec: 0.9515 - rec: 0.9876"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m model \u001b[39m=\u001b[39m compile_new_model(model)\n\u001b[1;32m     13\u001b[0m model\u001b[39m.\u001b[39mload_weights(\u001b[39m'\u001b[39m\u001b[39m/home/ubuntu/ml-data-training/ship_seg_weights/classification/res50_baseline_128_class_weights/81.h5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m model_hist \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     15\u001b[0m     train_dataset,\n\u001b[1;32m     16\u001b[0m     validation_data \u001b[39m=\u001b[39;49m val_dataset,\n\u001b[1;32m     17\u001b[0m     verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m     18\u001b[0m     epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m     steps_per_epoch \u001b[39m=\u001b[39;49m \u001b[39mlen\u001b[39;49m(train) \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m (batch_size \u001b[39m*\u001b[39;49m REPLICAS),\n\u001b[1;32m     20\u001b[0m     callbacks \u001b[39m=\u001b[39;49m [\n\u001b[1;32m     21\u001b[0m         tensorboard_callback,\n\u001b[1;32m     22\u001b[0m         weights_save\n\u001b[1;32m     23\u001b[0m     ],\n\u001b[1;32m     24\u001b[0m     class_weight \u001b[39m=\u001b[39;49m class_weight_dict\n\u001b[1;32m     25\u001b[0m )\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/res50_baseline_128_class_weights_v1/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/res50_baseline_128_class_weights/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path, epoch_number = 83)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(128, 128), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(128, 128), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (128, 128, 3))\n",
    "model = compile_new_model(model)\n",
    "model.load_weights('/home/ubuntu/ml-data-training/ship_seg_weights/classification/res50_baseline_128_class_weights/81.h5')\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 100,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    "    class_weight = class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(128, 128), batch_size = batch_size, repeat=False, shuffle=False)\n",
    "val_dataset = get_data(val, shape=(128, 128), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (128, 128, 3))\n",
    "model = compile_new_model(model)\n",
    "model.load_weights('/home/ubuntu/ml-data-training/ship_seg_weights/classification/res50_baseline_128_class_weights/72.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 49s 128ms/step - loss: 0.2116 - prec: 0.8636 - rec: 0.9333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21162523329257965, 0.8636000752449036, 0.9333317279815674]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 883ms/step - loss: 0.2455 - prec: 0.8421 - rec: 0.8910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24545831978321075, 0.8421052694320679, 0.8909512758255005]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = get_data(test, shape=(128, 128), repeat = False, shuffle = False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 853ms/step - loss: 0.2330 - prec: 0.8410 - rec: 0.9061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23297208547592163, 0.8409585952758789, 0.9061033129692078]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/test_env/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "val_imgs = val_dataset.map(lambda x, y: x)\n",
    "val_labels = val_dataset.map(lambda x, y: y)\n",
    "test_imgs = test_dataset.map(lambda x, y: x)\n",
    "test_labels = test_dataset.map(lambda x, y: y)\n",
    "train_imgs = train_dataset.map(lambda x, y: x)\n",
    "train_labels = train_dataset.map(lambda x, y: y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_g_truth = np.array(val['class_labels'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 13s 299ms/step\n",
      "2/2 [==============================] - 1s 303ms/step\n",
      "185/185 [==============================] - 19s 96ms/step\n"
     ]
    }
   ],
   "source": [
    "val_preds = model.predict(val_imgs, verbose=1)\n",
    "test_preds = model.predict(test_imgs, verbose=1)\n",
    "train_preds = model.predict(train_imgs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = val_preds.flatten()\n",
    "test_preds = test_preds.flatten()\n",
    "train_preds = train_preds.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_0_5 = np.where(val_preds > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_index = np.where(np.logical_and(val_g_truth == 0, pred_0_5 == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_index = np.where(np.logical_and(val_g_truth == 1, pred_0_5 == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  18,   41,   92,  112,  126,  135,  150,  187,  239,  241,  252,\n",
       "         301,  306,  362,  366,  403,  407,  419,  459,  548,  552,  560,\n",
       "         561,  593,  605,  680,  749,  775,  797,  816,  838,  859,  861,\n",
       "         872,  940,  960,  965,  978, 1019, 1029, 1089, 1097, 1108, 1155,\n",
       "        1169, 1206, 1262, 1286, 1419, 1435, 1444, 1532, 1563, 1584, 1646,\n",
       "        1669, 1693, 1694, 1701, 1710, 1713, 1737, 1747, 1754, 1805, 1819,\n",
       "        1822, 1842, 1845, 1874, 1875, 1884]),)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for x in fp_index[0]:\n",
    "fp_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  95,  145,  213,  223,  254,  263,  275,  280,  340,  344,  377,\n",
       "         429,  466,  540,  574,  591,  608,  612,  671,  686,  736,  777,\n",
       "         780,  821,  934, 1013, 1028, 1067, 1085, 1094, 1171, 1185, 1187,\n",
       "        1240, 1245, 1259, 1298, 1308, 1312, 1353, 1396, 1423, 1437, 1633,\n",
       "        1773, 1791, 1810]),)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = val['fixed_paths'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['class_labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.406096, G-Mean=0.948\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(train['class_labels'].values, train_preds)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = np.argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "pyplot.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0253925 , 0.03287205, 0.02053146, ..., 0.9827223 , 0.9868522 ,\n",
       "       0.9888123 ], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.840958605664488\n",
      "Recall: 0.9061032863849765\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision: {precision_score(test['class_labels'].values, np.where(test_preds > 0.5, 1, 0))}\")\n",
    "print(f\"Recall: {recall_score(test['class_labels'].values, np.where(test_preds > 0.5, 1, 0))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8301125193457983\n",
      "Recall: 0.9518453679944363\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision: {precision_score(train['class_labels'].values, np.where(train_preds > 0.406096, 1, 0))}\")\n",
    "print(f\"Recall: {recall_score(train['class_labels'].values, np.where(train_preds > 0.406096, 1, 0))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('../saved_model_128/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_model = tfmot.quantization.keras.quantize_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 2049      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,589,761\n",
      "Trainable params: 23,536,641\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50weights = model.get_layer('resnet50').get_weights()\n",
    "sigmoid_weights = model.get_layer('dense').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ = keras.applications.ResNet50(include_top = False,\n",
    "    input_shape = (128, 128, 3),\n",
    "    weights = None, \n",
    "    pooling = 'avg')\n",
    "sigmoid_layer = keras.layers.Dense(1, activation = 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_layers = []\n",
    "for x in res_.layers:\n",
    "    res_layers.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x7f2550153640>,\n",
       " <keras.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x7f255061a460>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25a4085f10>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f255062ad30>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25806f4370>,\n",
       " <keras.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x7f25a4083460>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f25806d3ac0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f258823f400>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25503ff4c0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25503ffd90>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f2550600310>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f258060b460>,\n",
       " <keras.layers.core.activation.Activation at 0x7f2580608310>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25503ffe80>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25805fda00>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25503ffa00>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25805fd370>,\n",
       " <keras.layers.merging.add.Add at 0x7f25805fdca0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f2580630bb0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25805fd3a0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f2580630ca0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f2580631310>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25806311f0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f2580631a00>,\n",
       " <keras.layers.core.activation.Activation at 0x7f258060b7c0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25503ffd00>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f2580617f10>,\n",
       " <keras.layers.merging.add.Add at 0x7f25886d6490>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25a40ab8e0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25506176a0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f2550617ac0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f2580588eb0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f2580588e50>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f2550412b80>,\n",
       " <keras.layers.core.activation.Activation at 0x7f2580588d00>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25507d3e80>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25507d3b20>,\n",
       " <keras.layers.merging.add.Add at 0x7f25507f3460>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25507f3700>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25507ce250>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25507e01c0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25507ce580>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25507d03a0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25507ba970>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25507f39d0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25507f3640>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25507d02e0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25507f3d90>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25507c87c0>,\n",
       " <keras.layers.merging.add.Add at 0x7f25507c8460>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25507c8790>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25507d1dc0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25507baf70>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25507c8520>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25507d1040>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25507bebe0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25507cc580>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25507d1fa0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25507d1070>,\n",
       " <keras.layers.merging.add.Add at 0x7f2588786880>,\n",
       " <keras.layers.core.activation.Activation at 0x7f2588786940>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f258878c400>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f258878c220>,\n",
       " <keras.layers.core.activation.Activation at 0x7f258878ce20>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25887865b0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25507c0a30>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25507d0550>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25507d1f10>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25507d1670>,\n",
       " <keras.layers.merging.add.Add at 0x7f25507d0ee0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25507d3520>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25507c5730>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25507c5490>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25a4083970>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25507c5ca0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25805fd520>,\n",
       " <keras.layers.core.activation.Activation at 0x7f2588335250>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25805fde20>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25507dbb50>,\n",
       " <keras.layers.merging.add.Add at 0x7f25507c1f40>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25887b4340>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25887b08e0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25887a9f10>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25887b04c0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f258869d400>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f258869d1c0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25886843a0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25887b41c0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25886a6070>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25887b4760>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25886b3820>,\n",
       " <keras.layers.merging.add.Add at 0x7f258879a340>,\n",
       " <keras.layers.core.activation.Activation at 0x7f258879a4c0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25886ace20>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25886b37f0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25886b34c0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25886ac070>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f258867fc40>,\n",
       " <keras.layers.core.activation.Activation at 0x7f258867f4c0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25886acfd0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25886acbe0>,\n",
       " <keras.layers.merging.add.Add at 0x7f258867fd00>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25505739a0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f2550570460>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f2550570130>,\n",
       " <keras.layers.core.activation.Activation at 0x7f2550570fd0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f2550573580>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f258869d7f0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f2550570ac0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f258869ef10>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25886b3f70>,\n",
       " <keras.layers.merging.add.Add at 0x7f258869d580>,\n",
       " <keras.layers.core.activation.Activation at 0x7f2588781d00>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25887b4910>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25887b4670>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25507db580>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f2550543100>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25505435e0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f2550543c10>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25505431f0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f258867ba60>,\n",
       " <keras.layers.merging.add.Add at 0x7f2550567040>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25886887c0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f258867b730>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f2588688e50>,\n",
       " <keras.layers.core.activation.Activation at 0x7f2550567220>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f255055f8e0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f255055f2b0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f255055f910>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f2580078e20>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f2580078bb0>,\n",
       " <keras.layers.merging.add.Add at 0x7f2550565940>,\n",
       " <keras.layers.core.activation.Activation at 0x7f258003b790>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f258003beb0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f258003be20>,\n",
       " <keras.layers.core.activation.Activation at 0x7f2550542fa0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25800535b0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f2580053b80>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25800482b0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f258006bc40>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f258006b9d0>,\n",
       " <keras.layers.merging.add.Add at 0x7f25801cc340>,\n",
       " <keras.layers.core.activation.Activation at 0x7f258003b6d0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25801dd7c0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25801dd2e0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25801d73d0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f258003b850>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f2580050a30>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25801ddf40>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25801cc8b0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f258003bb20>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25801d1820>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25507c0d30>,\n",
       " <keras.layers.merging.add.Add at 0x7f25505678e0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f2550567a00>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25507ba820>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25507db910>,\n",
       " <keras.layers.core.activation.Activation at 0x7f255055f4c0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25886ac670>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25886881c0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25886b5a90>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f2580048ac0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25800488b0>,\n",
       " <keras.layers.merging.add.Add at 0x7f2580070610>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25505479a0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25801be730>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25801beaf0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25801e8fa0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f2580044460>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f2580070f70>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25801bed00>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f25801b98b0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f25801b9d90>,\n",
       " <keras.layers.merging.add.Add at 0x7f25801cb640>,\n",
       " <keras.layers.core.activation.Activation at 0x7f25801f1dc0>,\n",
       " <keras.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x7f258867bee0>]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer \"dense_6\" with a weight list of length 2, but the layer was expecting 0 weights. Provided weights: [array([[ 0.03903475],\n       [ 0.0230649 ],\n     ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m res_\u001b[39m.\u001b[39mset_weights(res50weights)\n\u001b[0;32m----> 2\u001b[0m sigmoid_layer\u001b[39m.\u001b[39;49mset_weights(sigmoid_weights)\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/keras/engine/base_layer.py:1784\u001b[0m, in \u001b[0;36mLayer.set_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1781\u001b[0m         expected_num_weights \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1783\u001b[0m \u001b[39mif\u001b[39;00m expected_num_weights \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(weights):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1785\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mYou called `set_weights(weights)` on layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1786\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwith a weight list of length \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, but the layer was \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1787\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mexpecting \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m weights. Provided weights: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1788\u001b[0m         \u001b[39m%\u001b[39m (\n\u001b[1;32m   1789\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m   1790\u001b[0m             \u001b[39mlen\u001b[39m(weights),\n\u001b[1;32m   1791\u001b[0m             expected_num_weights,\n\u001b[1;32m   1792\u001b[0m             \u001b[39mstr\u001b[39m(weights)[:\u001b[39m50\u001b[39m],\n\u001b[1;32m   1793\u001b[0m         )\n\u001b[1;32m   1794\u001b[0m     )\n\u001b[1;32m   1796\u001b[0m weight_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   1797\u001b[0m weight_value_tuples \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: You called `set_weights(weights)` on layer \"dense_6\" with a weight list of length 2, but the layer was expecting 0 weights. Provided weights: [array([[ 0.03903475],\n       [ 0.0230649 ],\n     ..."
     ]
    }
   ],
   "source": [
    "res_.set_weights(res50weights)\n",
    "sigmoid_layer.set_weights(sigmoid_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.io.read_file(train['fixed_paths'].values.tolist()[0])\n",
    "img = tf.image.decode_jpeg(img, channels=3)\n",
    "img = tf.image.resize(img, size = (128, 128))\n",
    "img = img / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_infer(data_path):\n",
    "    img = tf.io.read_file(data_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, size = (128, 128))\n",
    "    img = img / 255\n",
    "    temp = res_.predict(img.numpy().reshape(1, 128, 128, 3), verbose=0)\n",
    "    preds = sigmoid(temp)\n",
    "    return preds\n",
    "\n",
    "def actual_model(data_path):\n",
    "    img = tf.io.read_file(data_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, size = (128, 128))\n",
    "    img = img / 255\n",
    "    # temp = res_.predict(img.numpy().reshape(1, 128, 128, 3), verbose=0)\n",
    "    preds = model.predict(img.numpy().reshape(1, 128, 128, 3), verbose=0)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:59<00:00,  8.36it/s]\n"
     ]
    }
   ],
   "source": [
    "custom_preds = []\n",
    "for x in tqdm(train['fixed_paths'].values.tolist()[:1000]):\n",
    "    custom_preds.append(custom_infer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [08:57<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "actual_preds = []\n",
    "for x in tqdm(train['fixed_paths'].values.tolist()[:1000]):\n",
    "    actual_preds.append(actual_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.11793849]], dtype=float32),\n",
       " array([[0.04476193]], dtype=float32),\n",
       " array([[0.38285363]], dtype=float32),\n",
       " array([[0.06439063]], dtype=float32),\n",
       " array([[0.03639622]], dtype=float32),\n",
       " array([[0.03237434]], dtype=float32),\n",
       " array([[0.9843277]], dtype=float32),\n",
       " array([[0.03357893]], dtype=float32),\n",
       " array([[0.0446183]], dtype=float32),\n",
       " array([[0.03253573]], dtype=float32)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.7750413]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.8023023]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.74180204]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.7893914]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.8325746]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.9721983]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.97698385]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.76763284]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.7891883]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.8371881]], dtype=float32)>]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
