{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import glob\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow.keras.backend as K\n",
    "sys.path.append('../scripts/helper_functions_cv/tensorflow_helpers/')\n",
    "from save_weights_every_epoch import CallbackForSavingModelWeights\n",
    "from multiprocessing import Pool\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "import multiprocessing as mp\n",
    "sys.path.append('../scripts/')\n",
    "from find_bad_ones import find_bad_ones\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "allowed_gpus = [0]\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "final_gpu_list = [gpus[x] for x in allowed_gpus]\n",
    "tf.config.set_visible_devices(final_gpu_list, \"GPU\")\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(REPLICAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('../config_files/dev.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data.pickle', 'rb') as handle:\n",
    "    updated_train_csv = pickle.load(handle)\n",
    "\n",
    "def split_datasets(data, test_size = 0.01):\n",
    "    train, test = train_test_split(data, test_size = test_size, random_state = 42) \n",
    "    train, val = train_test_split(train, test_size = test_size, random_state = 42)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = split_datasets(updated_train_csv)\n",
    "\n",
    "train_labels = train['class_labels'].values.tolist()\n",
    "computed = compute_class_weight(class_weight='balanced', classes=[0, 1], y=train_labels)\n",
    "class_weight_dict = {\n",
    "    0: computed[0],\n",
    "    1: computed[1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_imgs(img, label, shape):\n",
    "    img = tf.io.read_file(img)\n",
    "    img = tf.image.decode_jpeg(img, channels = 3)\n",
    "    img = tf.image.resize(img, size = shape)\n",
    "    img = img / 255\n",
    "    return img, label\n",
    "\n",
    "def get_data(data, shape = (256, 256), shuffle = True, repeat = True, batch = True, batch_size = 32):\n",
    "    imgs, labels = data['fixed_paths'].values.tolist(), data['class_labels'].values.tolist()\n",
    "    shapes = [shape for x in range(len(imgs))]\n",
    "    tensor = tf.data.Dataset.from_tensor_slices((imgs, labels, shapes))\n",
    "    tensor = tensor.cache()\n",
    "    if repeat:\n",
    "        tensor = tensor.repeat()\n",
    "    if shuffle:\n",
    "        tensor = tensor.shuffle(8048 * 1)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        tensor = tensor.with_options(opt)\n",
    "    tensor = tensor.map(read_train_imgs)\n",
    "    if batch:\n",
    "        tensor = tensor.batch(batch_size * REPLICAS)\n",
    "    tensor = tensor.prefetch(AUTO)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name, shape):\n",
    "    with strategy.scope():\n",
    "        input_layer = tf.keras.Input(shape = shape)\n",
    "        construct = getattr(keras.applications, model_name)\n",
    "        mid_layer = construct(include_top = False, \n",
    "                            weights = None, \n",
    "                            pooling = 'avg')(input_layer)\n",
    "        last_layer = keras.layers.Dense(1, activation = 'sigmoid')(mid_layer)\n",
    "        model = keras.Model(input_layer, last_layer)\n",
    "    return model\n",
    "def compile_new_model(model):\n",
    "    with strategy.scope():\n",
    "        loss = keras.losses.BinaryCrossentropy(label_smoothing=0.05)\n",
    "        optimizer = keras.optimizers.SGD()\n",
    "        prec = keras.metrics.Precision(name = 'prec')\n",
    "        rec = keras.metrics.Recall(name = 'rec')\n",
    "        model.compile(\n",
    "            loss = loss,\n",
    "            optimizer = optimizer,\n",
    "            metrics = [prec, rec]\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/ship_segmentation/TB/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['tb_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "  6/184 [..............................] - ETA: 30s - loss: 0.9848 - prec: 0.2471 - rec: 0.2533WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1672s vs `on_train_batch_end` time: 0.4839s). Check your callbacks.\n",
      "184/184 [==============================] - 185s 259ms/step - loss: 0.6816 - prec: 0.3246 - rec: 0.1562 - val_loss: 0.5668 - val_prec: 0.0000e+00 - val_rec: 0.0000e+00\n",
      "Epoch 2/30\n",
      "184/184 [==============================] - 39s 210ms/step - loss: 0.5519 - prec: 0.4758 - rec: 0.2385 - val_loss: 0.5562 - val_prec: 0.5000 - val_rec: 0.0023\n",
      "Epoch 3/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.5122 - prec: 0.5281 - rec: 0.2991 - val_loss: 0.5565 - val_prec: 0.6588 - val_rec: 0.1299\n",
      "Epoch 4/30\n",
      "184/184 [==============================] - 38s 208ms/step - loss: 0.4811 - prec: 0.5765 - rec: 0.3441 - val_loss: 0.4640 - val_prec: 0.5734 - val_rec: 0.3898\n",
      "Epoch 5/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.4670 - prec: 0.5966 - rec: 0.3887 - val_loss: 0.4673 - val_prec: 0.6364 - val_rec: 0.3573\n",
      "Epoch 6/30\n",
      "184/184 [==============================] - 39s 211ms/step - loss: 0.4503 - prec: 0.6151 - rec: 0.4162 - val_loss: 0.5010 - val_prec: 0.6859 - val_rec: 0.2483\n",
      "Epoch 7/30\n",
      "184/184 [==============================] - 39s 211ms/step - loss: 0.4388 - prec: 0.6310 - rec: 0.4385 - val_loss: 0.5594 - val_prec: 0.7045 - val_rec: 0.2877\n",
      "Epoch 8/30\n",
      "184/184 [==============================] - 39s 210ms/step - loss: 0.4301 - prec: 0.6425 - rec: 0.4592 - val_loss: 0.5487 - val_prec: 0.6852 - val_rec: 0.3434\n",
      "Epoch 9/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.4248 - prec: 0.6526 - rec: 0.4782 - val_loss: 0.4614 - val_prec: 0.6034 - val_rec: 0.5754\n",
      "Epoch 10/30\n",
      "184/184 [==============================] - 39s 209ms/step - loss: 0.4173 - prec: 0.6619 - rec: 0.4903 - val_loss: 0.4187 - val_prec: 0.7108 - val_rec: 0.4733\n",
      "Epoch 11/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.4096 - prec: 0.6761 - rec: 0.5040 - val_loss: 0.4519 - val_prec: 0.7259 - val_rec: 0.4548\n",
      "Epoch 12/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.4053 - prec: 0.6834 - rec: 0.5163 - val_loss: 0.4400 - val_prec: 0.7012 - val_rec: 0.5499\n",
      "Epoch 13/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.3994 - prec: 0.6930 - rec: 0.5253 - val_loss: 0.4608 - val_prec: 0.7681 - val_rec: 0.4687\n",
      "Epoch 14/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.3969 - prec: 0.6953 - rec: 0.5315 - val_loss: 0.4417 - val_prec: 0.6848 - val_rec: 0.5545\n",
      "Epoch 15/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.3906 - prec: 0.7078 - rec: 0.5476 - val_loss: 0.3977 - val_prec: 0.6658 - val_rec: 0.6288\n",
      "Epoch 16/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.3852 - prec: 0.7136 - rec: 0.5526 - val_loss: 0.3946 - val_prec: 0.7051 - val_rec: 0.5824\n",
      "Epoch 17/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.3808 - prec: 0.7220 - rec: 0.5645 - val_loss: 0.3951 - val_prec: 0.7692 - val_rec: 0.5104\n",
      "Epoch 18/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.3758 - prec: 0.7268 - rec: 0.5712 - val_loss: 0.3883 - val_prec: 0.7134 - val_rec: 0.5313\n",
      "Epoch 19/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.3731 - prec: 0.7323 - rec: 0.5765 - val_loss: 0.3894 - val_prec: 0.7734 - val_rec: 0.4988\n",
      "Epoch 20/30\n",
      "184/184 [==============================] - 39s 209ms/step - loss: 0.3704 - prec: 0.7371 - rec: 0.5821 - val_loss: 0.4166 - val_prec: 0.7107 - val_rec: 0.5244\n",
      "Epoch 21/30\n",
      "184/184 [==============================] - 38s 208ms/step - loss: 0.3659 - prec: 0.7409 - rec: 0.5890 - val_loss: 0.4550 - val_prec: 0.6127 - val_rec: 0.6937\n",
      "Epoch 22/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.3634 - prec: 0.7453 - rec: 0.5973 - val_loss: 0.3754 - val_prec: 0.7762 - val_rec: 0.5151\n",
      "Epoch 23/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.3592 - prec: 0.7532 - rec: 0.6040 - val_loss: 0.4096 - val_prec: 0.6667 - val_rec: 0.5800\n",
      "Epoch 24/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.3566 - prec: 0.7550 - rec: 0.6100 - val_loss: 0.3930 - val_prec: 0.6902 - val_rec: 0.5893\n",
      "Epoch 25/30\n",
      "184/184 [==============================] - 38s 208ms/step - loss: 0.3531 - prec: 0.7598 - rec: 0.6157 - val_loss: 0.3644 - val_prec: 0.7515 - val_rec: 0.5824\n",
      "Epoch 26/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.3496 - prec: 0.7690 - rec: 0.6230 - val_loss: 0.4061 - val_prec: 0.6296 - val_rec: 0.6705\n",
      "Epoch 27/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.3478 - prec: 0.7686 - rec: 0.6248 - val_loss: 0.4065 - val_prec: 0.7423 - val_rec: 0.5615\n",
      "Epoch 28/30\n",
      "184/184 [==============================] - 37s 203ms/step - loss: 0.3459 - prec: 0.7719 - rec: 0.6273 - val_loss: 0.3691 - val_prec: 0.7649 - val_rec: 0.5661\n",
      "Epoch 29/30\n",
      "184/184 [==============================] - 38s 205ms/step - loss: 0.3417 - prec: 0.7783 - rec: 0.6358 - val_loss: 0.3974 - val_prec: 0.6919 - val_rec: 0.5940\n",
      "Epoch 30/30\n",
      "184/184 [==============================] - 38s 205ms/step - loss: 0.3408 - prec: 0.7776 - rec: 0.6369 - val_loss: 0.3795 - val_prec: 0.7500 - val_rec: 0.6195\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/res_50_baseline/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(32, 32), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(32, 32), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (32, 32, 3))\n",
    "model = compile_new_model(model)\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 30,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "  6/184 [..............................] - ETA: 29s - loss: 0.3327 - prec: 0.7977 - rec: 0.6529WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1617s vs `on_train_batch_end` time: 0.5167s). Check your callbacks.\n",
      "184/184 [==============================] - 172s 242ms/step - loss: 0.3373 - prec: 0.7850 - rec: 0.6445 - val_loss: 0.4081 - val_prec: 0.6700 - val_rec: 0.6311\n",
      "Epoch 2/30\n",
      "184/184 [==============================] - 39s 210ms/step - loss: 0.3330 - prec: 0.7888 - rec: 0.6492 - val_loss: 0.3788 - val_prec: 0.7194 - val_rec: 0.6009\n",
      "Epoch 3/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.3316 - prec: 0.7898 - rec: 0.6536 - val_loss: 0.3745 - val_prec: 0.7683 - val_rec: 0.6079\n",
      "Epoch 4/30\n",
      "184/184 [==============================] - 38s 208ms/step - loss: 0.3293 - prec: 0.7972 - rec: 0.6552 - val_loss: 0.3774 - val_prec: 0.7205 - val_rec: 0.6520\n",
      "Epoch 5/30\n",
      "184/184 [==============================] - 38s 208ms/step - loss: 0.3255 - prec: 0.7992 - rec: 0.6640 - val_loss: 0.4080 - val_prec: 0.7873 - val_rec: 0.4896\n",
      "Epoch 6/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.3234 - prec: 0.8017 - rec: 0.6666 - val_loss: 0.3849 - val_prec: 0.7147 - val_rec: 0.5986\n",
      "Epoch 7/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.3211 - prec: 0.8054 - rec: 0.6690 - val_loss: 0.3682 - val_prec: 0.7341 - val_rec: 0.6148\n",
      "Epoch 8/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.3204 - prec: 0.8070 - rec: 0.6730 - val_loss: 0.3874 - val_prec: 0.7697 - val_rec: 0.5661\n",
      "Epoch 9/30\n",
      "184/184 [==============================] - 38s 204ms/step - loss: 0.3163 - prec: 0.8107 - rec: 0.6803 - val_loss: 0.3918 - val_prec: 0.7874 - val_rec: 0.5499\n",
      "Epoch 10/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.3149 - prec: 0.8106 - rec: 0.6812 - val_loss: 0.4592 - val_prec: 0.7666 - val_rec: 0.5104\n",
      "Epoch 11/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.3130 - prec: 0.8154 - rec: 0.6833 - val_loss: 0.3635 - val_prec: 0.7591 - val_rec: 0.6288\n",
      "Epoch 12/30\n",
      "184/184 [==============================] - 38s 205ms/step - loss: 0.3095 - prec: 0.8195 - rec: 0.6891 - val_loss: 0.3965 - val_prec: 0.7601 - val_rec: 0.5661\n",
      "Epoch 13/30\n",
      "184/184 [==============================] - 39s 209ms/step - loss: 0.3071 - prec: 0.8238 - rec: 0.6922 - val_loss: 0.3925 - val_prec: 0.7185 - val_rec: 0.6218\n",
      "Epoch 14/30\n",
      "184/184 [==============================] - 38s 208ms/step - loss: 0.3059 - prec: 0.8267 - rec: 0.6963 - val_loss: 0.3997 - val_prec: 0.6889 - val_rec: 0.6218\n",
      "Epoch 15/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.3040 - prec: 0.8255 - rec: 0.7020 - val_loss: 0.3943 - val_prec: 0.7643 - val_rec: 0.5267\n",
      "Epoch 16/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.3014 - prec: 0.8293 - rec: 0.7036 - val_loss: 0.4309 - val_prec: 0.6008 - val_rec: 0.7332\n",
      "Epoch 17/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.2992 - prec: 0.8300 - rec: 0.7056 - val_loss: 0.3786 - val_prec: 0.7528 - val_rec: 0.6148\n",
      "Epoch 18/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.2978 - prec: 0.8342 - rec: 0.7094 - val_loss: 0.4079 - val_prec: 0.7508 - val_rec: 0.5592\n",
      "Epoch 19/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.2956 - prec: 0.8368 - rec: 0.7130 - val_loss: 0.4123 - val_prec: 0.6399 - val_rec: 0.6845\n",
      "Epoch 20/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.2936 - prec: 0.8396 - rec: 0.7161 - val_loss: 0.4331 - val_prec: 0.5757 - val_rec: 0.7146\n",
      "Epoch 21/30\n",
      "184/184 [==============================] - 38s 207ms/step - loss: 0.2925 - prec: 0.8409 - rec: 0.7185 - val_loss: 0.4124 - val_prec: 0.6245 - val_rec: 0.7796\n",
      "Epoch 22/30\n",
      "184/184 [==============================] - 38s 205ms/step - loss: 0.2914 - prec: 0.8442 - rec: 0.7232 - val_loss: 0.3641 - val_prec: 0.7948 - val_rec: 0.6381\n",
      "Epoch 23/30\n",
      "184/184 [==============================] - 38s 208ms/step - loss: 0.2883 - prec: 0.8461 - rec: 0.7249 - val_loss: 0.3692 - val_prec: 0.7574 - val_rec: 0.6520\n",
      "Epoch 24/30\n",
      "184/184 [==============================] - 38s 205ms/step - loss: 0.2866 - prec: 0.8457 - rec: 0.7273 - val_loss: 0.3834 - val_prec: 0.7193 - val_rec: 0.7077\n",
      "Epoch 25/30\n",
      "184/184 [==============================] - 37s 202ms/step - loss: 0.2845 - prec: 0.8498 - rec: 0.7328 - val_loss: 0.3927 - val_prec: 0.7018 - val_rec: 0.7100\n",
      "Epoch 26/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.2841 - prec: 0.8503 - rec: 0.7344 - val_loss: 0.4154 - val_prec: 0.7611 - val_rec: 0.5174\n",
      "Epoch 27/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.2833 - prec: 0.8506 - rec: 0.7351 - val_loss: 0.3839 - val_prec: 0.7337 - val_rec: 0.6520\n",
      "Epoch 28/30\n",
      "184/184 [==============================] - 38s 209ms/step - loss: 0.2812 - prec: 0.8533 - rec: 0.7390 - val_loss: 0.3844 - val_prec: 0.7500 - val_rec: 0.6265\n",
      "Epoch 29/30\n",
      "184/184 [==============================] - 39s 210ms/step - loss: 0.2791 - prec: 0.8578 - rec: 0.7420 - val_loss: 0.3844 - val_prec: 0.7117 - val_rec: 0.6473\n",
      "Epoch 30/30\n",
      "184/184 [==============================] - 38s 206ms/step - loss: 0.2773 - prec: 0.8606 - rec: 0.7452 - val_loss: 0.3801 - val_prec: 0.7097 - val_rec: 0.6636\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/res_50_baseline_v1/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path, epoch_number=31)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(32, 32), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(32, 32), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (32, 32, 3))\n",
    "model = compile_new_model(model)\n",
    "model.load_weights('/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline/30.h5')\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 30,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "  6/184 [..............................] - ETA: 31s - loss: 0.6757 - prec: 0.2471 - rec: 0.1064WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1745s vs `on_train_batch_end` time: 0.5093s). Check your callbacks.\n",
      "184/184 [==============================] - 184s 261ms/step - loss: 0.6489 - prec: 0.3073 - rec: 0.0972 - val_loss: 0.5703 - val_prec: 0.0000e+00 - val_rec: 0.0000e+00\n",
      "Epoch 2/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.5496 - prec: 0.4712 - rec: 0.1734 - val_loss: 0.5416 - val_prec: 0.2273 - val_rec: 0.0232\n",
      "Epoch 3/30\n",
      "184/184 [==============================] - 42s 227ms/step - loss: 0.5069 - prec: 0.5339 - rec: 0.2608 - val_loss: 0.5957 - val_prec: 0.4400 - val_rec: 0.0255\n",
      "Epoch 4/30\n",
      "184/184 [==============================] - 42s 228ms/step - loss: 0.4808 - prec: 0.5815 - rec: 0.3388 - val_loss: 0.6401 - val_prec: 0.6087 - val_rec: 0.0650\n",
      "Epoch 5/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.4645 - prec: 0.6023 - rec: 0.3890 - val_loss: 0.6708 - val_prec: 0.6724 - val_rec: 0.1810\n",
      "Epoch 6/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.4476 - prec: 0.6265 - rec: 0.4245 - val_loss: 0.4527 - val_prec: 0.5833 - val_rec: 0.4872\n",
      "Epoch 7/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.4353 - prec: 0.6498 - rec: 0.4501 - val_loss: 0.4795 - val_prec: 0.6102 - val_rec: 0.5012\n",
      "Epoch 8/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.4226 - prec: 0.6646 - rec: 0.4738 - val_loss: 0.4642 - val_prec: 0.6817 - val_rec: 0.4919\n",
      "Epoch 9/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.4125 - prec: 0.6847 - rec: 0.4947 - val_loss: 0.4210 - val_prec: 0.7280 - val_rec: 0.4223\n",
      "Epoch 10/30\n",
      "184/184 [==============================] - 41s 223ms/step - loss: 0.4028 - prec: 0.6951 - rec: 0.5097 - val_loss: 0.4499 - val_prec: 0.7449 - val_rec: 0.4200\n",
      "Epoch 11/30\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.3934 - prec: 0.7155 - rec: 0.5275 - val_loss: 0.5171 - val_prec: 0.7892 - val_rec: 0.3735\n",
      "Epoch 12/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3880 - prec: 0.7283 - rec: 0.5420 - val_loss: 0.3957 - val_prec: 0.6477 - val_rec: 0.6613\n",
      "Epoch 13/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3784 - prec: 0.7407 - rec: 0.5530 - val_loss: 0.3847 - val_prec: 0.7597 - val_rec: 0.5429\n",
      "Epoch 14/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3750 - prec: 0.7487 - rec: 0.5623 - val_loss: 0.3779 - val_prec: 0.7445 - val_rec: 0.5476\n",
      "Epoch 15/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3671 - prec: 0.7563 - rec: 0.5756 - val_loss: 0.4156 - val_prec: 0.7155 - val_rec: 0.5777\n",
      "Epoch 16/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.3624 - prec: 0.7630 - rec: 0.5870 - val_loss: 0.3689 - val_prec: 0.8316 - val_rec: 0.5499\n",
      "Epoch 17/30\n",
      "184/184 [==============================] - 42s 228ms/step - loss: 0.3579 - prec: 0.7711 - rec: 0.5955 - val_loss: 0.3626 - val_prec: 0.8047 - val_rec: 0.5545\n",
      "Epoch 18/30\n",
      "184/184 [==============================] - 41s 222ms/step - loss: 0.3534 - prec: 0.7747 - rec: 0.6024 - val_loss: 0.3541 - val_prec: 0.7359 - val_rec: 0.6659\n",
      "Epoch 19/30\n",
      "184/184 [==============================] - 41s 222ms/step - loss: 0.3495 - prec: 0.7806 - rec: 0.6122 - val_loss: 0.3704 - val_prec: 0.7908 - val_rec: 0.5174\n",
      "Epoch 20/30\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.3459 - prec: 0.7861 - rec: 0.6215 - val_loss: 0.3777 - val_prec: 0.8656 - val_rec: 0.5081\n",
      "Epoch 21/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3415 - prec: 0.7908 - rec: 0.6277 - val_loss: 0.3526 - val_prec: 0.7446 - val_rec: 0.6357\n",
      "Epoch 22/30\n",
      "184/184 [==============================] - 42s 227ms/step - loss: 0.3386 - prec: 0.7957 - rec: 0.6340 - val_loss: 0.3547 - val_prec: 0.8495 - val_rec: 0.5499\n",
      "Epoch 23/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3357 - prec: 0.7974 - rec: 0.6392 - val_loss: 0.3442 - val_prec: 0.8688 - val_rec: 0.5684\n",
      "Epoch 24/30\n",
      "184/184 [==============================] - 41s 221ms/step - loss: 0.3315 - prec: 0.8026 - rec: 0.6470 - val_loss: 0.3769 - val_prec: 0.8755 - val_rec: 0.5383\n",
      "Epoch 25/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3293 - prec: 0.8059 - rec: 0.6517 - val_loss: 0.3276 - val_prec: 0.8358 - val_rec: 0.6613\n",
      "Epoch 26/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3267 - prec: 0.8090 - rec: 0.6595 - val_loss: 0.3255 - val_prec: 0.8000 - val_rec: 0.6311\n",
      "Epoch 27/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3247 - prec: 0.8087 - rec: 0.6642 - val_loss: 0.3319 - val_prec: 0.8360 - val_rec: 0.6032\n",
      "Epoch 28/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.3204 - prec: 0.8151 - rec: 0.6666 - val_loss: 0.3438 - val_prec: 0.7890 - val_rec: 0.6334\n",
      "Epoch 29/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.3179 - prec: 0.8164 - rec: 0.6738 - val_loss: 0.3278 - val_prec: 0.8022 - val_rec: 0.6682\n",
      "Epoch 30/30\n",
      "184/184 [==============================] - 41s 221ms/step - loss: 0.3149 - prec: 0.8227 - rec: 0.6788 - val_loss: 0.3412 - val_prec: 0.7603 - val_rec: 0.6404\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/res_50_baseline_64/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline_64/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(64, 64), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(64, 64), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (64, 64, 3))\n",
    "model = compile_new_model(model)\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 30,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "  6/184 [..............................] - ETA: 32s - loss: 0.3144 - prec: 0.8325 - rec: 0.6836WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1764s vs `on_train_batch_end` time: 0.4367s). Check your callbacks.\n",
      "184/184 [==============================] - 187s 260ms/step - loss: 0.3133 - prec: 0.8231 - rec: 0.6829 - val_loss: 0.3358 - val_prec: 0.7760 - val_rec: 0.6752\n",
      "Epoch 2/30\n",
      "184/184 [==============================] - 42s 230ms/step - loss: 0.3115 - prec: 0.8268 - rec: 0.6867 - val_loss: 0.3180 - val_prec: 0.8254 - val_rec: 0.6473\n",
      "Epoch 3/30\n",
      "184/184 [==============================] - 42s 229ms/step - loss: 0.3085 - prec: 0.8269 - rec: 0.6893 - val_loss: 0.4022 - val_prec: 0.7473 - val_rec: 0.6311\n",
      "Epoch 4/30\n",
      "184/184 [==============================] - 42s 229ms/step - loss: 0.3062 - prec: 0.8300 - rec: 0.6961 - val_loss: 0.3267 - val_prec: 0.8424 - val_rec: 0.6450\n",
      "Epoch 5/30\n",
      "184/184 [==============================] - 42s 227ms/step - loss: 0.3042 - prec: 0.8327 - rec: 0.7007 - val_loss: 0.3190 - val_prec: 0.7958 - val_rec: 0.6961\n",
      "Epoch 6/30\n",
      "184/184 [==============================] - 42s 229ms/step - loss: 0.3026 - prec: 0.8322 - rec: 0.7064 - val_loss: 0.3314 - val_prec: 0.8553 - val_rec: 0.6311\n",
      "Epoch 7/30\n",
      "184/184 [==============================] - 42s 228ms/step - loss: 0.2987 - prec: 0.8387 - rec: 0.7088 - val_loss: 0.3789 - val_prec: 0.6569 - val_rec: 0.7819\n",
      "Epoch 8/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.2975 - prec: 0.8365 - rec: 0.7146 - val_loss: 0.3534 - val_prec: 0.7035 - val_rec: 0.7541\n",
      "Epoch 9/30\n",
      "184/184 [==============================] - 41s 223ms/step - loss: 0.2943 - prec: 0.8405 - rec: 0.7196 - val_loss: 0.3110 - val_prec: 0.8110 - val_rec: 0.6868\n",
      "Epoch 10/30\n",
      "184/184 [==============================] - 42s 229ms/step - loss: 0.2933 - prec: 0.8438 - rec: 0.7201 - val_loss: 0.3191 - val_prec: 0.8529 - val_rec: 0.6589\n",
      "Epoch 11/30\n",
      "184/184 [==============================] - 42s 227ms/step - loss: 0.2906 - prec: 0.8469 - rec: 0.7258 - val_loss: 0.3305 - val_prec: 0.8861 - val_rec: 0.5777\n",
      "Epoch 12/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.2886 - prec: 0.8469 - rec: 0.7284 - val_loss: 0.3172 - val_prec: 0.8309 - val_rec: 0.6613\n",
      "Epoch 13/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.2882 - prec: 0.8463 - rec: 0.7289 - val_loss: 0.3201 - val_prec: 0.8107 - val_rec: 0.6659\n",
      "Epoch 14/30\n",
      "184/184 [==============================] - 42s 227ms/step - loss: 0.2834 - prec: 0.8530 - rec: 0.7375 - val_loss: 0.3302 - val_prec: 0.7448 - val_rec: 0.7517\n",
      "Epoch 15/30\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.2830 - prec: 0.8523 - rec: 0.7407 - val_loss: 0.3254 - val_prec: 0.8433 - val_rec: 0.6241\n",
      "Epoch 16/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.2808 - prec: 0.8581 - rec: 0.7425 - val_loss: 0.3115 - val_prec: 0.8395 - val_rec: 0.6798\n",
      "Epoch 17/30\n",
      "184/184 [==============================] - 42s 228ms/step - loss: 0.2783 - prec: 0.8587 - rec: 0.7486 - val_loss: 0.3233 - val_prec: 0.7488 - val_rec: 0.7541\n",
      "Epoch 18/30\n",
      "184/184 [==============================] - 42s 227ms/step - loss: 0.2773 - prec: 0.8594 - rec: 0.7505 - val_loss: 0.3538 - val_prec: 0.8671 - val_rec: 0.6659\n",
      "Epoch 19/30\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.2750 - prec: 0.8622 - rec: 0.7545 - val_loss: 0.3177 - val_prec: 0.7756 - val_rec: 0.7216\n",
      "Epoch 20/30\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.2716 - prec: 0.8646 - rec: 0.7594 - val_loss: 0.3093 - val_prec: 0.8429 - val_rec: 0.6845\n",
      "Epoch 21/30\n",
      "184/184 [==============================] - 41s 223ms/step - loss: 0.2708 - prec: 0.8676 - rec: 0.7611 - val_loss: 0.3028 - val_prec: 0.8146 - val_rec: 0.7239\n",
      "Epoch 22/30\n",
      "184/184 [==============================] - 42s 227ms/step - loss: 0.2688 - prec: 0.8686 - rec: 0.7658 - val_loss: 0.3732 - val_prec: 0.8922 - val_rec: 0.5568\n",
      "Epoch 23/30\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.2664 - prec: 0.8707 - rec: 0.7683 - val_loss: 0.3064 - val_prec: 0.7939 - val_rec: 0.7239\n",
      "Epoch 24/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.2644 - prec: 0.8741 - rec: 0.7725 - val_loss: 0.3698 - val_prec: 0.6883 - val_rec: 0.7633\n",
      "Epoch 25/30\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.2629 - prec: 0.8750 - rec: 0.7757 - val_loss: 0.3235 - val_prec: 0.8097 - val_rec: 0.6613\n",
      "Epoch 26/30\n",
      "184/184 [==============================] - 41s 225ms/step - loss: 0.2604 - prec: 0.8790 - rec: 0.7794 - val_loss: 0.3622 - val_prec: 0.7359 - val_rec: 0.6659\n",
      "Epoch 27/30\n",
      "184/184 [==============================] - 42s 226ms/step - loss: 0.2595 - prec: 0.8805 - rec: 0.7828 - val_loss: 0.3580 - val_prec: 0.8585 - val_rec: 0.6334\n",
      "Epoch 28/30\n",
      "184/184 [==============================] - 41s 223ms/step - loss: 0.2573 - prec: 0.8817 - rec: 0.7835 - val_loss: 0.3078 - val_prec: 0.8550 - val_rec: 0.6705\n",
      "Epoch 29/30\n",
      "184/184 [==============================] - 41s 224ms/step - loss: 0.2528 - prec: 0.8870 - rec: 0.7914 - val_loss: 0.3280 - val_prec: 0.8571 - val_rec: 0.6682\n",
      "Epoch 30/30\n",
      "184/184 [==============================] - 41s 221ms/step - loss: 0.2520 - prec: 0.8871 - rec: 0.7955 - val_loss: 0.3017 - val_prec: 0.8529 - val_rec: 0.7262\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/res_50_baseline_64_v1/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline_64/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path, epoch_number=31)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(64, 64), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(64, 64), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (64, 64, 3))\n",
    "model = compile_new_model(model)\n",
    "model.load_weights('/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline_64/30.h5')\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 30,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  6/184 [..............................] - ETA: 39s - loss: 1.1728 - prec: 0.2396 - rec: 0.2001WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2162s vs `on_train_batch_end` time: 0.4461s). Check your callbacks.\n",
      "184/184 [==============================] - 187s 297ms/step - loss: 0.6092 - prec: 0.4112 - rec: 0.1381 - val_loss: 0.6192 - val_prec: 0.0000e+00 - val_rec: 0.0000e+00\n",
      "Epoch 2/30\n",
      "184/184 [==============================] - 52s 283ms/step - loss: 0.5211 - prec: 0.5183 - rec: 0.2663 - val_loss: 0.5579 - val_prec: 0.5128 - val_rec: 0.0464\n",
      "Epoch 3/30\n",
      "184/184 [==============================] - 52s 283ms/step - loss: 0.4843 - prec: 0.5614 - rec: 0.3410 - val_loss: 0.6362 - val_prec: 0.4049 - val_rec: 0.6566\n",
      "Epoch 4/30\n",
      "184/184 [==============================] - 52s 282ms/step - loss: 0.4479 - prec: 0.6240 - rec: 0.4201 - val_loss: 0.5203 - val_prec: 0.6974 - val_rec: 0.2459\n",
      "Epoch 5/30\n",
      "184/184 [==============================] - 52s 283ms/step - loss: 0.4247 - prec: 0.6740 - rec: 0.4781 - val_loss: 0.5731 - val_prec: 0.5862 - val_rec: 0.4733\n",
      "Epoch 6/30\n",
      "184/184 [==============================] - 52s 282ms/step - loss: 0.3943 - prec: 0.7252 - rec: 0.5182 - val_loss: 0.4434 - val_prec: 0.6134 - val_rec: 0.6148\n",
      "Epoch 7/30\n",
      "184/184 [==============================] - 51s 280ms/step - loss: 0.3785 - prec: 0.7518 - rec: 0.5491 - val_loss: 0.4495 - val_prec: 0.5991 - val_rec: 0.6102\n",
      "Epoch 8/30\n",
      "184/184 [==============================] - 52s 280ms/step - loss: 0.3616 - prec: 0.7771 - rec: 0.5764 - val_loss: 0.3786 - val_prec: 0.7032 - val_rec: 0.6543\n",
      "Epoch 9/30\n",
      "184/184 [==============================] - 51s 279ms/step - loss: 0.3482 - prec: 0.7962 - rec: 0.6026 - val_loss: 0.4761 - val_prec: 0.5615 - val_rec: 0.8051\n",
      "Epoch 10/30\n",
      "184/184 [==============================] - 51s 278ms/step - loss: 0.3379 - prec: 0.8093 - rec: 0.6258 - val_loss: 0.4146 - val_prec: 0.9357 - val_rec: 0.3712\n",
      "Epoch 11/30\n",
      "184/184 [==============================] - 51s 279ms/step - loss: 0.3265 - prec: 0.8231 - rec: 0.6478 - val_loss: 0.3275 - val_prec: 0.8276 - val_rec: 0.6682\n",
      "Epoch 12/30\n",
      "184/184 [==============================] - 52s 283ms/step - loss: 0.3168 - prec: 0.8325 - rec: 0.6671 - val_loss: 0.3272 - val_prec: 0.8624 - val_rec: 0.6543\n",
      "Epoch 13/30\n",
      "184/184 [==============================] - 52s 283ms/step - loss: 0.3065 - prec: 0.8429 - rec: 0.6884 - val_loss: 0.3595 - val_prec: 0.8690 - val_rec: 0.6311\n",
      "Epoch 14/30\n",
      "184/184 [==============================] - 52s 281ms/step - loss: 0.3018 - prec: 0.8447 - rec: 0.7018 - val_loss: 0.3219 - val_prec: 0.7657 - val_rec: 0.7053\n",
      "Epoch 15/30\n",
      "184/184 [==============================] - 52s 280ms/step - loss: 0.2942 - prec: 0.8537 - rec: 0.7157 - val_loss: 0.3397 - val_prec: 0.8338 - val_rec: 0.6752\n",
      "Epoch 16/30\n",
      "184/184 [==============================] - 52s 282ms/step - loss: 0.2861 - prec: 0.8591 - rec: 0.7300 - val_loss: 0.2991 - val_prec: 0.8645 - val_rec: 0.6659\n",
      "Epoch 17/30\n",
      "184/184 [==============================] - 51s 279ms/step - loss: 0.2822 - prec: 0.8638 - rec: 0.7409 - val_loss: 0.2976 - val_prec: 0.8736 - val_rec: 0.7378\n",
      "Epoch 18/30\n",
      "184/184 [==============================] - 52s 280ms/step - loss: 0.2765 - prec: 0.8664 - rec: 0.7484 - val_loss: 0.3108 - val_prec: 0.8082 - val_rec: 0.7819\n",
      "Epoch 19/30\n",
      "184/184 [==============================] - 52s 280ms/step - loss: 0.2725 - prec: 0.8688 - rec: 0.7592 - val_loss: 0.2972 - val_prec: 0.8622 - val_rec: 0.7401\n",
      "Epoch 20/30\n",
      "184/184 [==============================] - 51s 279ms/step - loss: 0.2688 - prec: 0.8734 - rec: 0.7665 - val_loss: 0.2846 - val_prec: 0.8964 - val_rec: 0.7425\n",
      "Epoch 21/30\n",
      "184/184 [==============================] - 51s 280ms/step - loss: 0.2649 - prec: 0.8763 - rec: 0.7723 - val_loss: 0.2867 - val_prec: 0.8118 - val_rec: 0.8306\n",
      "Epoch 22/30\n",
      "184/184 [==============================] - 52s 281ms/step - loss: 0.2609 - prec: 0.8798 - rec: 0.7809 - val_loss: 0.2827 - val_prec: 0.8362 - val_rec: 0.7819\n",
      "Epoch 23/30\n",
      "184/184 [==============================] - 52s 280ms/step - loss: 0.2574 - prec: 0.8815 - rec: 0.7844 - val_loss: 0.3120 - val_prec: 0.8663 - val_rec: 0.6914\n",
      "Epoch 24/30\n",
      "184/184 [==============================] - 52s 280ms/step - loss: 0.2548 - prec: 0.8833 - rec: 0.7931 - val_loss: 0.2718 - val_prec: 0.8564 - val_rec: 0.7889\n",
      "Epoch 25/30\n",
      "184/184 [==============================] - 51s 279ms/step - loss: 0.2524 - prec: 0.8836 - rec: 0.7957 - val_loss: 0.2554 - val_prec: 0.9005 - val_rec: 0.7773\n",
      "Epoch 26/30\n",
      "184/184 [==============================] - 52s 282ms/step - loss: 0.2496 - prec: 0.8870 - rec: 0.8019 - val_loss: 0.2552 - val_prec: 0.8645 - val_rec: 0.8144\n",
      "Epoch 27/30\n",
      "184/184 [==============================] - 52s 283ms/step - loss: 0.2466 - prec: 0.8890 - rec: 0.8071 - val_loss: 0.2820 - val_prec: 0.9159 - val_rec: 0.7332\n",
      "Epoch 28/30\n",
      "184/184 [==============================] - 52s 280ms/step - loss: 0.2445 - prec: 0.8919 - rec: 0.8085 - val_loss: 0.2675 - val_prec: 0.8416 - val_rec: 0.8260\n",
      "Epoch 29/30\n",
      "184/184 [==============================] - 52s 282ms/step - loss: 0.2427 - prec: 0.8917 - rec: 0.8136 - val_loss: 0.2537 - val_prec: 0.8947 - val_rec: 0.7889\n",
      "Epoch 30/30\n",
      "121/184 [==================>...........] - ETA: 16s - loss: 0.2393 - prec: 0.8960 - rec: 0.8179"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/res_50_baseline_128/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline_128/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(128, 128), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(128, 128), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (128, 128, 3))\n",
    "model = compile_new_model(model)\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 30,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/res_50_baseline_128_v1/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline_128/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path, epoch_number = 31)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(128, 128), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(128, 128), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (128, 128, 3))\n",
    "model = compile_new_model(model)\n",
    "model.load_weights('/home/ubuntu/ml-data-training/ship_seg_weights/classification/res_50_baseline_128/30.h5')\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 30,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1/30\n",
      "INFO:tensorflow:batch_all_reduce: 213 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 213 all-reduces with algorithm = nccl, num_packs = 1\n",
      "  6/184 [..............................] - ETA: 47s - loss: 0.6690 - prec: 0.2951 - rec: 0.1275WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2594s vs `on_train_batch_end` time: 0.6325s). Check your callbacks.\n",
      "184/184 [==============================] - 231s 345ms/step - loss: 0.5723 - prec: 0.2760 - rec: 0.0164 - val_loss: 0.5548 - val_prec: 0.0000e+00 - val_rec: 0.0000e+00\n",
      "Epoch 2/30\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.5456 - prec: 0.3230 - rec: 0.0088 - val_loss: 0.5511 - val_prec: 0.0000e+00 - val_rec: 0.0000e+00\n",
      "Epoch 3/30\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.5409 - prec: 0.4149 - rec: 0.0165 - val_loss: 0.5527 - val_prec: 0.0000e+00 - val_rec: 0.0000e+00\n",
      "Epoch 4/30\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.5348 - prec: 0.4939 - rec: 0.0359 - val_loss: 0.5452 - val_prec: 0.6000 - val_rec: 0.0070\n",
      "Epoch 5/30\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.5223 - prec: 0.5993 - rec: 0.0840 - val_loss: 0.5324 - val_prec: 0.6290 - val_rec: 0.0905\n",
      "Epoch 6/30\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.5004 - prec: 0.6895 - rec: 0.1609 - val_loss: 0.5357 - val_prec: 0.4643 - val_rec: 0.1206\n",
      "Epoch 7/30\n",
      "184/184 [==============================] - 52s 285ms/step - loss: 0.4696 - prec: 0.7262 - rec: 0.2628 - val_loss: 0.4715 - val_prec: 0.8922 - val_rec: 0.2111\n",
      "Epoch 8/30\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.4417 - prec: 0.7473 - rec: 0.3489 - val_loss: 2.1440 - val_prec: 0.2277 - val_rec: 0.5452\n",
      "Epoch 9/30\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.4192 - prec: 0.7731 - rec: 0.4078 - val_loss: 0.4540 - val_prec: 0.6872 - val_rec: 0.2854\n",
      "Epoch 10/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.4020 - prec: 0.7850 - rec: 0.4541 - val_loss: 0.4327 - val_prec: 0.7885 - val_rec: 0.4756\n",
      "Epoch 11/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3889 - prec: 0.7937 - rec: 0.4887 - val_loss: 0.7277 - val_prec: 0.9286 - val_rec: 0.0302\n",
      "Epoch 12/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3784 - prec: 0.7984 - rec: 0.5157 - val_loss: 0.4478 - val_prec: 0.8553 - val_rec: 0.3155\n",
      "Epoch 13/30\n",
      "184/184 [==============================] - 50s 271ms/step - loss: 0.3710 - prec: 0.8018 - rec: 0.5365 - val_loss: 0.3983 - val_prec: 0.8532 - val_rec: 0.4316\n",
      "Epoch 14/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3642 - prec: 0.8094 - rec: 0.5554 - val_loss: 0.4473 - val_prec: 0.9114 - val_rec: 0.3341\n",
      "Epoch 15/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3587 - prec: 0.8112 - rec: 0.5662 - val_loss: 0.3913 - val_prec: 0.8839 - val_rec: 0.4594\n",
      "Epoch 16/30\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.3538 - prec: 0.8145 - rec: 0.5821 - val_loss: 0.3674 - val_prec: 0.8741 - val_rec: 0.5476\n",
      "Epoch 17/30\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.3493 - prec: 0.8178 - rec: 0.5906 - val_loss: 0.3560 - val_prec: 0.8830 - val_rec: 0.5429\n",
      "Epoch 18/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3452 - prec: 0.8250 - rec: 0.5990 - val_loss: 0.3703 - val_prec: 0.8800 - val_rec: 0.5104\n",
      "Epoch 19/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3406 - prec: 0.8280 - rec: 0.6126 - val_loss: 0.3661 - val_prec: 0.8837 - val_rec: 0.5290\n",
      "Epoch 20/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3369 - prec: 0.8297 - rec: 0.6227 - val_loss: 0.4060 - val_prec: 0.8787 - val_rec: 0.4872\n",
      "Epoch 21/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3348 - prec: 0.8282 - rec: 0.6233 - val_loss: 0.3916 - val_prec: 0.8952 - val_rec: 0.5151\n",
      "Epoch 22/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3320 - prec: 0.8319 - rec: 0.6342 - val_loss: 0.3909 - val_prec: 0.9042 - val_rec: 0.5035\n",
      "Epoch 23/30\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.3289 - prec: 0.8363 - rec: 0.6408 - val_loss: 0.3728 - val_prec: 0.9066 - val_rec: 0.5406\n",
      "Epoch 24/30\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.3263 - prec: 0.8353 - rec: 0.6451 - val_loss: 0.4199 - val_prec: 0.9238 - val_rec: 0.4780\n",
      "Epoch 25/30\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.3242 - prec: 0.8384 - rec: 0.6518 - val_loss: 0.3465 - val_prec: 0.8796 - val_rec: 0.5592\n",
      "Epoch 26/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3221 - prec: 0.8383 - rec: 0.6559 - val_loss: 0.3724 - val_prec: 0.9271 - val_rec: 0.5313\n",
      "Epoch 27/30\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.3188 - prec: 0.8414 - rec: 0.6619 - val_loss: 0.3543 - val_prec: 0.8864 - val_rec: 0.5615\n",
      "Epoch 28/30\n",
      "184/184 [==============================] - 50s 271ms/step - loss: 0.3172 - prec: 0.8432 - rec: 0.6669 - val_loss: 0.3715 - val_prec: 0.8810 - val_rec: 0.5499\n",
      "Epoch 29/30\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3156 - prec: 0.8437 - rec: 0.6684 - val_loss: 0.4174 - val_prec: 0.9008 - val_rec: 0.5058\n",
      "Epoch 30/30\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.3138 - prec: 0.8441 - rec: 0.6760 - val_loss: 0.3519 - val_prec: 0.9205 - val_rec: 0.5638\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/efb0_baseline_128/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/efb0_baseline_128/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path, epoch_number = 31)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(128, 128), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(128, 128), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('EfficientNetB0', (128, 128, 3))\n",
    "model = compile_new_model(model)\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 30,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "  6/184 [..............................] - ETA: 33s - loss: 1.6456 - prec: 0.2234 - rec: 0.5059WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1846s vs `on_train_batch_end` time: 0.5341s). Check your callbacks.\n",
      "184/184 [==============================] - 199s 299ms/step - loss: 0.8470 - prec: 0.3285 - rec: 0.6218 - val_loss: 0.5538 - val_prec: 0.0000e+00 - val_rec: 0.0000e+00\n",
      "Epoch 2/50\n",
      "184/184 [==============================] - 53s 288ms/step - loss: 0.5924 - prec: 0.4475 - rec: 0.7401 - val_loss: 0.5971 - val_prec: 0.3091 - val_rec: 0.3063\n",
      "Epoch 3/50\n",
      "184/184 [==============================] - 51s 278ms/step - loss: 0.5185 - prec: 0.5001 - rec: 0.7813 - val_loss: 0.7650 - val_prec: 0.3998 - val_rec: 0.7773\n",
      "Epoch 4/50\n",
      "184/184 [==============================] - 51s 279ms/step - loss: 0.4805 - prec: 0.5297 - rec: 0.8003 - val_loss: 0.5128 - val_prec: 0.4804 - val_rec: 0.7981\n",
      "Epoch 5/50\n",
      "184/184 [==============================] - 51s 278ms/step - loss: 0.4547 - prec: 0.5564 - rec: 0.8127 - val_loss: 0.4341 - val_prec: 0.6090 - val_rec: 0.7193\n",
      "Epoch 6/50\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.4350 - prec: 0.5781 - rec: 0.8199 - val_loss: 0.4608 - val_prec: 0.5681 - val_rec: 0.7935\n",
      "Epoch 7/50\n",
      "184/184 [==============================] - 51s 278ms/step - loss: 0.4173 - prec: 0.5976 - rec: 0.8278 - val_loss: 0.5625 - val_prec: 0.4711 - val_rec: 0.8144\n",
      "Epoch 8/50\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.4011 - prec: 0.6151 - rec: 0.8368 - val_loss: 0.4306 - val_prec: 0.6842 - val_rec: 0.6334\n",
      "Epoch 9/50\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.3863 - prec: 0.6349 - rec: 0.8441 - val_loss: 0.4726 - val_prec: 0.5250 - val_rec: 0.8538\n",
      "Epoch 10/50\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.3703 - prec: 0.6507 - rec: 0.8520 - val_loss: 0.4852 - val_prec: 0.5530 - val_rec: 0.8237\n",
      "Epoch 11/50\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.3597 - prec: 0.6644 - rec: 0.8638 - val_loss: 0.3347 - val_prec: 0.7117 - val_rec: 0.8306\n",
      "Epoch 12/50\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.3453 - prec: 0.6809 - rec: 0.8731 - val_loss: 0.3583 - val_prec: 0.7584 - val_rec: 0.7355\n",
      "Epoch 13/50\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.3325 - prec: 0.6933 - rec: 0.8818 - val_loss: 0.3216 - val_prec: 0.7289 - val_rec: 0.8422\n",
      "Epoch 14/50\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.3241 - prec: 0.7037 - rec: 0.8881 - val_loss: 0.3434 - val_prec: 0.7212 - val_rec: 0.8701\n",
      "Epoch 15/50\n",
      "184/184 [==============================] - 51s 280ms/step - loss: 0.3164 - prec: 0.7087 - rec: 0.8951 - val_loss: 0.2852 - val_prec: 0.7891 - val_rec: 0.8422\n",
      "Epoch 16/50\n",
      "184/184 [==============================] - 51s 279ms/step - loss: 0.3082 - prec: 0.7182 - rec: 0.8999 - val_loss: 0.3158 - val_prec: 0.6880 - val_rec: 0.8492\n",
      "Epoch 17/50\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.3036 - prec: 0.7218 - rec: 0.9028 - val_loss: 0.2994 - val_prec: 0.7579 - val_rec: 0.8933\n",
      "Epoch 18/50\n",
      "184/184 [==============================] - 51s 278ms/step - loss: 0.2976 - prec: 0.7299 - rec: 0.9072 - val_loss: 0.3530 - val_prec: 0.6429 - val_rec: 0.8979\n",
      "Epoch 19/50\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.2904 - prec: 0.7358 - rec: 0.9102 - val_loss: 0.2748 - val_prec: 0.7905 - val_rec: 0.8840\n",
      "Epoch 20/50\n",
      "184/184 [==============================] - 51s 279ms/step - loss: 0.2875 - prec: 0.7385 - rec: 0.9125 - val_loss: 0.3574 - val_prec: 0.6164 - val_rec: 0.9582\n",
      "Epoch 21/50\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.2835 - prec: 0.7423 - rec: 0.9158 - val_loss: 0.3304 - val_prec: 0.7038 - val_rec: 0.8933\n",
      "Epoch 22/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2788 - prec: 0.7494 - rec: 0.9177 - val_loss: 0.2619 - val_prec: 0.8427 - val_rec: 0.8329\n",
      "Epoch 23/50\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.2752 - prec: 0.7530 - rec: 0.9206 - val_loss: 0.2967 - val_prec: 0.7634 - val_rec: 0.8608\n",
      "Epoch 24/50\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2715 - prec: 0.7564 - rec: 0.9229 - val_loss: 0.3086 - val_prec: 0.8422 - val_rec: 0.8051\n",
      "Epoch 25/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2698 - prec: 0.7606 - rec: 0.9243 - val_loss: 0.3166 - val_prec: 0.7156 - val_rec: 0.9165\n",
      "Epoch 26/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2643 - prec: 0.7661 - rec: 0.9288 - val_loss: 0.2837 - val_prec: 0.8086 - val_rec: 0.8329\n",
      "Epoch 27/50\n",
      "184/184 [==============================] - 50s 271ms/step - loss: 0.2613 - prec: 0.7690 - rec: 0.9299 - val_loss: 0.2847 - val_prec: 0.7754 - val_rec: 0.8492\n",
      "Epoch 28/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2589 - prec: 0.7730 - rec: 0.9300 - val_loss: 0.3291 - val_prec: 0.6738 - val_rec: 0.9536\n",
      "Epoch 29/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2583 - prec: 0.7717 - rec: 0.9315 - val_loss: 0.3137 - val_prec: 0.7547 - val_rec: 0.8353\n",
      "Epoch 30/50\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.2545 - prec: 0.7788 - rec: 0.9316 - val_loss: 0.2948 - val_prec: 0.8027 - val_rec: 0.8306\n",
      "Epoch 31/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2523 - prec: 0.7797 - rec: 0.9347 - val_loss: 0.2616 - val_prec: 0.8292 - val_rec: 0.8561\n",
      "Epoch 32/50\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.2504 - prec: 0.7834 - rec: 0.9358 - val_loss: 0.2787 - val_prec: 0.7572 - val_rec: 0.9188\n",
      "Epoch 33/50\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.2477 - prec: 0.7873 - rec: 0.9373 - val_loss: 0.2735 - val_prec: 0.7882 - val_rec: 0.8979\n",
      "Epoch 34/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2450 - prec: 0.7919 - rec: 0.9389 - val_loss: 0.2578 - val_prec: 0.8054 - val_rec: 0.8933\n",
      "Epoch 35/50\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2419 - prec: 0.7950 - rec: 0.9410 - val_loss: 0.3959 - val_prec: 0.6072 - val_rec: 0.9397\n",
      "Epoch 36/50\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2419 - prec: 0.7957 - rec: 0.9412 - val_loss: 0.3125 - val_prec: 0.7654 - val_rec: 0.8631\n",
      "Epoch 37/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2407 - prec: 0.7966 - rec: 0.9412 - val_loss: 0.2691 - val_prec: 0.8395 - val_rec: 0.8376\n",
      "Epoch 38/50\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.2384 - prec: 0.8004 - rec: 0.9434 - val_loss: 0.2713 - val_prec: 0.8226 - val_rec: 0.8608\n",
      "Epoch 39/50\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.2355 - prec: 0.8035 - rec: 0.9445 - val_loss: 0.3364 - val_prec: 0.6787 - val_rec: 0.9606\n",
      "Epoch 40/50\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.2325 - prec: 0.8089 - rec: 0.9473 - val_loss: 0.2640 - val_prec: 0.8992 - val_rec: 0.7657\n",
      "Epoch 41/50\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2302 - prec: 0.8109 - rec: 0.9472 - val_loss: 0.2457 - val_prec: 0.8462 - val_rec: 0.8933\n",
      "Epoch 42/50\n",
      "184/184 [==============================] - 50s 270ms/step - loss: 0.2299 - prec: 0.8111 - rec: 0.9478 - val_loss: 0.2523 - val_prec: 0.8535 - val_rec: 0.8654\n",
      "Epoch 43/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2269 - prec: 0.8164 - rec: 0.9502 - val_loss: 0.2438 - val_prec: 0.8337 - val_rec: 0.8840\n",
      "Epoch 44/50\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2254 - prec: 0.8207 - rec: 0.9506 - val_loss: 0.2505 - val_prec: 0.8601 - val_rec: 0.8701\n",
      "Epoch 45/50\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.2250 - prec: 0.8208 - rec: 0.9497 - val_loss: 0.2875 - val_prec: 0.7543 - val_rec: 0.9258\n",
      "Epoch 46/50\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2226 - prec: 0.8226 - rec: 0.9523 - val_loss: 0.2850 - val_prec: 0.8208 - val_rec: 0.8074\n",
      "Epoch 47/50\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.2202 - prec: 0.8284 - rec: 0.9534 - val_loss: 0.3302 - val_prec: 0.7180 - val_rec: 0.8979\n",
      "Epoch 48/50\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2177 - prec: 0.8318 - rec: 0.9530 - val_loss: 0.3004 - val_prec: 0.7610 - val_rec: 0.8422\n",
      "Epoch 49/50\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2161 - prec: 0.8330 - rec: 0.9562 - val_loss: 0.2688 - val_prec: 0.7908 - val_rec: 0.8770\n",
      "Epoch 50/50\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.2155 - prec: 0.8352 - rec: 0.9550 - val_loss: 0.3200 - val_prec: 0.7589 - val_rec: 0.9350\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/res50_baseline_128_class_weights/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/res50_baseline_128_class_weights/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path, epoch_number = 31)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(128, 128), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(128, 128), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (128, 128, 3))\n",
    "model = compile_new_model(model)\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 50,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    "    class_weight = class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 214 all-reduces with algorithm = nccl, num_packs = 1\n",
      "  6/184 [..............................] - ETA: 33s - loss: 0.2177 - prec: 0.8299 - rec: 0.9558WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1812s vs `on_train_batch_end` time: 0.5564s). Check your callbacks.\n",
      "184/184 [==============================] - 175s 291ms/step - loss: 0.2135 - prec: 0.8382 - rec: 0.9565 - val_loss: 0.2470 - val_prec: 0.8762 - val_rec: 0.8376\n",
      "Epoch 2/100\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.2114 - prec: 0.8421 - rec: 0.9567 - val_loss: 0.2808 - val_prec: 0.7840 - val_rec: 0.8840\n",
      "Epoch 3/100\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.2091 - prec: 0.8448 - rec: 0.9584 - val_loss: 0.2501 - val_prec: 0.8555 - val_rec: 0.8654\n",
      "Epoch 4/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.2065 - prec: 0.8488 - rec: 0.9603 - val_loss: 0.2709 - val_prec: 0.8017 - val_rec: 0.8724\n",
      "Epoch 5/100\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2059 - prec: 0.8511 - rec: 0.9600 - val_loss: 0.3113 - val_prec: 0.7321 - val_rec: 0.9258\n",
      "Epoch 6/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.2040 - prec: 0.8521 - rec: 0.9612 - val_loss: 0.2645 - val_prec: 0.8668 - val_rec: 0.8005\n",
      "Epoch 7/100\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.2037 - prec: 0.8543 - rec: 0.9608 - val_loss: 0.2854 - val_prec: 0.7366 - val_rec: 0.9281\n",
      "Epoch 8/100\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.2011 - prec: 0.8586 - rec: 0.9625 - val_loss: 0.2726 - val_prec: 0.7778 - val_rec: 0.8933\n",
      "Epoch 9/100\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.1992 - prec: 0.8602 - rec: 0.9639 - val_loss: 0.2441 - val_prec: 0.8637 - val_rec: 0.9118\n",
      "Epoch 10/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1973 - prec: 0.8646 - rec: 0.9642 - val_loss: 0.2574 - val_prec: 0.8637 - val_rec: 0.8237\n",
      "Epoch 11/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1957 - prec: 0.8673 - rec: 0.9660 - val_loss: 0.2432 - val_prec: 0.8732 - val_rec: 0.8631\n",
      "Epoch 12/100\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.1936 - prec: 0.8713 - rec: 0.9670 - val_loss: 0.2668 - val_prec: 0.8843 - val_rec: 0.8515\n",
      "Epoch 13/100\n",
      "184/184 [==============================] - 51s 278ms/step - loss: 0.1925 - prec: 0.8725 - rec: 0.9657 - val_loss: 0.2666 - val_prec: 0.8682 - val_rec: 0.8097\n",
      "Epoch 14/100\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.1922 - prec: 0.8745 - rec: 0.9662 - val_loss: 0.2595 - val_prec: 0.8458 - val_rec: 0.8399\n",
      "Epoch 15/100\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.1904 - prec: 0.8789 - rec: 0.9673 - val_loss: 0.2374 - val_prec: 0.8555 - val_rec: 0.8794\n",
      "Epoch 16/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.1878 - prec: 0.8805 - rec: 0.9690 - val_loss: 0.2387 - val_prec: 0.8604 - val_rec: 0.8863\n",
      "Epoch 17/100\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.1862 - prec: 0.8846 - rec: 0.9690 - val_loss: 0.2645 - val_prec: 0.8430 - val_rec: 0.8724\n",
      "Epoch 18/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1860 - prec: 0.8855 - rec: 0.9690 - val_loss: 0.2548 - val_prec: 0.8897 - val_rec: 0.8237\n",
      "Epoch 19/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1851 - prec: 0.8884 - rec: 0.9698 - val_loss: 0.2520 - val_prec: 0.8555 - val_rec: 0.8515\n",
      "Epoch 20/100\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.1835 - prec: 0.8897 - rec: 0.9709 - val_loss: 0.2947 - val_prec: 0.7591 - val_rec: 0.9211\n",
      "Epoch 21/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1832 - prec: 0.8899 - rec: 0.9705 - val_loss: 0.2450 - val_prec: 0.8174 - val_rec: 0.9142\n",
      "Epoch 22/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1813 - prec: 0.8939 - rec: 0.9713 - val_loss: 0.2467 - val_prec: 0.8510 - val_rec: 0.8747\n",
      "Epoch 23/100\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.1796 - prec: 0.8971 - rec: 0.9726 - val_loss: 0.2725 - val_prec: 0.7952 - val_rec: 0.9281\n",
      "Epoch 24/100\n",
      "184/184 [==============================] - 50s 270ms/step - loss: 0.1785 - prec: 0.8993 - rec: 0.9734 - val_loss: 0.2791 - val_prec: 0.7784 - val_rec: 0.9374\n",
      "Epoch 25/100\n",
      "184/184 [==============================] - 50s 271ms/step - loss: 0.1762 - prec: 0.9030 - rec: 0.9744 - val_loss: 0.2429 - val_prec: 0.8509 - val_rec: 0.9002\n",
      "Epoch 26/100\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.1741 - prec: 0.9073 - rec: 0.9754 - val_loss: 0.2878 - val_prec: 0.8399 - val_rec: 0.7912\n",
      "Epoch 27/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1752 - prec: 0.9069 - rec: 0.9744 - val_loss: 0.2582 - val_prec: 0.8326 - val_rec: 0.9118\n",
      "Epoch 28/100\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.1745 - prec: 0.9082 - rec: 0.9745 - val_loss: 0.2787 - val_prec: 0.8330 - val_rec: 0.8561\n",
      "Epoch 29/100\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.1725 - prec: 0.9101 - rec: 0.9758 - val_loss: 0.2874 - val_prec: 0.7816 - val_rec: 0.9466\n",
      "Epoch 30/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1701 - prec: 0.9138 - rec: 0.9770 - val_loss: 0.2682 - val_prec: 0.8070 - val_rec: 0.9118\n",
      "Epoch 31/100\n",
      "184/184 [==============================] - 50s 273ms/step - loss: 0.1704 - prec: 0.9139 - rec: 0.9768 - val_loss: 0.2846 - val_prec: 0.8174 - val_rec: 0.8515\n",
      "Epoch 32/100\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.1693 - prec: 0.9164 - rec: 0.9766 - val_loss: 0.2475 - val_prec: 0.8531 - val_rec: 0.9026\n",
      "Epoch 33/100\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.1673 - prec: 0.9181 - rec: 0.9787 - val_loss: 0.2521 - val_prec: 0.8841 - val_rec: 0.8492\n",
      "Epoch 34/100\n",
      "184/184 [==============================] - 54s 292ms/step - loss: 0.1679 - prec: 0.9191 - rec: 0.9778 - val_loss: 0.2560 - val_prec: 0.8371 - val_rec: 0.8585\n",
      "Epoch 35/100\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.1651 - prec: 0.9237 - rec: 0.9791 - val_loss: 0.2525 - val_prec: 0.8581 - val_rec: 0.8701\n",
      "Epoch 36/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.1655 - prec: 0.9224 - rec: 0.9790 - val_loss: 0.2539 - val_prec: 0.8269 - val_rec: 0.8979\n",
      "Epoch 37/100\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.1639 - prec: 0.9251 - rec: 0.9799 - val_loss: 0.2933 - val_prec: 0.8238 - val_rec: 0.8677\n",
      "Epoch 38/100\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.1631 - prec: 0.9270 - rec: 0.9797 - val_loss: 0.2639 - val_prec: 0.8808 - val_rec: 0.8399\n",
      "Epoch 39/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.1627 - prec: 0.9288 - rec: 0.9798 - val_loss: 0.2725 - val_prec: 0.8510 - val_rec: 0.8747\n",
      "Epoch 40/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.1612 - prec: 0.9310 - rec: 0.9818 - val_loss: 0.2799 - val_prec: 0.7906 - val_rec: 0.9374\n",
      "Epoch 41/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1605 - prec: 0.9314 - rec: 0.9815 - val_loss: 0.3146 - val_prec: 0.7715 - val_rec: 0.9165\n",
      "Epoch 42/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.1598 - prec: 0.9333 - rec: 0.9815 - val_loss: 0.2990 - val_prec: 0.9308 - val_rec: 0.7494\n",
      "Epoch 43/100\n",
      "184/184 [==============================] - 51s 276ms/step - loss: 0.1591 - prec: 0.9353 - rec: 0.9830 - val_loss: 0.2585 - val_prec: 0.8442 - val_rec: 0.8677\n",
      "Epoch 44/100\n",
      "184/184 [==============================] - 51s 274ms/step - loss: 0.1583 - prec: 0.9370 - rec: 0.9818 - val_loss: 0.2822 - val_prec: 0.7771 - val_rec: 0.9304\n",
      "Epoch 45/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.1587 - prec: 0.9358 - rec: 0.9820 - val_loss: 0.2708 - val_prec: 0.8157 - val_rec: 0.9142\n",
      "Epoch 46/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.1578 - prec: 0.9373 - rec: 0.9826 - val_loss: 0.2733 - val_prec: 0.9150 - val_rec: 0.7494\n",
      "Epoch 47/100\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.1554 - prec: 0.9414 - rec: 0.9832 - val_loss: 0.2722 - val_prec: 0.8980 - val_rec: 0.8167\n",
      "Epoch 48/100\n",
      "184/184 [==============================] - 50s 274ms/step - loss: 0.1561 - prec: 0.9389 - rec: 0.9838 - val_loss: 0.2484 - val_prec: 0.8916 - val_rec: 0.8585\n",
      "Epoch 49/100\n",
      "184/184 [==============================] - 50s 271ms/step - loss: 0.1550 - prec: 0.9430 - rec: 0.9837 - val_loss: 0.2533 - val_prec: 0.8679 - val_rec: 0.8840\n",
      "Epoch 50/100\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.1534 - prec: 0.9461 - rec: 0.9842 - val_loss: 0.2546 - val_prec: 0.8467 - val_rec: 0.8840\n",
      "Epoch 51/100\n",
      "184/184 [==============================] - 50s 271ms/step - loss: 0.1539 - prec: 0.9447 - rec: 0.9842 - val_loss: 0.2905 - val_prec: 0.9366 - val_rec: 0.7541\n",
      "Epoch 52/100\n",
      "184/184 [==============================] - 50s 271ms/step - loss: 0.1518 - prec: 0.9485 - rec: 0.9858 - val_loss: 0.2485 - val_prec: 0.8485 - val_rec: 0.9095\n",
      "Epoch 53/100\n",
      "184/184 [==============================] - 51s 277ms/step - loss: 0.1530 - prec: 0.9466 - rec: 0.9846 - val_loss: 0.2676 - val_prec: 0.8732 - val_rec: 0.8631\n",
      "Epoch 54/100\n",
      "184/184 [==============================] - 50s 272ms/step - loss: 0.1508 - prec: 0.9492 - rec: 0.9866 - val_loss: 0.2397 - val_prec: 0.8613 - val_rec: 0.8933\n",
      "Epoch 55/100\n",
      " 89/184 [=============>................] - ETA: 24s - loss: 0.1490 - prec: 0.9515 - rec: 0.9876"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m model \u001b[39m=\u001b[39m compile_new_model(model)\n\u001b[1;32m     13\u001b[0m model\u001b[39m.\u001b[39mload_weights(\u001b[39m'\u001b[39m\u001b[39m/home/ubuntu/ml-data-training/ship_seg_weights/classification/res50_baseline_128_class_weights/81.h5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m model_hist \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     15\u001b[0m     train_dataset,\n\u001b[1;32m     16\u001b[0m     validation_data \u001b[39m=\u001b[39;49m val_dataset,\n\u001b[1;32m     17\u001b[0m     verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m     18\u001b[0m     epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m     steps_per_epoch \u001b[39m=\u001b[39;49m \u001b[39mlen\u001b[39;49m(train) \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m (batch_size \u001b[39m*\u001b[39;49m REPLICAS),\n\u001b[1;32m     20\u001b[0m     callbacks \u001b[39m=\u001b[39;49m [\n\u001b[1;32m     21\u001b[0m         tensorboard_callback,\n\u001b[1;32m     22\u001b[0m         weights_save\n\u001b[1;32m     23\u001b[0m     ],\n\u001b[1;32m     24\u001b[0m     class_weight \u001b[39m=\u001b[39;49m class_weight_dict\n\u001b[1;32m     25\u001b[0m )\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "log_dir = f\"{os.environ['tb_path']}classification/res50_baseline_128_class_weights_v1/\"\n",
    "if os.path.exists(log_dir) == False:\n",
    "    os.makedirs(log_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "weights_path = f'/home/ubuntu/ml-data-training/ship_seg_weights/classification/res50_baseline_128_class_weights/'\n",
    "weights_save = CallbackForSavingModelWeights(weights_path, epoch_number = 83)\n",
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(128, 128), batch_size = batch_size)\n",
    "val_dataset = get_data(val, shape=(128, 128), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (128, 128, 3))\n",
    "model = compile_new_model(model)\n",
    "model.load_weights('/home/ubuntu/ml-data-training/ship_seg_weights/classification/res50_baseline_128_class_weights/81.h5')\n",
    "model_hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    verbose = 1,\n",
    "    epochs = 100,\n",
    "    steps_per_epoch = len(train) // (batch_size * REPLICAS),\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        weights_save\n",
    "    ],\n",
    "    class_weight = class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_dataset = get_data(train, shape=(128, 128), batch_size = batch_size, repeat=False, shuffle=False)\n",
    "val_dataset = get_data(val, shape=(128, 128), repeat = False, shuffle = False, batch_size=batch_size)\n",
    "model = create_model('ResNet50', (128, 128, 3))\n",
    "model = compile_new_model(model)\n",
    "model.load_weights('/home/ubuntu/ml-data-training/ship_seg_weights/classification/res50_baseline_128_class_weights/72.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1475/1475 [==============================] - 65s 41ms/step - loss: 0.2116 - prec: 0.8634 - rec: 0.9335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21163372695446014, 0.8634044528007507, 0.9334516525268555]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 59ms/step - loss: 0.2455 - prec: 0.8421 - rec: 0.8910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24549339711666107, 0.8421052694320679, 0.8909512758255005]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = get_data(test, shape=(128, 128), repeat = False, shuffle = False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 49ms/step - loss: 0.2330 - prec: 0.8410 - rec: 0.9061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2329612523317337, 0.8409585952758789, 0.9061033129692078]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs = val_dataset.map(lambda x, y: x)\n",
    "val_labels = val_dataset.map(lambda x, y: y)\n",
    "test_imgs = test_dataset.map(lambda x, y: x)\n",
    "test_labels = test_dataset.map(lambda x, y: y)\n",
    "train_imgs = train_dataset.map(lambda x, y: x)\n",
    "train_labels = train_dataset.map(lambda x, y: y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_g_truth = np.array(val['class_labels'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 2s 40ms/step\n",
      "16/16 [==============================] - 1s 44ms/step\n",
      "1475/1475 [==============================] - 57s 38ms/step\n"
     ]
    }
   ],
   "source": [
    "val_preds = model.predict(val_imgs, verbose=1)\n",
    "test_preds = model.predict(test_imgs, verbose=1)\n",
    "train_preds = model.predict(train_imgs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = val_preds.flatten()\n",
    "test_preds = test_preds.flatten()\n",
    "train_preds = train_preds.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_0_5 = np.where(val_preds > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_index = np.where(np.logical_and(val_g_truth == 0, pred_0_5 == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_index = np.where(np.logical_and(val_g_truth == 1, pred_0_5 == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  18,   41,   92,  112,  126,  135,  150,  187,  239,  241,  252,\n",
       "         301,  306,  362,  366,  403,  407,  419,  459,  548,  552,  560,\n",
       "         561,  593,  605,  680,  749,  775,  797,  816,  838,  859,  861,\n",
       "         872,  940,  960,  965,  978, 1019, 1029, 1089, 1097, 1108, 1155,\n",
       "        1169, 1206, 1262, 1286, 1419, 1435, 1444, 1532, 1563, 1584, 1646,\n",
       "        1669, 1693, 1694, 1701, 1710, 1713, 1737, 1747, 1754, 1805, 1819,\n",
       "        1822, 1842, 1845, 1874, 1875, 1884]),)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for x in fp_index[0]:\n",
    "fp_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  95,  145,  213,  223,  254,  263,  275,  280,  340,  344,  377,\n",
       "         429,  466,  540,  574,  591,  608,  612,  671,  686,  736,  777,\n",
       "         780,  821,  934, 1013, 1028, 1067, 1085, 1094, 1171, 1185, 1187,\n",
       "        1240, 1245, 1259, 1298, 1308, 1312, 1353, 1396, 1423, 1437, 1633,\n",
       "        1773, 1791, 1810]),)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = val['fixed_paths'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['class_labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.405276, G-Mean=0.948\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyD0lEQVR4nO3deXgUVdbA4d9JCIRACISwhxBW2TcDiAwKIouKMK44oII6w+i464eAoIKKy4igjo6KggviNoiKoIKObKMomzHsyk7Y1wAJgSzn+6M6GCAkHZNKp7vP+zwxXVW3q04B9ulbt+pcUVWMMcYErxBfB2CMMca3LBEYY0yQs0RgjDFBzhKBMcYEOUsExhgT5Mr4OoDCiomJ0fj4eF+HYYwxfmX58uX7VbVaXtv8LhHEx8ezbNkyX4dhjDF+RUS2nmubXRoyxpggZ4nAGGOCnCUCY4wJcpYIjDEmyFkiMMaYIOdaIhCRKSKyV0RWnWO7iMhLIrJBRJJEpL1bsRhjjDk3N28ffRt4GXj3HNsvAxp7fjoBr3p+G3Nu3zwG378IWNVcv1QmAjLTfB1FMRDPb3VeR9WF1P0QEgoZaaBZzrlmZzg/p94WCiFlIOuEsxwVB/evhGVvw7yn4OQxaHoFXPOGs27t59CsPyQMge1LYMsiiO8KdTsW79m4WYZaROKBWaraMo9trwPzVfUDz/J6oJuq7spvnwkJCWrPEZzDu1fBpu98HYUxpjBCy/2eGHJUrAnHdp9azKzdgdDdPyOa7bQfPLPQyUBElqtqQl7bfPlAWR1ge67lZM+6sxKBiAwFhgLExcWVSHA+MybK1xEYY0rSmUkATksCCoTsXJqr/UmnZ1CMvQK/eLJYVScBk8DpEfg4nKIZUxm7rGGMOSW8CqQfOn1d7fPRncud1wofhF3FDfoVodkZEFrWuTxUjHyZCHYAdXMtx3rWBY6x0c61QmOMw8YIvBojyLpqEi88O4oOaQs51vAKrhn0EKG7l7s2RuDLRDATuEtEPsQZJE4paHyg1LNr9CWnQg0Y9quvozCmeCQMgYQhHEo9SeWIMEJFaHHlPVSu/BAXxVZ22tTtWOwJIIdriUBEPgC6ATEikgw8BoQBqOprwJfA5cAGIA24xa1YXFcar+uPSfF1BMYYL6kqnyXuYOwXaxjepyl/6RhHn5Y1S+z4riUCVf1LAdsVuNOt45cINxNAl/ug51j39m+MKRV2Hj7OqE9XMm/9PtrFVSahXpUSj8EvBotLneJKAPat3Zig9nniDkZ9uoqsbOXRvs0ZfGE8oSFS8BuLmSUCL02bNo1Ro0ax4cZ9hIaWAfl9uMgr9qFvjDlDVPkw2tatzNNXt6JudITP4nD1gTI3+OKBsmnTpjFg7VBCQkIQcT7+c36fk33wG2POkJmVzeT/bSYjK5u7LmkMOOMDBX6eFIPS+kCZ3xiw9h+EhoYCpycAVTjr788SgDEmD2t2HmH4J0ms3JHCFa1rnUoAJZEECmKJoCBjoggNdXpNpyeBnJ6UZ50lAGNMHk5kZvHydxt4df5GKkeE8e9B7bmsZc1SkQByWCLIz6lBYTn1zT/3pbSsLKHMk5YAjDHntmV/Gq8t2Ei/trV55IrmVKlQ1tchncUSwbnkujPozCSgqmRnZ/NRs0kM8kVsxphSLfVEJt+s2cOf29XhvJqR/PeBbsRV9d1gcEFsYpq85HF7qOfiEFlZWTR4t6qTBAZZGjDGnG7Rb/vo/cJC7v84kQ17jwKU6iQA1iM42zmeERDPf8o8mcqWJ0s0ImOMH0hJy2Dcl2v4eFkyDWIq8NHQzjSqHunrsLxiiaAwbEDYGJOHrGzlmtd+YPP+VP7RrSH39GhMeFior8PymiWC3PJ7YtiSgDHmDAdTT1K5fBihIcKw3udRp3J5WtYphbXHCmBjBDksCRhjvKSqfLI8me7j5/PhUmd+rd4tavplEgDrERhjTKEkH0rj4U9XsfDXfZxfrwod60f7OqQis0QA1hswxnjl05+TGf3pKhQY268FN11QjxAfFIkrbpYI8mNJwBiTS3SFcpwfH81TV7UktkrpviW0MCwRvOzOjD/GGP+XkZXNG4s2kZml3NOjMRc3qcZFjWNKVXmI4mCJYP/6vNdbb8CYoLZqRwrDP0li9c4jXNmmdqkqElfcLBEYY0wu6RlZvPTf33h94SaqRJTltRvb06dlLV+H5argTgTnGiS23oAxQWvrgTTeWLSJq9vVYfQVzYmKCPN1SK4L7kRgjDE4ReLmrN7N1e1jOa9mJN892M2nM4aVNEsExpigtuDXfTw8YyU7U47TOjaKRtUjgyoJgCWCs9llIWOCwqHUkzwxew0zVuygYbUK/Ofv/lMkrrgFbyLI7yEyY0xAyykSt/VAGnd1b8RdlzTyqyJxxS14E4ExJugcOHaCKhFlCQ0RRvRpSp0q5WlR274UWtE5Y0zAU1U+Xrad7uPn88HSbQD0alHTkoCH9Qhys/EBYwLO9oNpPPzpShb9tp+O8dF0blDV1yGVOsGZCGx8wJigMGNFMqM/W4UAT/y5JYM6xgVEkbjiFpyJwBgTFGIqlqNj/WjGXdWKOpXL+zqcUssSgTEmYGRkZfP6go1kZcO9lzbmoibVuKhJNV+HVepZIshh4wPG+LVVO1IYNj2JtbuO0L/t70XiTMEsERhj/Fp6RhYvfPsbbyzaRHSFsrx+0/n0blHT12H5FVdvHxWRPiKyXkQ2iMiIPLbHicg8EflZRJJE5HI34zHGBJ5tB9OY/L9NXNs+lm/vv9iSwB/gWo9AREKBV4CeQDKwVERmquqaXM1GAx+r6qsi0hz4Eoh3KybA7hgyJgAcTc/g61W7uS6hLk1qRDLv/7oF1IxhJc3NS0MdgQ2quglARD4E+gO5E4EClTyvo4CdLsZjjAkA89btZdSnK9l9JJ12cZVpVD3SkkARuZkI6gDbcy0nA53OaDMGmCsidwMVgEvz2pGIDAWGAsTFxRV7oMaY0u9g6kmemLWGT3/eQePqFZl+x4VBWySuuPl6sPgvwNuq+ryIdAamikhLVc3O3UhVJwGTABISErTYo7A7howp1bKylWtf/YFtB9O4p0dj7uzekHJlgrdIXHFzMxHsAOrmWo71rMvtNqAPgKouFpFwIAbY62Jcxhg/se/oCapWcIrEPXx5M+pUKU+zWpUKfqMpFDfvGloKNBaR+iJSFrgBmHlGm21ADwARaQaEA/tcjMkY4wdUlY+WbuOS5+fz/hKnSNylzWtYEnCJaz0CVc0UkbuAOUAoMEVVV4vI48AyVZ0JPAi8ISL34wwcD1HV4r/0Y4zxG9sOpDFiRhI/bDxAp/rR/KlRjK9DCniujhGo6pc4t4TmXvdortdrgC5uxmCM8R/TlyfzyGerCA0Rxl3Vkr90sCJxJcHXg8XGGHNKjUrluLBhVZ68qiW1oqxIXEmxRGCM8ZmTmdm8On8j2arc37MJXRtXo2tjKxJX0oIrEdhTxcaUGr9sP8xD05NYv+coV7erY0XifCi4EoExxueOn8xiwjfrmfy/zVSPDOfNmxO4tHkNX4cV1CwRGGNK1PZDabzzw1Zu6BjHiMuaUik8zNchBT1LBPZUsTGuO+IpEne9p0jc/GHdqG0zhpUalgiMMa76bt0eHp6xir1H02kfV4VG1StaEihlLBEYY1xx4NgJHp+1hs8Td3JejUheu+l8GlWv6OuwTB4sERhjil1WtnLda4vZfiiN+y9twh3dGlK2jKvzYJkisERgjCk2e4+mE1OhHKEhwqgrmhFbJYLzalqp6NLO6xQtIjbzgzEmT9nZyrSftnLJ+AVM8xSJ69GshiUBP1FgIhCRC0VkDbDOs9xGRP7temTGGL+wZX8qA9/8kVGfrqJ1bBQX25PBfsebS0MTgd54Skir6i8icpGrURlj/MLHy7bzyGerKBsawjNXt2JAh7r2dLAf8mqMQFW3n/GXm+VOOMYYf1KncnkualKNJ/q3pGZUuK/DMX+QN4lgu4hcCKiIhAH3AmvdDcsYUxqdyMzi3/M2oqo80Os8ujSKoYvNF+D3vEkEtwMv4kxGvwOYC/zDzaCMMaXPz9sOMfyTJH7dc4xr2sdakbgA4k0iOE9VB+VeISJdgO/dCckYU5qknczk+bm/MuX7zdSsFM6UIQlc0tSKxAUSbxLBv4D2XqwzxgSgHYeOM/XHrQzqFMfwPk2JtCJxAeeciUBEOgMXAtVE5IFcmyrhzEHsX2wuAmO8lnI8g69W7uKGjnE0rhHJgmHdbMawAJZfj6AsUNHTJvdTIUeAa90MyhjjO3NX72b0Z6s4kHqShPhoGlWvaEkgwJ0zEajqAmCBiLytqltLMCZjjA/sP3aCMTNXMytpF01rRvLm4AQrEhckvBkjSBOR54AWwKkbhVX1EteiKik2F4ExgFMk7tpXf2Dn4XT+r1cT/n5xQ8JCrUhcsPAmEUwDPgL64txKOhjY52ZQxpiSsedIOtUqOkXiHruyBbFVytO4htUHCjbepPyqqjoZyFDVBap6K+D/vQFjglh2tjL1x630eH4B035yrvx2b1rdkkCQ8qZHkOH5vUtErgB2AtHuhWSMcdOmfccYMWMlSzYf5E+NYuh2XnVfh2R8zJtE8KSIRAEP4jw/UAm4z82gjDHu+GjpNh79fDXlyoTwz2tbc935sfZ0sCk4EajqLM/LFKA7nHqy2BjjZ2KrRNDtPKdIXPVKViTOOPJ7oCwUuB6nxtDXqrpKRPoCDwPlgXYlE6Ix5o86kZnFv/67AYD/621F4kze8usRTAbqAkuAl0RkJ5AAjFDVz0ogNmNMESzfepCHpiexcV8q1ydYkThzbvklggSgtapmi0g4sBtoqKoHSiY0Y8wfkXoik+fmrOedxVuoHVWed27tyMVNbNYwc2753T56UlWzAVQ1HdhU2CQgIn1EZL2IbBCREedoc72IrBGR1SLyfmH2b4w5287Dx3l/yTZuvqAec+6/yJKAKVB+PYKmIpLkeS1AQ8+yAKqqrfPbsWeM4RWgJ5AMLBWRmaq6JlebxsBIoIuqHhIRu4/NmD8gJS2D2St3MbCTUyRu0UPdqWGDwcZL+SWCZkXcd0dgg6puAhCRD4H+wJpcbf4GvKKqhwBUdW8Rj2lM0Pl61W4e+XwVB1NP0qlBNA2rVbQkYAolv6JzRS00VwfYnms5Geh0RpsmACLyPU5p6zGq+vWZOxKRocBQgLi4uCKGZUxg2Hs0nTEzV/Plyt00r1WJt4Z0oGE1KxJnCs+ryetdPn5joBsQCywUkVaqejh3I1WdBEwCSEhI0BKO0ZhSJytbuf61xexMSWdY7/MYelEDKxJn/jA3E8EOnNtPc8R61uWWDPykqhnAZhH5FScxLHUxLmP81q6U49SIDHeKxPVrQd0qEVYq2hSZV18hRKS8iJxXyH0vBRqLSH0RKQvcAMw8o81nOL0BRCQG51LRpkIex5iAl52tvP39Zno8v4D3corEnVfdkoApFgUmAhG5EkgEvvYstxWRMz/Qz6KqmcBdwBxgLfCxqq4WkcdFpJ+n2RzggIisAeYBw+w5BWNOt2HvMa5/fTFjvlhDQnw0lzS1m+tM8fLm0tAYnDuA5gOoaqKI1Pdm56r6JfDlGesezfVagQc8P8aYM3y4ZBuPzlxN+bBQnr+uDVe3r2NPB5ti51UZalVNOeMfnw3YGlMC4qpGcGmz6ozt15JqkeV8HY4JUN4kgtUiMhAI9TwAdg/wg7thGROc0jOyeOm/vwHwUJ+mXNgwhgsbWpE44y5vBovvxpmv+ATwPk456vtcjMmYoLRsy0Euf2kR/56/kYOpJ3GunBrjPm96BE1VdRQwyu1gjAlGx05k8tzX63j3x63UqVyed2/tyEVWH8iUIG8SwfMiUhOYDnykqqtcjsmYoLI75TgfLt3O4M7xDOt9HhXK+fo5TxNsCrw0pKrdcWYm2we8LiIrRWS065EZE8AOpZ5k6o/O8wCNqjtF4sb0a2FJwPiEVw+UqepuVX0JuB3nmYJH83+HMSYvqsqXK3fRc+ICxs5czcZ9xwBs2kjjUwV+/RCRZsAA4BrgAPARzkT2xphC2HsknUc+X8Wc1XtoVSeKd2/tZEXiTKngTT90Cs6Hf29V3elyPMYEpKxs5brXF7M7JZ2RlzXltj/Vp4wViTOlRIGJQFU7l0QgxgSinYePU7OSUyTu8f4tqVulPA2sF2BKmXN+JRGRjz2/V4pIUq6flblmLjPG5CErW3nrjCJxFzepZknAlEr59Qju9fzuWxKBGBMoNuw9ykPTk1ix7TDdzqtGj2Y1fB2SMfnKb4ayXZ6X/1DV4bm3icizwPCz32VMcHv/p22MmbmaCuVCmTigDX9ua0XiTOnnzWhVzzzWXVbcgRgTCOJjIujVogbfPHAxV7WLtSRg/MI5ewQicgfwD6DBGWMCkcD3bgdmjD9Iz8hi4re/IggjLrMiccY/5TdG8D7wFfA0MCLX+qOqetDVqIzxAz9tOsCIGSvZvD+VQZ3iUFXrARi/lF8iUFXdIiJ3nrlBRKItGZhgdTQ9g2e/Xsd7P24jLjqC9//aiQsbWS/A+K+CegR9geU4E9Hk/qqjQAMX4zKm1Npz5ATTlyfz1z/V54FeTYgoa/WBjH/L766hvp7fXk1LaUwgO5h6ktlJO7mpczyNqldk0UOX2IxhJmB4U2uoC5CoqqkiciPQHnhBVbe5Hp0xPqaqzEraxZiZqzmSnkGXRjE0qFbRkoAJKN7cPvoqkCYibXCKzW0EproalTGlwJ4j6fzt3eXc/cHP1KlSni/u/pM9GWwCkjcXNzNVVUWkP/Cyqk4WkdvcDswYX8rKVq73FIkbdXkzbukSb0XiTMDyJhEcFZGRwE1AVxEJAcLcDcsY30g+lEatqPKEhghP9G9JXHQE8TEVfB2WMa7y5ivOAJyJ629V1d1ALPCcq1EZU8KyspU3F23i0gkLeM8zc9hFTapZEjBBwZsy1LtFZBrQQUT6AktU9V33QzOmZKzffZSHPknil+2H6dG0Or1aWJE4E1y8uWvoepwewHycZwn+JSLDVHW6y7EZ47r3ftzK2C9WExkexos3tKVfm9r2dLAJOt6MEYwCOqjqXgARqQZ8C1giMH4rpxxEo+oVubxVLR7t25yqFe2WUBOcvEkEITlJwOMAXk56b0xpc/xkFhO+WU9IiDDysmZc0KAqFzSo6uuwjPEpbxLB1yIyB/jAszwA+NK9kIxxx+KNBxgxI4mtB9K46YJ6ViTOGA9vBouHicjVwJ88qyap6qfuhmVM8TmSnsHTX67jgyXbqFc1gvf/1slKRRuTS37zETQGxgMNgZXA/6nqjpIKrFiNqezrCIwP7T1ygs9+3sHQixpw/6VNKF821NchGVOq5HetfwowC7gGpwLpvwq7cxHpIyLrRWSDiIzIp901IqIiklDYY3hH3dmtKbUOHDvB299vBqBR9Yr8b3h3Hr68mSUBY/KQ36WhSFV9w/N6vYisKMyORSQUeAVnqstkYKmIzFTVNWe0iwTuBX4qzP6LrMt9JXo4UzJUlZm/7GTMzNUcO5HJRU2q0aBaRbsjyJh85JcIwkWkHb/PQ1A+97KqFpQYOgIbVHUTgIh8CPQH1pzR7gngWWBYIWMvmp5jS/Rwxn07Dx9n9Ger+G7dXtrWrcw/r21tReKM8UJ+iWAXMCHX8u5cywpcUsC+6wDbcy0nA51yNxCR9kBdVZ0tIudMBCIyFBgKEBcXV8BhTTDKzMrmhkk/su/oCR7p25whF8YTGmJ3BBnjjfwmpunu5oE9xesmAEMKaquqk4BJAAkJCXbB35yy/WAatSuXp0xoCE9d1Yq46Ajiqkb4Oixj/IqbD4btAOrmWo71rMsRCbQE5ovIFuACYKZ7A8YmkGRmZTNp4UYunbCAqYu3APCnxjGWBIz5A9ycbHUp0FhE6uMkgBuAgTkbVTUFOHUzt4jMx7lFdZmLMZkAsHbXEYZ/kkRScgo9m9fgsla1fB2SMX7NtUSgqpkichcwBwgFpqjqahF5HFimqjPdOrYJXFMXb2HsF2uIKh/GywPbcUWrWvZ0sDFF5E31UQEGAQ1U9XERiQNqquqSgt6rql9yRjkKVX30HG27eRWxCUo55SCa1Ijkyja1eaRvc6IrlPV1WMYEBG96BP8GsnHuEnocOAp8AnRwMS5jAEg7mcn4Ob9SJlR4+PJmdGpQlU5WJM6YYuXNYHEnVb0TSAdQ1UOAfRUzrvt+w356v7CQKd9v5mRmNqp2w5gxbvCmR5DheUpY4dR8BNmuRmWCWsrxDJ6avZaPlm2nfkwFPv57ZzrWj/Z1WMYELG8SwUvAp0B1ERkHXAuMdjUqE9T2HzvBF0k7uf3ihtx3aWPCw6w+kDFu8qYM9TQRWQ70wCkv8WdVXet6ZCao7Dt6gi9+2cmtf6pPw2oV+d/wS2ww2JgS4s1dQ3FAGvBF7nWqus3NwExwUFU+S9zB2C/WkHYii+5Nq1M/poIlAWNKkDeXhmbjjA8IEA7UB9YDLVyMywSBHYePM+rTlcxfv4/2cU6RuPoxFXwdljFBx5tLQ61yL3sKxf3DtYhMUHCKxC3mwLGTjLmyOTd1tiJxxvhKoZ8sVtUVItKp4JbGnG3bgTTqVHGKxD1zdWvioiOoG231gYzxJW/GCB7ItRgCtAd2uhaRCUiZWdm8sWgzE7/9lZGXNeWWLvXp0sjmDTamNPCmRxCZ63UmzpjBJ+6EYwLR6p0pDP8kiVU7jtC7RQ2usCJxxpQq+SYCz4Nkkar6fyUUjwkw7/ywhSdmraFyRFleHdTeKoUaUwqdMxGISBlPBdEuJRmQCQw5ReKa1oykf9s6PNK3GZUj7JZQY0qj/HoES3DGAxJFZCbwHyA1Z6OqznA5NuOHUk9k8tyc9YSFCqOuaG5F4ozxA96MEYQDB3Cqj+Y8T6CAJQJzmoW/7mPkjJXsTDnO4M7xp3oFxpjSLb9EUN1zx9Aqfk8AOawMpDklJS2DJ2avYfryZBpUc4rEdYi3InHG+Iv8EkEoUJHTE0AOSwTmlP2pJ/hq5S7+0a0h9/SwInHG+Jv8EsEuVX28xCIxfmXv0XRmJu7kr10bnCoSV8XqAxnjl/JLBHZx15xFVflkxQ6emLWG4xlZ9GhWg/oxFSwJGOPH8ksEPUosCuMXth9M4+FPV7Lot/0k1KvCM9dYkThjAsE5E4GqHizJQEzplpmVzV/e+JFDqSd5on8LBnWqR4gViTMmIBS66JwJLlv2p1I3OoIyoSH881qnSFxsFSsSZ0wg8WbyehOEMrKyeWXeBnpNXMi7i7cAcGHDGEsCxgQg6xGYs6zakcJD05NYs+sIV7SqRd/WtX0dkjHGRZYIzGne+n4zT85eS3SFsrx24/n0aVnT1yEZY1xmicAAvxeJa1E7iqvb1WH0Fc2JigjzdVjGmBJgiSDIHTuRyT+/XkfZ0BBG921Ox/rRdKxv5SGMCSY2WBzE5q/fS++JC5n641YUp1dgjAk+1iMIQodST/LE7DXMWLGDRtUrMv32Czm/XhVfh2WM8RFLBEHoUNpJ5q7ewz2XNOLOSxpRrowViTMmmLmaCESkD/AiTiXTN1X1mTO2PwD8FWcu5H3Araq61c2YgtXeI+l8lriDv3VtQINqFfl++CU2GGz8TkZGBsnJyaSnp/s6lFIrPDyc2NhYwsK8///btUTgme/4FaAnkAwsFZGZqromV7OfgQRVTRORO4B/AgPciikYqSr/WZbME7PXcDIzm57Na1I/poIlAeOXkpOTiYyMJD4+3iY9yoOqcuDAAZKTk6lfv77X73NzsLgjsEFVN6nqSeBDoH/uBqo6T1XTPIs/ArEuxhN0th9M46bJS3jokySa1arEV/d2tSJxxq+lp6dTtWpVSwLnICJUrVq10D0mNy8N1QG251pOBjrl0/424Ku8NojIUGAoQFxcXHHFF9ByisQdTsvgyT+3ZGDHOCsSZwKCJYH8/ZE/n1IxWCwiNwIJwMV5bVfVScAkgISEBLvHMR+b96cS5ykS99y1bahXNYLalcv7OixjTCnm5qWhHUDdXMuxnnWnEZFLgVFAP1U94WI8AS0jK5t//fc3ek9cyDs/bAGgc8OqlgSMKWYiwoMPPnhqefz48YwZM8br9+/Zs4e+ffvSpk0bmjdvzuWXXw7A/Pnz6du371ntZ86cyTPPOPfZjBkzhvHjxwMwZMgQpk+fXoQz+Z2bPYKlQGMRqY+TAG4ABuZuICLtgNeBPqq618VYAlpS8mEemp7Eut1HubJNbfq1tSJxxrilXLlyzJgxg5EjRxITE1Po9z/66KP07NmTe++9F4CkpKR82/fr149+/fr9oVi95VoiUNVMEbkLmINz++gUVV0tIo8Dy1R1JvAcUBH4j+e61jZVdfeMA8yU/23mydlrqBZZjjduTqBn8xq+DsmYEjPg9cVnrevbuhY3dY7n+Mkshry15Kzt154fy3UJdTmYepI73lt+2raP/t65wGOWKVOGoUOHMnHiRMaNG3fati1btnDrrbeyf/9+qlWrxltvvXXWuOauXbvo1avXqeXWrVufdYylS5cydOhQpk+fzqJFi1i2bBkvv/xygbH9Ua6WmFDVL1W1iao2VNVxnnWPepIAqnqpqtZQ1baeH0sCXsopB9E6NooBHeoy9/6LLQkYU0LuvPNOpk2bRkpKymnr7777bgYPHkxSUhKDBg3innvuyfO9t912G927d2fcuHHs3LnztO0//PADt99+O59//jkNGzZ09TxylIrBYuO9o+kZPPPVOsqVCeXRK5uTEB9NQrwViTPBKb9v8OXLhua7PbpCWa96AHmpVKkSN998My+99BLly/8+Drd48WJmzJgBwE033cRDDz101nt79+7Npk2b+Prrr/nqq69o164dq1atAmDt2rUMHTqUuXPnUrt2yV3itaJzfmTeur30mriQD5Zso0yoWJE4Y3zovvvuY/LkyaSmphb6vdHR0QwcOJCpU6fSoUMHFi5cCECtWrUIDw/n559/Lu5w82WJwA8cTD3JfR/+zC1vLyUyvAyf3HEhD1/ezO6nNsaHoqOjuf7665k8efKpdRdeeCEffvghANOmTaNr165nve+7774jLc15jvbo0aNs3Ljx1DhC5cqVmT17NiNHjmT+/Pnun4SHJQI/kHI8g/+u3cu9PRoz6+6utIuzSqHGlAYPPvgg+/fvP7X8r3/9i7feeovWrVszdepUXnzxxbPes3z5chISEmjdujWdO3fmr3/9Kx06dDi1vUaNGsyaNYs777yTn376qUTOQ/zt8kJCQoIuW7ascG8aE5XHupSz15Uiu1OcInF/v6gBIkLK8Qyiylt9IBPc1q5dS7NmzXwdRqmX15+TiCxX1YS82ttgcSmjqny4dDtPzV5LRnY2fVrUJD6mgiUBY4xrLBGUIlsPpDLik5Us3nSACxpE88zVrYm3InHGGJdZIiglMrOyGfjGT6Qcz+Cpq1pxQ4e6ViTOGFMiLBH42MZ9x6jnKRL3/PVOkbhaUVYfyBhTcuyuIR85mZnNC9/+Sp8XFvLuYmdStgsaVLUkYIwpcdYj8IHE7YcZPj2J9XuO0r9tbf7cro6vQzLGBDHrEZSwyf/bzNX//p6U4xlMHpzAize0I7pCWV+HZYzxUsWKFYu8j2XLluVZhyjHli1beP/9971uX1TWIyghqoqI0LZuFDd0jGPEZU2pFG63hBrjuu1LYMsiiO8KdTv6OhoAEhISSEjI85Z+4PdEMHDgQK/aF5UlApcdSc/g6S/XER4WwmNXtuD8etGcX8+KxBlTZF+NgN0r829z4gjsWQWaDRICNVpCuUrnbl+zFVz2TKFDSUxM5PbbbyctLY2GDRsyZcoUqlSpwtKlS7ntttsICQmhZ8+efPXVV6xatYr58+czfvx4Zs2axYIFC07NTSAiLFy4kBEjRrB27Vratm3L4MGDadeu3an2x44d4+6772bZsmWICI899hjXXHNNoWPOzS4NuejbNXvoOWEBHy3dRtkyIVYkzpiSlp7iJAFwfqe7U1Hg5ptv5tlnnyUpKYlWrVoxduxYAG655RZef/11EhMTCQ0NzfO948eP55VXXiExMZFFixZRvnx5nnnmGbp27UpiYiL333//ae2feOIJoqKiWLlyJUlJSVxyySVFjt96BC44cOwEY79Yw8xfdtK0ZiSTbkqgTd3Kvg7LmMDizTf37UvgnX6QdRJCy8I1bxb75aGUlBQOHz7MxRc7U64PHjyY6667jsOHD3P06FE6d3ZKXQ8cOJBZs2ad9f4uXbrwwAMPMGjQIK6++mpiY2PzPd633357qrAdQJUqRa89ZonABUfTM5m3fi/3X9qEO7o1pGwZ63gZ4xN1O8LgmaVujCC3ESNGcMUVV/Dll1/SpUsX5syZU+Ix2CdUMdl5+DivzNuAqhIfU4HvR1zCvZc2tiRgjK/V7QhdH3QtCURFRVGlShUWLVoEwNSpU7n44oupXLkykZGRpyqI5v4Wn9vGjRtp1aoVw4cPp0OHDqxbt47IyEiOHj2aZ/uePXvyyiuvnFo+dOhQkc/BPqWKKDtbee/HrfSauJCXv9vA1gNOnXG7I8iYwJSWlkZsbOypnwkTJvDOO+8wbNgwWrduTWJiIo8++igAkydP5m9/+xtt27YlNTWVqKizKyG/8MILtGzZktatWxMWFsZll11G69atCQ0NpU2bNkycOPG09qNHj+bQoUO0bNmSNm3aMG/evCKfk5WhLoLN+1MZ8UkSP20+SJdGVXn6qtbEVY0o8n6NMXnztzLUx44dO/XcwTPPPMOuXbvynKOguFkZ6hKSmZXNjW/+xJH0DP55TWuuS4i1GcOMMaeZPXs2Tz/9NJmZmdSrV4+3337b1yHlyRJBIW3Ye5T4qhUoExrCxAFtqVc1ghqVwn0dljGmFBowYAADBgzwdRgFsjECL53IzGLCN7/S54VFvOMpEtexfrQlAWOM37MegRdWbDvE8OlJ/Lb3GFe3q8PVViTOGBNALBEU4I2Fm3jqq7XUqhTOW7d0oPt51X0dkjHGFCtLBOeQna2EhAjt61VmUKc4hvdpSqTdEmqMCUA2RnCGlOMZPDT9F8Z+sRqA8+tF8+SfW1kSMMYAEBoaStu2bWnTpg3t27fnhx9++EP7eeGFF0hLSyvm6P4YSwS5zFm9m54TFvDJih1UKFfGisQZ4+emTZtGfHw8ISEhxMfHM23atCLvs3z58iQmJvLLL7/w9NNPM3LkyD+0n9KUCOzSELD/2Ake+3w1s1fuonmtSkwZ0oGWdfJ4CM0Y4zemTZvG0KFDT33Ybt26laFDhwIwaNCgYjnGkSNHTiv69txzz/Hxxx9z4sQJrrrqKsaOHUtqairXX389ycnJZGVl8cgjj7Bnzx527txJ9+7diYmJKZang4vCEgFwLD2TRb/tY1jv8xh6UQPCQq2jZIy/GzVq1FnfuNPS0hg1alSREsHx48dp27Yt6enp7Nq1i++++w6AuXPn8ttvv7FkyRJUlX79+rFw4UL27dtH7dq1mT17NuBUK42KimLChAnMmzePmJiYP36SxSRoP/F2HD7Oy9/9dqpI3A8je3Bn90aWBIwJENu2bSvUem/lXBpat24dX3/9NTfffDOqyty5c5k7dy7t2rWjffv2rFu3jt9++41WrVrxzTffMHz4cBYtWpRnvSFfc/VTT0T6iMh6EdkgIiPy2F5ORD7ybP9JROLdjAdAgWyg14QFvDJv46kicRXLWefImEASFxdXqPV/ROfOndm/fz/79u1DVRk5ciSJiYkkJiayYcMGbrvtNpo0acKKFSto1aoVo0eP5vHHHy+24xcX1xKBiIQCrwCXAc2Bv4hI8zOa3QYcUtVGwETgWVeCCTn9jp8TmaG0r1eFufdfRHxMBVcOaYzxrXHjxhERcXoRyIiICMaNG1dsx1i3bh1ZWVlUrVqV3r17M2XKFI4dOwbAjh072Lt3Lzt37iQiIoIbb7yRYcOGsWLFCoB8S02XNDe/BncENqjqJgAR+RDoD6zJ1aY/MMbzejrwsoiIFvftOqFl0ewMckrClSkXxru3drQiccYEsJxxgFGjRrFt2zbi4uIYN25ckQeKc8YIAFSVd955h9DQUHr16sXatWtPzUhWsWJF3nvvPTZs2MCwYcMICQkhLCyMV199FYChQ4fSp08fateu7fPBYtfKUIvItUAfVf2rZ/kmoJOq3pWrzSpPm2TP8kZPm/1n7GsoMBQgLi7u/K1btxYumOeaQOoewLk0JBVqwLBf/+CZGWN8xd/KUPtKYctQ+8XIqKpOUtUEVU2oVq1a4XfQ/eFTL+WMZWOMCXZuXhraAdTNtRzrWZdXm2QRKQNEAQeKPZKEIc7vtZ9Ds/6/LxtjjHE1ESwFGotIfZwP/BuAgWe0mQkMBhYD1wLfFfv4QI6EIZYAjAkAqmrje/n4Ix+hrl0aUtVM4C5gDrAW+FhVV4vI4yLSz9NsMlBVRDYADwBn3WJqjDE5wsPDOXDggJV/OQdV5cCBA4SHF26elOCYs9gYExAyMjJITk4mPT3d16GUWuHh4cTGxhIWdvpt8zZnsTEmIISFhVG/fn1fhxFw/OKuIWOMMe6xRGCMMUHOEoExxgQ5vxssFpF9QCEfLT4lBthfYKvAYuccHOycg0NRzrmequb5RK7fJYKiEJFl5xo1D1R2zsHBzjk4uHXOdmnIGGOCnCUCY4wJcsGWCCb5OgAfsHMODnbOwcGVcw6qMQJjjDFnC7YegTHGmDNYIjDGmCAXkIlARPqIyHoR2SAiZ1U0FZFyIvKRZ/tPIhLvgzCLlRfn/ICIrBGRJBH5r4jU80Wcxamgc87V7hoRURHx+1sNvTlnEbne83e9WkTeL+kYi5sX/7bjRGSeiPzs+fd9uS/iLC4iMkVE9npmcMxru4jIS54/jyQRaV/kg6pqQP0AocBGoAFQFvgFaH5Gm38Ar3le3wB85Ou4S+CcuwMRntd3BMM5e9pFAguBH4EEX8ddAn/PjYGfgSqe5eq+jrsEznkScIfndXNgi6/jLuI5XwS0B1adY/vlwFc4Ey5eAPxU1GMGYo+gI7BBVTep6kngQ6D/GW36A+94Xk8Heoh/z3RR4Dmr6jxVTfMs/ogzY5w/8+bvGeAJ4FkgEOoWe3POfwNeUdVDAKq6t4RjLG7enLMClTyvo4CdJRhfsVPVhcDBfJr0B95Vx49AZRGpVZRjBmIiqANsz7Wc7FmXZxt1JtBJAaqWSHTu8Oacc7sN5xuFPyvwnD1d5rqqOrskA3ORN3/PTYAmIvK9iPwoIn1KLDp3eHPOY4AbRSQZ+BK4u2RC85nC/v9eIJuPIMiIyI1AAnCxr2Nxk4iEABOAIT4OpaSVwbk81A2n17dQRFqp6mFfBuWyvwBvq+rzItIZmCoiLVU129eB+YtA7BHsAOrmWo71rMuzjYiUwelOHiiR6NzhzTkjIpcCo4B+qnqihGJzS0HnHAm0BOaLyBaca6kz/XzA2Ju/52RgpqpmqOpm4FecxOCvvDnn24CPAVR1MRCOU5wtUHn1/3thBGIiWAo0FpH6IlIWZzB45hltZgKDPa+vBb5TzyiMnyrwnEWkHfA6ThLw9+vGUMA5q2qKqsaoaryqxuOMi/RTVX+e59Sbf9uf4fQGEJEYnEtFm0owxuLmzTlvA3oAiEgznESwr0SjLFkzgZs9dw9dAKSo6q6i7DDgLg2paqaI3AXMwbnjYIqqrhaRx4FlqjoTmIzTfdyAMyhzg+8iLjovz/k5oCLwH8+4+DZV7eezoIvIy3MOKF6e8xygl4isAbKAYarqt71dL8/5QeANEbkfZ+B4iD9/sRORD3CSeYxn3OMxIAxAVV/DGQe5HNgApAG3FPmYfvznZYwxphgE4qUhY4wxhWCJwBhjgpwlAmOMCXKWCIwxJshZIjDGmCBnicCUSiKSJSKJuX7i82l7rBiO97aIbPYca4XnCdXC7uNNEWnuef3wGdt+KGqMnv3k/LmsEpEvRKRyAe3b+ns1TuM+u33UlEoickxVKxZ323z28TYwS1Wni0gvYLyqti7C/oocU0H7FZF3gF9VdVw+7YfgVF29q7hjMYHDegTGL4hIRc88CitEZKWInFVpVERqicjCXN+Yu3rW9xKRxZ73/kdECvqAXgg08rz3Ac++VonIfZ51FURktoj84lk/wLN+vogkiMgzQHlPHNM82455fn8oIlfkivltEblWREJF5DkRWeqpMf93L/5YFuMpNiYiHT3n+LOI/CAi53mexH0cGOCJZYAn9ikissTTNq+KrSbY+Lr2tv3YT14/OE/FJnp+PsV5Cr6SZ1sMzlOVOT3aY57fDwKjPK9DceoNxeB8sFfwrB8OPJrH8d4GrvW8vg74CTgfWAlUwHkqezXQDrgGeCPXe6M8v+fjmfMgJ6ZcbXJivAp4x/O6LE4VyfLAUGC0Z305YBlQP484j+U6v/8AfTzLlYAynteXAp94Xg8BXs71/qeAGz2vK+PUIqrg679v+/HtT8CVmDAB47iqts1ZEJEw4CkRuQjIxvkmXAPYnes9S4EpnrafqWqiiFyMM1nJ957SGmVxvknn5TkRGY1Tp+Y2nPo1n6pqqieGGUBX4GvgeRF5Fudy0qJCnNdXwIsiUg7oAyxU1eOey1GtReRaT7sonGJxm894f3kRSfSc/1rgm1zt3xGRxjhlFsLOcfxeQD8R+T/PcjgQ59mXCVKWCIy/GARUA85X1QxxKoqG526gqgs9ieIK4G0RmQAcAr5R1b94cYxhqjo9Z0FEeuTVSFV/FWeug8uBJ0Xkv6r6uDcnoarpIjIf6A0MwJloBZzZpu5W1TkF7OK4qrYVkQic+jt3Ai/hTMAzT1Wv8gyszz/H+wW4RlXXexOvCQ42RmD8RRSw15MEugNnzbkszjzMe1T1DeBNnOn+fgS6iEjONf8KItLEy2MuAv4sIhEiUgHnss4iEakNpKnqezjF/PKaMzbD0zPJy0c4hcJyehfgfKjfkfMeEWniOWae1Jlt7h7gQfm9lHpOKeIhuZoexblElmMOcLd4ukfiVKU1Qc4SgfEX04AEEVkJ3Aysy6NNN+AXEfkZ59v2i6q6D+eD8QMRScK5LNTUmwOq6gqcsYMlOGMGb6rqz0ArYInnEs1jwJN5vH0SkJQzWHyGuTgTA32rzvSL4CSuNcAKcSYtf50CeuyeWJJwJmb5J/C059xzv28e0DxnsBin5xDmiW21Z9kEObt91Bhjgpz1CIwxJshZIjDGmCBnicAYY4KcJQJjjAlylgiMMSbIWSIwxpggZ4nAGGOC3P8DX3hL4CksJU0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(train['class_labels'].values, train_preds)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = np.argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "pyplot.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02540877, 0.03297017, 0.02066709, ..., 0.9827627 , 0.98686385,\n",
       "       0.98882335], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.840958605664488\n",
      "Recall: 0.9061032863849765\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision: {precision_score(test['class_labels'].values, np.where(test_preds > 0.5, 1, 0))}\")\n",
    "print(f\"Recall: {recall_score(test['class_labels'].values, np.where(test_preds > 0.5, 1, 0))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8299700968193889\n",
      "Recall: 0.9518213866039953\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision: {precision_score(train['class_labels'].values, np.where(train_preds > 0.406096, 1, 0))}\")\n",
    "print(f\"Recall: {recall_score(train['class_labels'].values, np.where(train_preds > 0.406096, 1, 0))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('../saved_model_128/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ = keras.applications.ResNet50(include_top = False,\n",
    "    input_shape = (128, 128, 3),\n",
    "    weights = None, \n",
    "    pooling = 'avg')\n",
    "sig = keras.layers.Dense(1, activation='sigmoid')(res_.layers[-1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x7fe56c995e80>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = keras.models.Model(inputs=res_.layers[0].input, outputs=sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 134, 134, 3)  0           ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 64, 64, 64)   9472        ['conv1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 64, 64, 64)   256         ['conv1_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 64, 64, 64)   0           ['conv1_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 66, 66, 64)   0           ['conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 32, 32, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 32, 32, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 32, 32, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 32, 32, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 32, 32, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 32, 32, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 32, 32, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 32, 32, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 32, 32, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 32, 32, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 32, 32, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 32, 32, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 32, 32, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 32, 32, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 32, 32, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 32, 32, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 32, 32, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 16, 16, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 16, 16, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 16, 16, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 16, 16, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 16, 16, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 16, 16, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 16, 16, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 16, 16, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 16, 16, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 16, 16, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 16, 16, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 16, 16, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 16, 16, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 16, 16, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 16, 16, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 16, 16, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 16, 16, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 16, 16, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 16, 16, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 16, 16, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 16, 16, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 16, 16, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 16, 16, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 16, 16, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 16, 16, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 8, 8, 256)    131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 8, 8, 256)    590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 8, 8, 1024)   525312      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 8, 8, 1024)   263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 8, 8, 1024)  4096        ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 8, 8, 1024)  4096        ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 8, 8, 1024)   0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                                                  'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 8, 8, 1024)   0           ['conv4_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 8, 8, 256)    262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 8, 8, 256)    590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 8, 8, 1024)   263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 8, 8, 1024)  4096        ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 8, 8, 1024)   0           ['conv4_block1_out[0][0]',       \n",
      "                                                                  'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 8, 8, 1024)   0           ['conv4_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 8, 8, 256)    262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 8, 8, 256)    590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 8, 8, 1024)   263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 8, 8, 1024)  4096        ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 8, 8, 1024)   0           ['conv4_block2_out[0][0]',       \n",
      "                                                                  'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 8, 8, 1024)   0           ['conv4_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 8, 8, 256)    262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 8, 8, 256)    590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 8, 8, 1024)   263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 8, 8, 1024)  4096        ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 8, 8, 1024)   0           ['conv4_block3_out[0][0]',       \n",
      "                                                                  'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 8, 8, 1024)   0           ['conv4_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 8, 8, 256)    262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 8, 8, 256)    590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 8, 8, 1024)   263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 8, 8, 1024)  4096        ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 8, 8, 1024)   0           ['conv4_block4_out[0][0]',       \n",
      "                                                                  'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 8, 8, 1024)   0           ['conv4_block5_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 8, 8, 256)    262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 8, 8, 256)    590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 8, 8, 1024)   263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 8, 8, 1024)  4096        ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 8, 8, 1024)   0           ['conv4_block5_out[0][0]',       \n",
      "                                                                  'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 8, 8, 1024)   0           ['conv4_block6_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 4, 4, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 4, 4, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 4, 4, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 4, 4, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 4, 4, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 4, 4, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 4, 4, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 4, 4, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 4, 4, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 4, 4, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 4, 4, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 4, 4, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 4, 4, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 4, 4, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 4, 4, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 4, 4, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 4, 4, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 4, 4, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 4, 4, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 4, 4, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 4, 4, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 4, 4, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 4, 4, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 4, 4, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 4, 4, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 4, 4, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " avg_pool (GlobalAveragePooling  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            2049        ['avg_pool[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,589,761\n",
      "Trainable params: 23,536,641\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer \"dense_6\" with a weight list of length 2, but the layer was expecting 0 weights. Provided weights: [array([[ 0.03903475],\n       [ 0.0230649 ],\n     ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m res_\u001b[39m.\u001b[39mset_weights(res50weights)\n\u001b[0;32m----> 2\u001b[0m sigmoid_layer\u001b[39m.\u001b[39;49mset_weights(sigmoid_weights)\n",
      "File \u001b[0;32m~/test_env/lib/python3.8/site-packages/keras/engine/base_layer.py:1784\u001b[0m, in \u001b[0;36mLayer.set_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1781\u001b[0m         expected_num_weights \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1783\u001b[0m \u001b[39mif\u001b[39;00m expected_num_weights \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(weights):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1785\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mYou called `set_weights(weights)` on layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1786\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwith a weight list of length \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, but the layer was \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1787\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mexpecting \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m weights. Provided weights: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1788\u001b[0m         \u001b[39m%\u001b[39m (\n\u001b[1;32m   1789\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m   1790\u001b[0m             \u001b[39mlen\u001b[39m(weights),\n\u001b[1;32m   1791\u001b[0m             expected_num_weights,\n\u001b[1;32m   1792\u001b[0m             \u001b[39mstr\u001b[39m(weights)[:\u001b[39m50\u001b[39m],\n\u001b[1;32m   1793\u001b[0m         )\n\u001b[1;32m   1794\u001b[0m     )\n\u001b[1;32m   1796\u001b[0m weight_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   1797\u001b[0m weight_value_tuples \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: You called `set_weights(weights)` on layer \"dense_6\" with a weight list of length 2, but the layer was expecting 0 weights. Provided weights: [array([[ 0.03903475],\n       [ 0.0230649 ],\n     ..."
     ]
    }
   ],
   "source": [
    "res_.set_weights(res50weights)\n",
    "sigmoid_layer.set_weights(sigmoid_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.io.read_file(train['fixed_paths'].values.tolist()[0])\n",
    "img = tf.image.decode_jpeg(img, channels=3)\n",
    "img = tf.image.resize(img, size = (128, 128))\n",
    "img = img / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_infer(data_path):\n",
    "    img = tf.io.read_file(data_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, size = (128, 128))\n",
    "    img = img / 255\n",
    "    temp = res_.predict(img.numpy().reshape(1, 128, 128, 3), verbose=0)\n",
    "    preds = sigmoid(temp)\n",
    "    return preds\n",
    "\n",
    "def actual_model(data_path):\n",
    "    img = tf.io.read_file(data_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, size = (128, 128))\n",
    "    img = img / 255\n",
    "    # temp = res_.predict(img.numpy().reshape(1, 128, 128, 3), verbose=0)\n",
    "    preds = model.predict(img.numpy().reshape(1, 128, 128, 3), verbose=0)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [01:59<00:00,  8.36it/s]\n"
     ]
    }
   ],
   "source": [
    "custom_preds = []\n",
    "for x in tqdm(train['fixed_paths'].values.tolist()[:1000]):\n",
    "    custom_preds.append(custom_infer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [08:57<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "actual_preds = []\n",
    "for x in tqdm(train['fixed_paths'].values.tolist()[:1000]):\n",
    "    actual_preds.append(actual_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.11793849]], dtype=float32),\n",
       " array([[0.04476193]], dtype=float32),\n",
       " array([[0.38285363]], dtype=float32),\n",
       " array([[0.06439063]], dtype=float32),\n",
       " array([[0.03639622]], dtype=float32),\n",
       " array([[0.03237434]], dtype=float32),\n",
       " array([[0.9843277]], dtype=float32),\n",
       " array([[0.03357893]], dtype=float32),\n",
       " array([[0.0446183]], dtype=float32),\n",
       " array([[0.03253573]], dtype=float32)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.7750413]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.8023023]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.74180204]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.7893914]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.8325746]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.9721983]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.97698385]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.76763284]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.7891883]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.8371881]], dtype=float32)>]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
